{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tiling分块技术 - 实践篇\n",
        "\n",
        "本notebook通过实际代码帮助你理解Tiling分块技术。\n",
        "\n",
        "**学习目标：**\n",
        "- 实现分块矩阵乘法\n",
        "- 实现简化版FlashAttention\n",
        "- 验证分块计算的正确性\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 分块矩阵乘法\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tiled_matmul(A, B, block_size=64):\n",
        "    \"\"\"\n",
        "    分块矩阵乘法\n",
        "    \n",
        "    Args:\n",
        "        A: [M, K] 矩阵\n",
        "        B: [K, N] 矩阵\n",
        "        block_size: 块大小\n",
        "    \n",
        "    Returns:\n",
        "        C: [M, N] = A @ B\n",
        "    \"\"\"\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2, \"矩阵维度不匹配\"\n",
        "    \n",
        "    # 初始化输出\n",
        "    C = torch.zeros(M, N, dtype=A.dtype, device=A.device)\n",
        "    \n",
        "    # 计算块数\n",
        "    num_blocks_m = (M + block_size - 1) // block_size\n",
        "    num_blocks_n = (N + block_size - 1) // block_size\n",
        "    num_blocks_k = (K + block_size - 1) // block_size\n",
        "    \n",
        "    # 三层循环：遍历输出块\n",
        "    for i in range(num_blocks_m):\n",
        "        for j in range(num_blocks_n):\n",
        "            # 输出块的范围\n",
        "            m_start = i * block_size\n",
        "            m_end = min((i + 1) * block_size, M)\n",
        "            n_start = j * block_size\n",
        "            n_end = min((j + 1) * block_size, N)\n",
        "            \n",
        "            # 累加器\n",
        "            C_block = torch.zeros(m_end - m_start, n_end - n_start, \n",
        "                                  dtype=A.dtype, device=A.device)\n",
        "            \n",
        "            # 内层循环：累加K维度的贡献\n",
        "            for k in range(num_blocks_k):\n",
        "                k_start = k * block_size\n",
        "                k_end = min((k + 1) * block_size, K)\n",
        "                \n",
        "                # 获取A和B的块\n",
        "                A_block = A[m_start:m_end, k_start:k_end]\n",
        "                B_block = B[k_start:k_end, n_start:n_end]\n",
        "                \n",
        "                # 累加\n",
        "                C_block += A_block @ B_block\n",
        "            \n",
        "            # 写入结果\n",
        "            C[m_start:m_end, n_start:n_end] = C_block\n",
        "    \n",
        "    return C\n",
        "\n",
        "# 测试分块矩阵乘法\n",
        "M, K, N = 256, 128, 192\n",
        "A = torch.randn(M, K)\n",
        "B = torch.randn(K, N)\n",
        "\n",
        "# 标准矩阵乘法\n",
        "C_standard = A @ B\n",
        "\n",
        "# 分块矩阵乘法\n",
        "C_tiled = tiled_matmul(A, B, block_size=64)\n",
        "\n",
        "# 验证正确性\n",
        "error = (C_standard - C_tiled).abs().max().item()\n",
        "print(f\"矩阵大小: A={A.shape}, B={B.shape}\")\n",
        "print(f\"最大误差: {error:.2e}\")\n",
        "print(f\"结果正确: {error < 1e-5}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 可视化分块过程\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_tiling(N, block_size):\n",
        "    \"\"\"可视化矩阵分块\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    num_blocks = (N + block_size - 1) // block_size\n",
        "    \n",
        "    # 创建分块矩阵的颜色图\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, num_blocks * num_blocks))\n",
        "    \n",
        "    for ax_idx, (title, shape) in enumerate([\n",
        "        ('Q 矩阵', (N, 64)),\n",
        "        ('K^T 矩阵', (64, N)),\n",
        "        ('S = QK^T', (N, N))\n",
        "    ]):\n",
        "        ax = axes[ax_idx]\n",
        "        \n",
        "        # 绘制分块\n",
        "        for i in range(num_blocks):\n",
        "            for j in range(num_blocks):\n",
        "                if ax_idx == 0:  # Q矩阵，只有行分块\n",
        "                    if j == 0:\n",
        "                        rect = plt.Rectangle((0, i * block_size), shape[1], block_size,\n",
        "                                            fill=True, alpha=0.5, \n",
        "                                            facecolor=colors[i * num_blocks])\n",
        "                        ax.add_patch(rect)\n",
        "                        ax.text(shape[1]/2, i * block_size + block_size/2, \n",
        "                               f'Q_{i+1}', ha='center', va='center', fontsize=12)\n",
        "                elif ax_idx == 1:  # K^T矩阵，只有列分块\n",
        "                    if i == 0:\n",
        "                        rect = plt.Rectangle((j * block_size, 0), block_size, shape[0],\n",
        "                                            fill=True, alpha=0.5, \n",
        "                                            facecolor=colors[j])\n",
        "                        ax.add_patch(rect)\n",
        "                        ax.text(j * block_size + block_size/2, shape[0]/2, \n",
        "                               f'K^T_{j+1}', ha='center', va='center', fontsize=12)\n",
        "                else:  # S矩阵，行列都分块\n",
        "                    rect = plt.Rectangle((j * block_size, i * block_size), \n",
        "                                        block_size, block_size,\n",
        "                                        fill=True, alpha=0.5, \n",
        "                                        facecolor=colors[i * num_blocks + j])\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(j * block_size + block_size/2, \n",
        "                           i * block_size + block_size/2, \n",
        "                           f'S_{i+1},{j+1}', ha='center', va='center', fontsize=10)\n",
        "        \n",
        "        ax.set_xlim(0, shape[1])\n",
        "        ax.set_ylim(0, shape[0])\n",
        "        ax.set_aspect('equal')\n",
        "        ax.invert_yaxis()\n",
        "        ax.set_title(f'{title}\\n形状: {shape}', fontsize=14)\n",
        "        ax.set_xlabel('列')\n",
        "        ax.set_ylabel('行')\n",
        "        \n",
        "        # 添加网格\n",
        "        for i in range(num_blocks + 1):\n",
        "            ax.axhline(y=i * block_size, color='black', linewidth=0.5)\n",
        "        for j in range(num_blocks + 1):\n",
        "            ax.axvline(x=j * block_size if ax_idx != 0 else 0, color='black', linewidth=0.5)\n",
        "        if ax_idx == 0:\n",
        "            ax.axvline(x=shape[1], color='black', linewidth=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f'矩阵分块示意图 (N={N}, block_size={block_size})', y=1.02, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "visualize_tiling(256, 64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 简化版FlashAttention实现\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flash_attention_forward(Q, K, V, Br=64, Bc=64):\n",
        "    \"\"\"\n",
        "    FlashAttention前向传播的简化Python实现\n",
        "    \n",
        "    Args:\n",
        "        Q, K, V: [batch, seq_len, d]\n",
        "        Br: Q的块大小\n",
        "        Bc: K/V的块大小\n",
        "    \n",
        "    Returns:\n",
        "        O: [batch, seq_len, d]\n",
        "    \"\"\"\n",
        "    batch, N, d = Q.shape\n",
        "    scale = 1.0 / (d ** 0.5)\n",
        "    \n",
        "    # 计算块数\n",
        "    Tr = (N + Br - 1) // Br\n",
        "    Tc = (N + Bc - 1) // Bc\n",
        "    \n",
        "    # 初始化输出\n",
        "    O = torch.zeros_like(Q)\n",
        "    \n",
        "    # 外循环：遍历Q的块\n",
        "    for i in range(Tr):\n",
        "        # 获取Q的第i块\n",
        "        q_start = i * Br\n",
        "        q_end = min((i + 1) * Br, N)\n",
        "        Q_i = Q[:, q_start:q_end, :]  # [batch, Br_actual, d]\n",
        "        \n",
        "        Br_actual = q_end - q_start\n",
        "        \n",
        "        # 初始化softmax统计量\n",
        "        m_i = torch.full((batch, Br_actual), float('-inf'), \n",
        "                         dtype=Q.dtype, device=Q.device)\n",
        "        l_i = torch.zeros(batch, Br_actual, dtype=Q.dtype, device=Q.device)\n",
        "        O_i = torch.zeros(batch, Br_actual, d, dtype=Q.dtype, device=Q.device)\n",
        "        \n",
        "        # 内循环：遍历K/V的块\n",
        "        for j in range(Tc):\n",
        "            # 获取K/V的第j块\n",
        "            kv_start = j * Bc\n",
        "            kv_end = min((j + 1) * Bc, N)\n",
        "            K_j = K[:, kv_start:kv_end, :]  # [batch, Bc_actual, d]\n",
        "            V_j = V[:, kv_start:kv_end, :]  # [batch, Bc_actual, d]\n",
        "            \n",
        "            # 计算局部注意力分数 S_ij = Q_i @ K_j^T / sqrt(d)\n",
        "            S_ij = torch.matmul(Q_i, K_j.transpose(-2, -1)) * scale  # [batch, Br_actual, Bc_actual]\n",
        "            \n",
        "            # Online Softmax: 更新最大值\n",
        "            m_ij = S_ij.max(dim=-1).values  # [batch, Br_actual]\n",
        "            m_new = torch.maximum(m_i, m_ij)\n",
        "            \n",
        "            # 计算指数和更新\n",
        "            exp_m_old = torch.exp(m_i - m_new)  # [batch, Br_actual]\n",
        "            P_ij = torch.exp(S_ij - m_new.unsqueeze(-1))  # [batch, Br_actual, Bc_actual]\n",
        "            l_new = exp_m_old * l_i + P_ij.sum(dim=-1)  # [batch, Br_actual]\n",
        "            \n",
        "            # 更新输出\n",
        "            O_i = exp_m_old.unsqueeze(-1) * O_i + torch.matmul(P_ij, V_j)\n",
        "            \n",
        "            # 更新统计量\n",
        "            m_i = m_new\n",
        "            l_i = l_new\n",
        "        \n",
        "        # 最终归一化\n",
        "        O_i = O_i / l_i.unsqueeze(-1)\n",
        "        \n",
        "        # 写回输出\n",
        "        O[:, q_start:q_end, :] = O_i\n",
        "    \n",
        "    return O\n",
        "\n",
        "# 测试FlashAttention\n",
        "batch_size = 2\n",
        "seq_len = 256\n",
        "d = 64\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, d)\n",
        "K = torch.randn(batch_size, seq_len, d)\n",
        "V = torch.randn(batch_size, seq_len, d)\n",
        "\n",
        "print(\"测试FlashAttention实现...\")\n",
        "print(f\"输入形状: Q={Q.shape}, K={K.shape}, V={V.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 验证正确性\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_attention(Q, K, V):\n",
        "    \"\"\"标准Attention实现\"\"\"\n",
        "    scale = 1.0 / (Q.shape[-1] ** 0.5)\n",
        "    S = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
        "    P = F.softmax(S, dim=-1)\n",
        "    O = torch.matmul(P, V)\n",
        "    return O\n",
        "\n",
        "# 对比标准实现和FlashAttention\n",
        "O_standard = standard_attention(Q, K, V)\n",
        "O_flash = flash_attention_forward(Q, K, V, Br=64, Bc=64)\n",
        "\n",
        "# 计算误差\n",
        "max_error = (O_standard - O_flash).abs().max().item()\n",
        "mean_error = (O_standard - O_flash).abs().mean().item()\n",
        "relative_error = ((O_standard - O_flash).abs() / (O_standard.abs() + 1e-8)).mean().item()\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"正确性验证\")\n",
        "print(\"=\"*50)\n",
        "print(f\"最大绝对误差: {max_error:.2e}\")\n",
        "print(f\"平均绝对误差: {mean_error:.2e}\")\n",
        "print(f\"平均相对误差: {relative_error:.2e}\")\n",
        "print(f\"结果匹配: {max_error < 1e-4}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 不同块大小的影响\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_block_sizes(seq_len, d=64):\n",
        "    \"\"\"分析不同块大小的SRAM使用和循环次数\"\"\"\n",
        "    block_sizes = [16, 32, 64, 128, 256]\n",
        "    \n",
        "    results = []\n",
        "    for Br in block_sizes:\n",
        "        for Bc in block_sizes:\n",
        "            # SRAM使用量估算 (FP16)\n",
        "            bytes_per_element = 2\n",
        "            sram_usage = (\n",
        "                Br * d +  # Q块\n",
        "                Bc * d +  # K块\n",
        "                Bc * d +  # V块\n",
        "                Br * Bc + # S块\n",
        "                Br * d +  # O块\n",
        "                2 * Br    # m, l统计量\n",
        "            ) * bytes_per_element\n",
        "            \n",
        "            # 循环次数\n",
        "            Tr = (seq_len + Br - 1) // Br\n",
        "            Tc = (seq_len + Bc - 1) // Bc\n",
        "            total_iterations = Tr * Tc\n",
        "            \n",
        "            results.append({\n",
        "                'Br': Br, 'Bc': Bc,\n",
        "                'SRAM (KB)': sram_usage / 1024,\n",
        "                'Iterations': total_iterations\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 分析N=1024的情况\n",
        "results = analyze_block_sizes(1024)\n",
        "\n",
        "# 显示部分结果\n",
        "print(\"块大小与资源使用分析 (N=1024, d=64)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Br':<8} {'Bc':<8} {'SRAM使用(KB)':<15} {'迭代次数':<15}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for r in results[:15]:  # 只显示前15个\n",
        "    print(f\"{r['Br']:<8} {r['Bc']:<8} {r['SRAM (KB)']:<15.1f} {r['Iterations']:<15}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\\n注意: SRAM使用需要小于GPU共享内存大小 (如A100为~164KB/SM)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 可视化块大小权衡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化块大小与SRAM使用、迭代次数的关系\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 假设 Br = Bc\n",
        "block_sizes = [16, 32, 64, 128, 256]\n",
        "sram_usage = []\n",
        "iterations_1024 = []\n",
        "iterations_4096 = []\n",
        "\n",
        "for bs in block_sizes:\n",
        "    # SRAM使用 (KB)\n",
        "    usage = (bs * 64 + 2 * bs * 64 + bs * bs + bs * 64 + 2 * bs) * 2 / 1024\n",
        "    sram_usage.append(usage)\n",
        "    \n",
        "    # 迭代次数\n",
        "    Tr_1024 = (1024 + bs - 1) // bs\n",
        "    Tc_1024 = (1024 + bs - 1) // bs\n",
        "    iterations_1024.append(Tr_1024 * Tc_1024)\n",
        "    \n",
        "    Tr_4096 = (4096 + bs - 1) // bs\n",
        "    Tc_4096 = (4096 + bs - 1) // bs\n",
        "    iterations_4096.append(Tr_4096 * Tc_4096)\n",
        "\n",
        "# 左图: SRAM使用\n",
        "ax1 = axes[0]\n",
        "bars = ax1.bar(range(len(block_sizes)), sram_usage, color='steelblue', alpha=0.7)\n",
        "ax1.axhline(y=164, color='red', linestyle='--', label='A100 SRAM限制 (164KB)')\n",
        "ax1.set_xticks(range(len(block_sizes)))\n",
        "ax1.set_xticklabels([str(bs) for bs in block_sizes])\n",
        "ax1.set_xlabel('块大小 (Br = Bc)')\n",
        "ax1.set_ylabel('SRAM使用 (KB)')\n",
        "ax1.set_title('块大小与SRAM使用')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 添加数值标签\n",
        "for bar, usage in zip(bars, sram_usage):\n",
        "    ax1.annotate(f'{usage:.1f}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
        "                xytext=(0, 3), textcoords=\"offset points\",\n",
        "                ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# 右图: 迭代次数\n",
        "ax2 = axes[1]\n",
        "x = np.arange(len(block_sizes))\n",
        "width = 0.35\n",
        "bars1 = ax2.bar(x - width/2, iterations_1024, width, label='N=1024', color='steelblue')\n",
        "bars2 = ax2.bar(x + width/2, iterations_4096, width, label='N=4096', color='coral')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels([str(bs) for bs in block_sizes])\n",
        "ax2.set_xlabel('块大小 (Br = Bc)')\n",
        "ax2.set_ylabel('迭代次数')\n",
        "ax2.set_title('块大小与迭代次数')\n",
        "ax2.legend()\n",
        "ax2.set_yscale('log')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n结论: 需要在SRAM容量限制和迭代次数之间找到平衡\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "\n",
        "1. **分块矩阵乘法**\n",
        "   - 将大矩阵分成小块逐步计算\n",
        "   - 结果与直接计算完全相同\n",
        "\n",
        "2. **FlashAttention的分块策略**\n",
        "   - 外循环遍历Q块，内循环遍历K/V块\n",
        "   - 使用Online Softmax增量更新\n",
        "\n",
        "3. **块大小的权衡**\n",
        "   - 大块: 更少迭代，但需要更多SRAM\n",
        "   - 小块: 更多迭代，但SRAM需求小\n",
        "   - 需要根据GPU硬件选择合适的块大小\n",
        "\n",
        "## 下一步\n",
        "\n",
        "在下一节\"Recomputation重计算策略\"中，我们将学习如何用计算换内存。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
