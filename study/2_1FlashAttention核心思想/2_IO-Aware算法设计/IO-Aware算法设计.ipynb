{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IO-Aware算法设计 - 实践篇\n",
        "\n",
        "本notebook通过实际代码帮助你理解IO-Aware算法设计的核心概念。\n",
        "\n",
        "**学习目标：**\n",
        "- 理解Roofline模型\n",
        "- 比较不同实现的HBM访问量\n",
        "- 可视化计算强度与性能的关系\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Roofline模型可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_roofline(peak_compute, memory_bandwidth, operations=None):\n",
        "    \"\"\"\n",
        "    绘制Roofline模型图\n",
        "    \n",
        "    Args:\n",
        "        peak_compute: 峰值计算能力 (TFLOPs)\n",
        "        memory_bandwidth: 内存带宽 (TB/s)\n",
        "        operations: 要标注的操作列表 [(name, arithmetic_intensity, color), ...]\n",
        "    \"\"\"\n",
        "    # 计算脊点\n",
        "    ridge_point = peak_compute / memory_bandwidth\n",
        "    \n",
        "    # 创建x轴（计算强度）\n",
        "    ai = np.logspace(-1, 3, 1000)  # 0.1 到 1000 FLOPs/Byte\n",
        "    \n",
        "    # 计算性能上限\n",
        "    memory_bound = ai * memory_bandwidth  # 内存绑定区域\n",
        "    compute_bound = np.full_like(ai, peak_compute)  # 计算绑定区域\n",
        "    performance = np.minimum(memory_bound, compute_bound)\n",
        "    \n",
        "    # 绘图\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    \n",
        "    # Roofline曲线\n",
        "    ax.loglog(ai, performance, 'b-', linewidth=2.5, label='Roofline')\n",
        "    \n",
        "    # 标注内存带宽限制和计算限制\n",
        "    ax.axhline(y=peak_compute, color='r', linestyle='--', alpha=0.5, label=f'峰值计算: {peak_compute} TFLOPs')\n",
        "    ax.axvline(x=ridge_point, color='g', linestyle='--', alpha=0.5, label=f'脊点: {ridge_point:.0f} FLOPs/Byte')\n",
        "    \n",
        "    # 填充区域\n",
        "    ax.fill_between(ai[ai < ridge_point], 0, memory_bound[ai < ridge_point], \n",
        "                    alpha=0.1, color='orange', label='内存绑定区域')\n",
        "    ax.fill_between(ai[ai >= ridge_point], 0, compute_bound[ai >= ridge_point], \n",
        "                    alpha=0.1, color='blue', label='计算绑定区域')\n",
        "    \n",
        "    # 标注操作点\n",
        "    if operations:\n",
        "        for name, op_ai, color in operations:\n",
        "            op_perf = min(op_ai * memory_bandwidth, peak_compute)\n",
        "            ax.scatter([op_ai], [op_perf], s=150, c=color, zorder=5, edgecolors='black')\n",
        "            ax.annotate(name, (op_ai, op_perf), textcoords=\"offset points\", \n",
        "                       xytext=(10, 10), fontsize=11, fontweight='bold')\n",
        "    \n",
        "    ax.set_xlabel('计算强度 (FLOPs/Byte)', fontsize=12)\n",
        "    ax.set_ylabel('性能 (TFLOPs)', fontsize=12)\n",
        "    ax.set_title('Roofline模型 - GPU性能分析', fontsize=14)\n",
        "    ax.legend(loc='lower right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim(0.1, 1000)\n",
        "    ax.set_ylim(1, 500)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return ridge_point\n",
        "\n",
        "# A100 GPU参数\n",
        "peak_compute = 312  # TFLOPs (FP16 Tensor Core)\n",
        "memory_bandwidth = 2.0  # TB/s\n",
        "\n",
        "# 常见操作的计算强度\n",
        "operations = [\n",
        "    ('Softmax', 2.5, 'red'),\n",
        "    ('矩阵乘法 (小)', 30, 'orange'),\n",
        "    ('Attention (标准)', 70, 'purple'),\n",
        "    ('矩阵乘法 (大)', 200, 'green'),\n",
        "]\n",
        "\n",
        "ridge = plot_roofline(peak_compute, memory_bandwidth, operations)\n",
        "print(f\"\\nA100 GPU脊点: {ridge:.0f} FLOPs/Byte\")\n",
        "print(\"标准Attention的计算强度约为70 FLOPs/Byte，远低于脊点，是内存绑定的！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. HBM访问量计算\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_hbm_access(seq_len, d, batch_size=1, num_heads=12, \n",
        "                          sram_size=100*1024, method='standard'):\n",
        "    \"\"\"\n",
        "    计算不同方法的HBM访问量\n",
        "    \n",
        "    Args:\n",
        "        seq_len: 序列长度\n",
        "        d: 头维度\n",
        "        sram_size: SRAM大小 (bytes)\n",
        "        method: 'standard' 或 'flash'\n",
        "    \"\"\"\n",
        "    bytes_per_element = 2  # FP16\n",
        "    \n",
        "    # Q, K, V, O 的大小\n",
        "    qkvo_size = batch_size * num_heads * seq_len * d * bytes_per_element\n",
        "    \n",
        "    # S, P 矩阵的大小\n",
        "    sp_size = batch_size * num_heads * seq_len * seq_len * bytes_per_element\n",
        "    \n",
        "    if method == 'standard':\n",
        "        # 标准Attention的HBM访问\n",
        "        # 读: Q, K (for QK^T), S (for softmax), P, V (for PV)\n",
        "        # 写: S, P, O\n",
        "        hbm_read = 3 * qkvo_size + 2 * sp_size  # Q,K,V + S,P\n",
        "        hbm_write = qkvo_size + 2 * sp_size     # O + S,P\n",
        "        \n",
        "    elif method == 'flash':\n",
        "        # FlashAttention的HBM访问\n",
        "        # 读: Q, K, V (多次读取，但总量约为 N²d/M)\n",
        "        # 写: O\n",
        "        M = sram_size // bytes_per_element  # SRAM能容纳的元素数\n",
        "        \n",
        "        # 外循环次数 ≈ N * d / M (Q块数)\n",
        "        # 每次外循环读取所有K,V\n",
        "        num_outer_loops = max(1, (seq_len * d) // M)\n",
        "        \n",
        "        hbm_read = qkvo_size + num_outer_loops * 2 * qkvo_size  # 初始Q + 多次K,V\n",
        "        hbm_write = qkvo_size  # 只写O\n",
        "        \n",
        "    return hbm_read, hbm_write, hbm_read + hbm_write\n",
        "\n",
        "# 对比不同序列长度下的HBM访问量\n",
        "print(\"HBM访问量对比 (batch=1, heads=12, d=64, FP16)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'序列长度':<12} {'标准Attention':<20} {'FlashAttention':<20} {'减少比例':<15}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "seq_lengths = [512, 1024, 2048, 4096, 8192]\n",
        "standard_access = []\n",
        "flash_access = []\n",
        "\n",
        "for seq_len in seq_lengths:\n",
        "    std_r, std_w, std_total = calculate_hbm_access(seq_len, 64, method='standard')\n",
        "    flash_r, flash_w, flash_total = calculate_hbm_access(seq_len, 64, method='flash')\n",
        "    \n",
        "    standard_access.append(std_total / 1e6)\n",
        "    flash_access.append(flash_total / 1e6)\n",
        "    \n",
        "    reduction = (std_total - flash_total) / std_total * 100\n",
        "    \n",
        "    print(f\"{seq_len:<12} {std_total/1e6:<20.2f} MB {flash_total/1e6:<20.2f} MB {reduction:<15.1f}%\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 可视化HBM访问量对比\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化HBM访问量\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 左图：绝对访问量\n",
        "ax1 = axes[0]\n",
        "x = np.arange(len(seq_lengths))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, standard_access, width, label='标准Attention', color='coral')\n",
        "bars2 = ax1.bar(x + width/2, flash_access, width, label='FlashAttention', color='steelblue')\n",
        "\n",
        "ax1.set_xlabel('序列长度')\n",
        "ax1.set_ylabel('HBM访问量 (MB)')\n",
        "ax1.set_title('HBM访问量对比')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels([str(s) for s in seq_lengths])\n",
        "ax1.legend()\n",
        "ax1.set_yscale('log')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 右图：减少比例\n",
        "ax2 = axes[1]\n",
        "reduction_ratios = [(s - f) / s * 100 for s, f in zip(standard_access, flash_access)]\n",
        "\n",
        "bars = ax2.bar(x, reduction_ratios, color='green', alpha=0.7)\n",
        "ax2.set_xlabel('序列长度')\n",
        "ax2.set_ylabel('HBM访问减少比例 (%)')\n",
        "ax2.set_title('FlashAttention减少的HBM访问量')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels([str(s) for s in seq_lengths])\n",
        "ax2.axhline(y=50, color='red', linestyle='--', label='50%减少')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 添加数值标签\n",
        "for bar, ratio in zip(bars, reduction_ratios):\n",
        "    height = bar.get_height()\n",
        "    ax2.annotate(f'{ratio:.1f}%',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. IO成本与计算成本的对比\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_time_breakdown(seq_len, d=64, batch_size=1, num_heads=12,\n",
        "                            peak_tflops=312, memory_bw_tbs=2.0):\n",
        "    \"\"\"\n",
        "    估算计算时间和内存访问时间的占比\n",
        "    \"\"\"\n",
        "    bytes_per_element = 2  # FP16\n",
        "    \n",
        "    # 计算量 (FLOPs)\n",
        "    # QK^T: 2 * N * N * d, PV: 2 * N * N * d, Softmax: ~5 * N * N\n",
        "    total_flops = batch_size * num_heads * (4 * seq_len * seq_len * d + 5 * seq_len * seq_len)\n",
        "    \n",
        "    # 标准Attention的HBM访问量 (bytes)\n",
        "    qkvo_bytes = 4 * batch_size * num_heads * seq_len * d * bytes_per_element\n",
        "    sp_bytes = 2 * batch_size * num_heads * seq_len * seq_len * bytes_per_element\n",
        "    total_bytes = qkvo_bytes + 2 * sp_bytes  # 读写S和P各一次\n",
        "    \n",
        "    # 时间估算\n",
        "    compute_time = total_flops / (peak_tflops * 1e12)  # 秒\n",
        "    memory_time = total_bytes / (memory_bw_tbs * 1e12)  # 秒\n",
        "    \n",
        "    return compute_time, memory_time\n",
        "\n",
        "print(\"计算时间 vs 内存访问时间 (A100 GPU)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'序列长度':<12} {'计算时间(μs)':<15} {'内存时间(μs)':<15} {'内存时间占比':<15}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "compute_times = []\n",
        "memory_times = []\n",
        "\n",
        "for seq_len in seq_lengths:\n",
        "    ct, mt = estimate_time_breakdown(seq_len)\n",
        "    compute_times.append(ct * 1e6)  # 转换为微秒\n",
        "    memory_times.append(mt * 1e6)\n",
        "    \n",
        "    mem_ratio = mt / (ct + mt) * 100\n",
        "    print(f\"{seq_len:<12} {ct*1e6:<15.2f} {mt*1e6:<15.2f} {mem_ratio:<15.1f}%\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\\n结论：内存访问时间占总时间的大部分，这就是为什么需要IO-Aware优化！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 可视化时间占比\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化时间占比\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 左图：堆叠柱状图\n",
        "ax1 = axes[0]\n",
        "x = np.arange(len(seq_lengths))\n",
        "width = 0.6\n",
        "\n",
        "ax1.bar(x, compute_times, width, label='计算时间', color='steelblue')\n",
        "ax1.bar(x, memory_times, width, bottom=compute_times, label='内存访问时间', color='coral')\n",
        "\n",
        "ax1.set_xlabel('序列长度')\n",
        "ax1.set_ylabel('时间 (μs)')\n",
        "ax1.set_title('Attention执行时间分解')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels([str(s) for s in seq_lengths])\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 右图：饼图（以N=4096为例）\n",
        "ax2 = axes[1]\n",
        "idx = seq_lengths.index(4096)\n",
        "sizes = [compute_times[idx], memory_times[idx]]\n",
        "labels = [f'计算时间\\n{compute_times[idx]:.1f}μs', f'内存访问时间\\n{memory_times[idx]:.1f}μs']\n",
        "colors = ['steelblue', 'coral']\n",
        "explode = (0, 0.05)\n",
        "\n",
        "ax2.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax2.set_title(f'N=4096时的时间分解')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 算子融合的效果模拟\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_kernel_fusion_effect(seq_len, d=64, batch_size=1, num_heads=12):\n",
        "    \"\"\"\n",
        "    模拟算子融合的效果\n",
        "    \"\"\"\n",
        "    bytes_per_element = 2\n",
        "    \n",
        "    # 非融合版本：三个独立kernel\n",
        "    # Kernel 1: 读Q,K -> 写S\n",
        "    # Kernel 2: 读S -> 写P\n",
        "    # Kernel 3: 读P,V -> 写O\n",
        "    \n",
        "    qkv_size = batch_size * num_heads * seq_len * d * bytes_per_element\n",
        "    sp_size = batch_size * num_heads * seq_len * seq_len * bytes_per_element\n",
        "    \n",
        "    unfused_io = {\n",
        "        'K1_read': 2 * qkv_size,  # Q, K\n",
        "        'K1_write': sp_size,       # S\n",
        "        'K2_read': sp_size,        # S\n",
        "        'K2_write': sp_size,       # P\n",
        "        'K3_read': sp_size + qkv_size,  # P, V\n",
        "        'K3_write': qkv_size,      # O\n",
        "    }\n",
        "    unfused_total = sum(unfused_io.values())\n",
        "    \n",
        "    # 融合版本：单个kernel\n",
        "    # 只需要读Q,K,V，写O\n",
        "    fused_io = {\n",
        "        'read': 3 * qkv_size,  # Q, K, V\n",
        "        'write': qkv_size,     # O\n",
        "    }\n",
        "    fused_total = sum(fused_io.values())\n",
        "    \n",
        "    return unfused_total, fused_total, unfused_io, fused_io\n",
        "\n",
        "print(\"算子融合效果模拟\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for seq_len in [1024, 2048, 4096]:\n",
        "    unfused, fused, unfused_io, fused_io = simulate_kernel_fusion_effect(seq_len)\n",
        "    reduction = (unfused - fused) / unfused * 100\n",
        "    \n",
        "    print(f\"\\n序列长度 N = {seq_len}\")\n",
        "    print(\"-\"*50)\n",
        "    print(\"非融合版本 (3个kernel):\")\n",
        "    for k, v in unfused_io.items():\n",
        "        print(f\"  {k}: {v/1e6:.2f} MB\")\n",
        "    print(f\"  总计: {unfused/1e6:.2f} MB\")\n",
        "    \n",
        "    print(\"\\n融合版本 (1个kernel):\")\n",
        "    for k, v in fused_io.items():\n",
        "        print(f\"  {k}: {v/1e6:.2f} MB\")\n",
        "    print(f\"  总计: {fused/1e6:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\n减少IO: {reduction:.1f}%\")\n",
        "    print(\"-\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "\n",
        "1. **Roofline模型**\n",
        "   - 性能上限 = min(峰值计算, 计算强度 × 带宽)\n",
        "   - Attention处于内存绑定区域\n",
        "\n",
        "2. **HBM访问量分析**\n",
        "   - 标准Attention的中间矩阵S,P导致大量HBM访问\n",
        "   - FlashAttention通过分块和融合显著减少HBM访问\n",
        "\n",
        "3. **时间分解**\n",
        "   - 内存访问时间占总时间的大部分\n",
        "   - 优化内存访问比优化计算更重要\n",
        "\n",
        "4. **算子融合**\n",
        "   - 将多个操作合并成单个kernel\n",
        "   - 避免中间结果写回HBM\n",
        "\n",
        "## 下一步\n",
        "\n",
        "在下一节\"Tiling分块技术\"中，我们将学习如何将大矩阵分块处理。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
