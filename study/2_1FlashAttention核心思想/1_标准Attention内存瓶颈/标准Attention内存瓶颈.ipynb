{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 标准Attention内存瓶颈 - 实践篇\n",
        "\n",
        "本notebook通过实际代码帮助你理解标准Attention的内存问题。\n",
        "\n",
        "**学习目标：**\n",
        "- 实现标准Attention并测量显存占用\n",
        "- 观察O(N²)内存增长\n",
        "- 理解内存绑定的概念\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 检查GPU\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"显存: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 标准Attention实现\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_attention(Q, K, V, scale=None):\n",
        "    \"\"\"\n",
        "    标准Self-Attention实现\n",
        "    \n",
        "    Args:\n",
        "        Q: Query矩阵 [batch, seq_len, d]\n",
        "        K: Key矩阵 [batch, seq_len, d]\n",
        "        V: Value矩阵 [batch, seq_len, d]\n",
        "        scale: 缩放因子，默认为 1/sqrt(d)\n",
        "    \n",
        "    Returns:\n",
        "        O: 输出矩阵 [batch, seq_len, d]\n",
        "        S: 注意力分数矩阵 [batch, seq_len, seq_len]\n",
        "        P: 注意力权重矩阵 [batch, seq_len, seq_len]\n",
        "    \"\"\"\n",
        "    if scale is None:\n",
        "        scale = 1.0 / (Q.shape[-1] ** 0.5)\n",
        "    \n",
        "    # 步骤1: 计算注意力分数 S = QK^T / sqrt(d)\n",
        "    # [batch, seq_len, d] @ [batch, d, seq_len] -> [batch, seq_len, seq_len]\n",
        "    S = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
        "    \n",
        "    # 步骤2: Softmax归一化\n",
        "    P = F.softmax(S, dim=-1)\n",
        "    \n",
        "    # 步骤3: 加权求和 O = PV\n",
        "    # [batch, seq_len, seq_len] @ [batch, seq_len, d] -> [batch, seq_len, d]\n",
        "    O = torch.matmul(P, V)\n",
        "    \n",
        "    return O, S, P\n",
        "\n",
        "# 测试\n",
        "batch_size = 2\n",
        "seq_len = 128\n",
        "d = 64\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, d)\n",
        "K = torch.randn(batch_size, seq_len, d)\n",
        "V = torch.randn(batch_size, seq_len, d)\n",
        "\n",
        "O, S, P = standard_attention(Q, K, V)\n",
        "\n",
        "print(f\"输入形状: Q={Q.shape}, K={K.shape}, V={V.shape}\")\n",
        "print(f\"中间矩阵形状: S={S.shape}, P={P.shape}\")\n",
        "print(f\"输出形状: O={O.shape}\")\n",
        "print(f\"\\nP的每行和（应该都是1）: {P[0, 0, :].sum().item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 内存占用分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_memory_usage(batch_size, seq_len, d, dtype=torch.float16):\n",
        "    \"\"\"计算标准Attention的理论内存占用\"\"\"\n",
        "    bytes_per_element = 2 if dtype == torch.float16 else 4\n",
        "    \n",
        "    # Q, K, V, O: 每个 batch × seq_len × d\n",
        "    qkvo_memory = 4 * batch_size * seq_len * d * bytes_per_element\n",
        "    \n",
        "    # S, P: 每个 batch × seq_len × seq_len\n",
        "    sp_memory = 2 * batch_size * seq_len * seq_len * bytes_per_element\n",
        "    \n",
        "    return qkvo_memory, sp_memory\n",
        "\n",
        "# 分析不同序列长度的内存占用\n",
        "d = 64\n",
        "batch_size = 1\n",
        "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"{'序列长度':<12} {'Q/K/V/O内存':<15} {'S+P内存':<15} {'S+P占比':<10}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "qkvo_list = []\n",
        "sp_list = []\n",
        "\n",
        "for seq_len in seq_lengths:\n",
        "    qkvo, sp = calculate_memory_usage(batch_size, seq_len, d)\n",
        "    total = qkvo + sp\n",
        "    sp_ratio = sp / total * 100\n",
        "    \n",
        "    qkvo_list.append(qkvo / 1e6)\n",
        "    sp_list.append(sp / 1e6)\n",
        "    \n",
        "    print(f\"{seq_len:<12} {qkvo/1e6:<15.2f} MB {sp/1e6:<15.2f} MB {sp_ratio:<10.1f}%\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 可视化内存增长\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化O(N) vs O(N²)增长\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 左图：绝对内存占用\n",
        "ax1 = axes[0]\n",
        "x = np.array(seq_lengths)\n",
        "ax1.bar(np.arange(len(seq_lengths)) - 0.2, qkvo_list, 0.4, label='Q/K/V/O (O(N))', color='steelblue')\n",
        "ax1.bar(np.arange(len(seq_lengths)) + 0.2, sp_list, 0.4, label='S+P (O(N²))', color='coral')\n",
        "ax1.set_xticks(np.arange(len(seq_lengths)))\n",
        "ax1.set_xticklabels([str(s) for s in seq_lengths])\n",
        "ax1.set_xlabel('序列长度 N')\n",
        "ax1.set_ylabel('内存占用 (MB)')\n",
        "ax1.set_title('标准Attention内存占用分解')\n",
        "ax1.legend()\n",
        "ax1.set_yscale('log')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 右图：内存增长曲线\n",
        "ax2 = axes[1]\n",
        "n = np.linspace(512, 16384, 100)\n",
        "linear_memory = n * 64 * 2 * 4 / 1e6  # Q,K,V,O\n",
        "quadratic_memory = n * n * 2 * 2 / 1e6  # S,P\n",
        "\n",
        "ax2.plot(n, linear_memory, label='O(N): Q/K/V/O', color='steelblue', linewidth=2)\n",
        "ax2.plot(n, quadratic_memory, label='O(N²): S+P', color='coral', linewidth=2)\n",
        "ax2.set_xlabel('序列长度 N')\n",
        "ax2.set_ylabel('内存占用 (MB)')\n",
        "ax2.set_title('内存增长趋势对比')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GPU显存实际测量\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_attention_memory(seq_len, d=64, batch_size=1, num_heads=12, dtype=torch.float16):\n",
        "    \"\"\"实际测量GPU上Attention的显存占用\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return None, None\n",
        "    \n",
        "    device = torch.device('cuda')\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # 初始显存\n",
        "    initial_memory = torch.cuda.memory_allocated()\n",
        "    \n",
        "    # 创建输入\n",
        "    Q = torch.randn(batch_size, num_heads, seq_len, d, device=device, dtype=dtype)\n",
        "    K = torch.randn(batch_size, num_heads, seq_len, d, device=device, dtype=dtype)\n",
        "    V = torch.randn(batch_size, num_heads, seq_len, d, device=device, dtype=dtype)\n",
        "    \n",
        "    input_memory = torch.cuda.memory_allocated() - initial_memory\n",
        "    \n",
        "    # 执行Attention\n",
        "    scale = 1.0 / (d ** 0.5)\n",
        "    S = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
        "    P = F.softmax(S, dim=-1)\n",
        "    O = torch.matmul(P, V)\n",
        "    \n",
        "    peak_memory = torch.cuda.max_memory_allocated() - initial_memory\n",
        "    \n",
        "    # 清理\n",
        "    del Q, K, V, S, P, O\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return input_memory, peak_memory\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"实际GPU显存测量 (batch=1, heads=12, d=64, FP16)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'序列长度':<12} {'输入显存':<15} {'峰值显存':<15} {'中间矩阵':<15}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for seq_len in [512, 1024, 2048, 4096]:\n",
        "        try:\n",
        "            input_mem, peak_mem = measure_attention_memory(seq_len)\n",
        "            intermediate = peak_mem - input_mem\n",
        "            print(f\"{seq_len:<12} {input_mem/1e6:<15.2f} MB {peak_mem/1e6:<15.2f} MB {intermediate/1e6:<15.2f} MB\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"{seq_len:<12} OOM - 显存不足!\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"CUDA不可用，跳过GPU测量\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 计算强度分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_arithmetic_intensity(seq_len, d, batch_size=1, num_heads=12):\n",
        "    \"\"\"计算Attention各步骤的计算强度\"\"\"\n",
        "    bytes_per_element = 2  # FP16\n",
        "    \n",
        "    # 步骤1: S = QK^T\n",
        "    flops_1 = 2 * batch_size * num_heads * seq_len * seq_len * d\n",
        "    memory_read_1 = 2 * batch_size * num_heads * seq_len * d * bytes_per_element  # Q, K\n",
        "    memory_write_1 = batch_size * num_heads * seq_len * seq_len * bytes_per_element  # S\n",
        "    ai_1 = flops_1 / (memory_read_1 + memory_write_1)\n",
        "    \n",
        "    # 步骤2: P = softmax(S)\n",
        "    flops_2 = 5 * batch_size * num_heads * seq_len * seq_len  # exp, sum, div, etc.\n",
        "    memory_2 = 2 * batch_size * num_heads * seq_len * seq_len * bytes_per_element  # read S, write P\n",
        "    ai_2 = flops_2 / memory_2\n",
        "    \n",
        "    # 步骤3: O = PV\n",
        "    flops_3 = 2 * batch_size * num_heads * seq_len * seq_len * d\n",
        "    memory_read_3 = batch_size * num_heads * (seq_len * seq_len + seq_len * d) * bytes_per_element  # P, V\n",
        "    memory_write_3 = batch_size * num_heads * seq_len * d * bytes_per_element  # O\n",
        "    ai_3 = flops_3 / (memory_read_3 + memory_write_3)\n",
        "    \n",
        "    return ai_1, ai_2, ai_3\n",
        "\n",
        "print(\"Attention各步骤的计算强度 (FLOPs/Byte)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'序列长度':<12} {'QK^T':<15} {'Softmax':<15} {'PV':<15}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for seq_len in [512, 1024, 2048, 4096, 8192]:\n",
        "    ai_1, ai_2, ai_3 = calculate_arithmetic_intensity(seq_len, 64)\n",
        "    print(f\"{seq_len:<12} {ai_1:<15.2f} {ai_2:<15.2f} {ai_3:<15.2f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nA100 GPU平衡点: ~156 FLOPs/Byte\")\n",
        "print(f\"H100 GPU平衡点: ~296 FLOPs/Byte\")\n",
        "print(f\"\\n结论: Attention的计算强度远低于GPU平衡点，是内存绑定的!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 训练时的显存问题\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def attention_with_grad(Q, K, V):\n",
        "    \"\"\"带梯度的Attention，用于测试训练时显存\"\"\"\n",
        "    scale = 1.0 / (Q.shape[-1] ** 0.5)\n",
        "    S = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
        "    P = F.softmax(S, dim=-1)\n",
        "    O = torch.matmul(P, V)\n",
        "    return O\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"训练模式 vs 推理模式 显存对比\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'序列长度':<12} {'推理显存':<15} {'训练显存':<15} {'增加倍数':<10}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    device = torch.device('cuda')\n",
        "    \n",
        "    for seq_len in [512, 1024, 2048]:\n",
        "        try:\n",
        "            # 推理模式\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                Q = torch.randn(1, 12, seq_len, 64, device=device, dtype=torch.float16)\n",
        "                K = torch.randn(1, 12, seq_len, 64, device=device, dtype=torch.float16)\n",
        "                V = torch.randn(1, 12, seq_len, 64, device=device, dtype=torch.float16)\n",
        "                O = attention_with_grad(Q, K, V)\n",
        "            \n",
        "            inference_mem = torch.cuda.max_memory_allocated() / 1e6\n",
        "            \n",
        "            del Q, K, V, O\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            # 训练模式\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            \n",
        "            Q = torch.randn(1, 12, seq_len, 64, device=device, dtype=torch.float16, requires_grad=True)\n",
        "            K = torch.randn(1, 12, seq_len, 64, device=device, dtype=torch.float16, requires_grad=True)\n",
        "            V = torch.randn(1, 12, seq_len, 64, device=device, dtype=torch.float16, requires_grad=True)\n",
        "            O = attention_with_grad(Q, K, V)\n",
        "            loss = O.sum()\n",
        "            loss.backward()\n",
        "            \n",
        "            training_mem = torch.cuda.max_memory_allocated() / 1e6\n",
        "            \n",
        "            ratio = training_mem / inference_mem\n",
        "            \n",
        "            print(f\"{seq_len:<12} {inference_mem:<15.2f} MB {training_mem:<15.2f} MB {ratio:<10.2f}x\")\n",
        "            \n",
        "            del Q, K, V, O, loss\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        except RuntimeError as e:\n",
        "            print(f\"{seq_len:<12} OOM\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n说明: 训练时需要保存中间结果用于反向传播，显存占用更大!\")\n",
        "else:\n",
        "    print(\"CUDA不可用，跳过测试\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "\n",
        "1. **O(N²)内存问题**\n",
        "   - 中间矩阵S和P的大小是N×N\n",
        "   - 随着序列长度增加，内存占用呈二次增长\n",
        "\n",
        "2. **内存绑定特性**\n",
        "   - Attention的计算强度(~64)远低于GPU平衡点(~150+)\n",
        "   - 性能受限于HBM带宽而非计算能力\n",
        "\n",
        "3. **训练时显存翻倍**\n",
        "   - 反向传播需要保存前向的中间结果\n",
        "   - 进一步加剧了内存问题\n",
        "\n",
        "## 下一步\n",
        "\n",
        "在下一节\"IO-Aware算法设计\"中，我们将学习如何从内存访问角度优化算法。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
