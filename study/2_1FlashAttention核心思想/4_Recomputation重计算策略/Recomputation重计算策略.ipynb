{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recomputation重计算策略 - 实践篇\n",
        "\n",
        "本notebook通过实际代码帮助你理解Recomputation策略。\n",
        "\n",
        "**学习目标：**\n",
        "- 理解PyTorch的gradient checkpointing\n",
        "- 对比有无重计算的显存占用\n",
        "- 理解时间与空间的权衡\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 理解反向传播的内存需求\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_attention(Q, K, V):\n",
        "    \"\"\"标准Attention - 保存所有中间结果\"\"\"\n",
        "    scale = 1.0 / (Q.shape[-1] ** 0.5)\n",
        "    \n",
        "    # S矩阵: 需要保存用于反向传播\n",
        "    S = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
        "    \n",
        "    # P矩阵: 需要保存用于反向传播\n",
        "    P = F.softmax(S, dim=-1)\n",
        "    \n",
        "    # 输出\n",
        "    O = torch.matmul(P, V)\n",
        "    \n",
        "    return O\n",
        "\n",
        "def analyze_memory_for_backward(seq_len, d=64, batch_size=1, num_heads=12):\n",
        "    \"\"\"分析反向传播需要保存的内存\"\"\"\n",
        "    bytes_per_element = 2  # FP16\n",
        "    \n",
        "    # 需要保存的tensor\n",
        "    saved_tensors = {\n",
        "        'Q': batch_size * num_heads * seq_len * d,\n",
        "        'K': batch_size * num_heads * seq_len * d,\n",
        "        'V': batch_size * num_heads * seq_len * d,\n",
        "        'S (QK^T)': batch_size * num_heads * seq_len * seq_len,\n",
        "        'P (softmax)': batch_size * num_heads * seq_len * seq_len,\n",
        "    }\n",
        "    \n",
        "    total = sum(saved_tensors.values()) * bytes_per_element\n",
        "    \n",
        "    return saved_tensors, total\n",
        "\n",
        "# 分析不同序列长度\n",
        "print(\"反向传播需要保存的tensor (标准Attention)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for seq_len in [512, 1024, 2048, 4096]:\n",
        "    tensors, total = analyze_memory_for_backward(seq_len)\n",
        "    print(f\"\\n序列长度 N = {seq_len}\")\n",
        "    print(\"-\"*50)\n",
        "    for name, size in tensors.items():\n",
        "        print(f\"  {name}: {size * 2 / 1e6:.2f} MB\")\n",
        "    print(f\"  总计: {total / 1e6:.2f} MB\")\n",
        "    \n",
        "    # 计算S和P的占比\n",
        "    sp_size = (tensors['S (QK^T)'] + tensors['P (softmax)']) * 2\n",
        "    sp_ratio = sp_size / total * 100\n",
        "    print(f\"  S+P占比: {sp_ratio:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. PyTorch Checkpoint使用示例\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    \"\"\"标准Attention层\"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        \n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch, seq_len, _ = x.shape\n",
        "        \n",
        "        # 投影\n",
        "        q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        \n",
        "        # Attention\n",
        "        scale = 1.0 / (self.d_head ** 0.5)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        \n",
        "        # 输出投影\n",
        "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, self.d_model)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "class AttentionWithCheckpoint(nn.Module):\n",
        "    \"\"\"使用Checkpoint的Attention层\"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.attention = AttentionLayer(d_model, num_heads)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # 使用checkpoint包装，减少内存占用\n",
        "        return checkpoint(self.attention, x, use_reentrant=False)\n",
        "\n",
        "# 测试\n",
        "d_model = 768\n",
        "num_heads = 12\n",
        "batch_size = 2\n",
        "seq_len = 128\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "attn_normal = AttentionLayer(d_model, num_heads)\n",
        "attn_checkpoint = AttentionWithCheckpoint(d_model, num_heads)\n",
        "attn_checkpoint.attention = attn_normal  # 共享权重\n",
        "\n",
        "# 验证输出相同\n",
        "out1 = attn_normal(x)\n",
        "out2 = attn_checkpoint(x)\n",
        "\n",
        "print(f\"输出形状: {out1.shape}\")\n",
        "print(f\"输出是否相同: {torch.allclose(out1, out2, atol=1e-5)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 显存占用对比（需要GPU）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_memory(model, x, use_checkpoint=False):\n",
        "    \"\"\"测量前向+反向传播的显存占用\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return None\n",
        "    \n",
        "    device = torch.device('cuda')\n",
        "    model = model.to(device)\n",
        "    x = x.to(device).requires_grad_(True)\n",
        "    \n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # 前向传播\n",
        "    out = model(x)\n",
        "    \n",
        "    # 反向传播\n",
        "    loss = out.sum()\n",
        "    loss.backward()\n",
        "    \n",
        "    peak_memory = torch.cuda.max_memory_allocated()\n",
        "    \n",
        "    # 清理\n",
        "    del out, loss, x\n",
        "    model.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return peak_memory\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"显存占用对比 (前向+反向)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"{'序列长度':<12} {'标准Attention':<20} {'With Checkpoint':<20} {'节省比例':<15}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    d_model = 768\n",
        "    num_heads = 12\n",
        "    batch_size = 1\n",
        "    \n",
        "    for seq_len in [256, 512, 1024]:\n",
        "        try:\n",
        "            x = torch.randn(batch_size, seq_len, d_model)\n",
        "            \n",
        "            # 标准Attention\n",
        "            model_normal = AttentionLayer(d_model, num_heads)\n",
        "            mem_normal = measure_memory(model_normal, x.clone())\n",
        "            \n",
        "            # With Checkpoint\n",
        "            model_ckpt = AttentionWithCheckpoint(d_model, num_heads)\n",
        "            mem_ckpt = measure_memory(model_ckpt, x.clone())\n",
        "            \n",
        "            if mem_normal and mem_ckpt:\n",
        "                saving = (mem_normal - mem_ckpt) / mem_normal * 100\n",
        "                print(f\"{seq_len:<12} {mem_normal/1e6:<20.2f} MB {mem_ckpt/1e6:<20.2f} MB {saving:<15.1f}%\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"{seq_len:<12} OOM\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"CUDA不可用，跳过显存测量\")\n",
        "    print(\"\\n理论分析：使用Checkpoint可以节省S和P矩阵的存储，约减少50-80%的激活值显存\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. FlashAttention的保存策略模拟\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_save_strategies(seq_len, d=64, batch_size=1, num_heads=12):\n",
        "    \"\"\"对比不同保存策略的内存占用\"\"\"\n",
        "    bytes_per_element = 2  # FP16\n",
        "    \n",
        "    # 标准Attention保存的数据\n",
        "    standard_save = {\n",
        "        'Q': batch_size * num_heads * seq_len * d,\n",
        "        'K': batch_size * num_heads * seq_len * d,\n",
        "        'V': batch_size * num_heads * seq_len * d,\n",
        "        'S': batch_size * num_heads * seq_len * seq_len,\n",
        "        'P': batch_size * num_heads * seq_len * seq_len,\n",
        "    }\n",
        "    \n",
        "    # FlashAttention保存的数据\n",
        "    flash_save = {\n",
        "        'Q': batch_size * num_heads * seq_len * d,\n",
        "        'K': batch_size * num_heads * seq_len * d,\n",
        "        'V': batch_size * num_heads * seq_len * d,\n",
        "        'O': batch_size * num_heads * seq_len * d,\n",
        "        'm (row max)': batch_size * num_heads * seq_len,\n",
        "        'l (row sum)': batch_size * num_heads * seq_len,\n",
        "    }\n",
        "    \n",
        "    standard_total = sum(standard_save.values()) * bytes_per_element\n",
        "    flash_total = sum(flash_save.values()) * bytes_per_element\n",
        "    \n",
        "    return standard_save, flash_save, standard_total, flash_total\n",
        "\n",
        "# 可视化对比\n",
        "print(\"保存策略对比分析\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "seq_lengths = [512, 1024, 2048, 4096, 8192]\n",
        "standard_mem = []\n",
        "flash_mem = []\n",
        "\n",
        "for seq_len in seq_lengths:\n",
        "    _, _, std_total, flash_total = compare_save_strategies(seq_len)\n",
        "    standard_mem.append(std_total / 1e6)\n",
        "    flash_mem.append(flash_total / 1e6)\n",
        "    \n",
        "    reduction = (std_total - flash_total) / std_total * 100\n",
        "    print(f\"N={seq_len:<5}: 标准={std_total/1e6:>8.1f}MB, Flash={flash_total/1e6:>8.1f}MB, 减少{reduction:.1f}%\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 可视化保存策略对比\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 左图: 绝对内存占用\n",
        "ax1 = axes[0]\n",
        "x = np.arange(len(seq_lengths))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, standard_mem, width, label='标准Attention', color='coral')\n",
        "bars2 = ax1.bar(x + width/2, flash_mem, width, label='FlashAttention', color='steelblue')\n",
        "\n",
        "ax1.set_xlabel('序列长度')\n",
        "ax1.set_ylabel('保存的激活值大小 (MB)')\n",
        "ax1.set_title('反向传播需要保存的数据量')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels([str(s) for s in seq_lengths])\n",
        "ax1.legend()\n",
        "ax1.set_yscale('log')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 右图: 详细分解 (以N=2048为例)\n",
        "ax2 = axes[1]\n",
        "\n",
        "std_save, flash_save, _, _ = compare_save_strategies(2048)\n",
        "bytes_per_element = 2\n",
        "\n",
        "# 标准Attention的分解\n",
        "std_labels = list(std_save.keys())\n",
        "std_sizes = [v * bytes_per_element / 1e6 for v in std_save.values()]\n",
        "\n",
        "# FlashAttention的分解\n",
        "flash_labels = list(flash_save.keys())\n",
        "flash_sizes = [v * bytes_per_element / 1e6 for v in flash_save.values()]\n",
        "\n",
        "# 绘制堆叠柱状图\n",
        "x_pos = [0, 1]\n",
        "colors_std = plt.cm.Reds(np.linspace(0.3, 0.7, len(std_labels)))\n",
        "colors_flash = plt.cm.Blues(np.linspace(0.3, 0.7, len(flash_labels)))\n",
        "\n",
        "bottom_std = 0\n",
        "for i, (label, size) in enumerate(zip(std_labels, std_sizes)):\n",
        "    ax2.bar(0, size, bottom=bottom_std, color=colors_std[i], label=f'Std: {label}')\n",
        "    bottom_std += size\n",
        "\n",
        "bottom_flash = 0\n",
        "for i, (label, size) in enumerate(zip(flash_labels, flash_sizes)):\n",
        "    ax2.bar(1, size, bottom=bottom_flash, color=colors_flash[i], label=f'FA: {label}')\n",
        "    bottom_flash += size\n",
        "\n",
        "ax2.set_xticks([0, 1])\n",
        "ax2.set_xticklabels(['标准Attention', 'FlashAttention'])\n",
        "ax2.set_ylabel('保存的数据量 (MB)')\n",
        "ax2.set_title('N=2048时的保存数据分解')\n",
        "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 时间vs空间权衡分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_time_space_tradeoff(seq_len, d=64, batch_size=1, num_heads=12,\n",
        "                                 peak_tflops=312, memory_bw_tbs=2.0):\n",
        "    \"\"\"分析时间vs空间的权衡\"\"\"\n",
        "    bytes_per_element = 2\n",
        "    \n",
        "    # 计算量 (FLOPs)\n",
        "    fwd_flops = batch_size * num_heads * (4 * seq_len * seq_len * d + 5 * seq_len * seq_len)\n",
        "    \n",
        "    # 标准方式\n",
        "    std_save_bytes = batch_size * num_heads * (3 * seq_len * d + 2 * seq_len * seq_len) * bytes_per_element\n",
        "    std_bwd_time = fwd_flops / (peak_tflops * 1e12)  # 只计算部分\n",
        "    \n",
        "    # FlashAttention方式\n",
        "    flash_save_bytes = batch_size * num_heads * (4 * seq_len * d + 2 * seq_len) * bytes_per_element\n",
        "    flash_bwd_time = 2 * fwd_flops / (peak_tflops * 1e12)  # 需要重新计算前向\n",
        "    \n",
        "    # 额外时间\n",
        "    extra_time = flash_bwd_time - std_bwd_time\n",
        "    \n",
        "    # 节省内存\n",
        "    saved_memory = std_save_bytes - flash_save_bytes\n",
        "    \n",
        "    return {\n",
        "        'std_memory_mb': std_save_bytes / 1e6,\n",
        "        'flash_memory_mb': flash_save_bytes / 1e6,\n",
        "        'saved_memory_mb': saved_memory / 1e6,\n",
        "        'extra_time_us': extra_time * 1e6,\n",
        "        'memory_saving_ratio': saved_memory / std_save_bytes * 100,\n",
        "    }\n",
        "\n",
        "print(\"时间vs空间权衡分析\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'序列长度':<10} {'标准内存':<15} {'Flash内存':<15} {'节省内存':<15} {'额外时间':<15}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for seq_len in [512, 1024, 2048, 4096]:\n",
        "    result = analyze_time_space_tradeoff(seq_len)\n",
        "    print(f\"{seq_len:<10} {result['std_memory_mb']:<15.1f}MB \"\n",
        "          f\"{result['flash_memory_mb']:<15.1f}MB \"\n",
        "          f\"{result['saved_memory_mb']:<15.1f}MB \"\n",
        "          f\"{result['extra_time_us']:<15.1f}μs\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\\n结论: FlashAttention用少量额外计算时间换取大量内存节省\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "\n",
        "1. **反向传播的内存需求**\n",
        "   - 需要保存前向传播的中间结果\n",
        "   - S和P矩阵占据大部分激活值内存\n",
        "\n",
        "2. **PyTorch Checkpoint机制**\n",
        "   - 使用`checkpoint`函数包装需要重计算的部分\n",
        "   - 反向时自动重新执行前向计算\n",
        "\n",
        "3. **FlashAttention的保存策略**\n",
        "   - 只保存Q, K, V, O, m, l\n",
        "   - 不保存S和P矩阵\n",
        "   - 显存从O(N²)降到O(N)\n",
        "\n",
        "4. **时间vs空间权衡**\n",
        "   - 用额外的计算时间换取大量内存节省\n",
        "   - 由于Attention是内存绑定的，实际总时间可能反而更快\n",
        "\n",
        "## 章节总结\n",
        "\n",
        "恭喜你完成了FlashAttention核心思想的学习！你现在应该理解：\n",
        "- 为什么标准Attention有内存瓶颈\n",
        "- IO-Aware算法设计的重要性\n",
        "- Tiling如何解决大矩阵问题\n",
        "- Recomputation如何节省显存\n",
        "\n",
        "下一步可以学习**Online Softmax算法**，这是FlashAttention的数学核心。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
