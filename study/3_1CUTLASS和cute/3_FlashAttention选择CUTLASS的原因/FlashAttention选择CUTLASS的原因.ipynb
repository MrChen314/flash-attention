{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FlashAttentioné€‰æ‹©CUTLASSçš„åŸå›  - å®è·µ\n",
        "\n",
        "> é€šè¿‡å®éªŒå¯¹æ¯”ç†è§£ä¸ºä»€ä¹ˆFlashAttentionéœ€è¦CUTLASS\n",
        "\n",
        "---\n",
        "\n",
        "æœ¬notebooké€šè¿‡å¯¹æ¯”å®éªŒå¸®åŠ©ä½ ç†è§£ï¼š\n",
        "1. cuBLASæ–¹æ¡ˆçš„å±€é™æ€§\n",
        "2. æ“ä½œèåˆçš„é‡è¦æ€§\n",
        "3. CUTLASSæä¾›çš„å…³é”®èƒ½åŠ›\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç¯å¢ƒå‡†å¤‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"ç¯å¢ƒä¿¡æ¯\")\n",
        "print(\"=\"*50)\n",
        "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f\"GPU: {props.name}\")\n",
        "    print(f\"æ˜¾å­˜: {props.total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. cuBLASæ–¹æ¡ˆçš„é—®é¢˜ï¼šå†…å­˜å ç”¨\n",
        "\n",
        "æ ‡å‡†Attentionä½¿ç”¨cuBLASéœ€è¦å­˜å‚¨å®Œæ•´çš„NÃ—Nä¸­é—´çŸ©é˜µã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_attention_memory(batch_size, num_heads, seq_len, head_dim, dtype_bytes=2):\n",
        "    \"\"\"\n",
        "    è®¡ç®—æ ‡å‡†Attentionçš„å†…å­˜å ç”¨\n",
        "    \n",
        "    éœ€è¦å­˜å‚¨:\n",
        "    - Q, K, V: å„ batch * heads * seq_len * head_dim\n",
        "    - S = QK^T: batch * heads * seq_len * seq_len\n",
        "    - P = softmax(S): batch * heads * seq_len * seq_len\n",
        "    - O: batch * heads * seq_len * head_dim\n",
        "    \"\"\"\n",
        "    # è¾“å…¥è¾“å‡ºå¼ é‡\n",
        "    qkv_memory = 3 * batch_size * num_heads * seq_len * head_dim * dtype_bytes\n",
        "    o_memory = batch_size * num_heads * seq_len * head_dim * dtype_bytes\n",
        "    \n",
        "    # ä¸­é—´çŸ©é˜µ S å’Œ P (O(NÂ²))\n",
        "    s_memory = batch_size * num_heads * seq_len * seq_len * dtype_bytes\n",
        "    p_memory = batch_size * num_heads * seq_len * seq_len * dtype_bytes\n",
        "    \n",
        "    return {\n",
        "        'QKV': qkv_memory / 1e9,\n",
        "        'O': o_memory / 1e9,\n",
        "        'S (NÃ—N)': s_memory / 1e9,\n",
        "        'P (NÃ—N)': p_memory / 1e9,\n",
        "        'Total': (qkv_memory + o_memory + s_memory + p_memory) / 1e9\n",
        "    }\n",
        "\n",
        "print(\"æ ‡å‡†Attentionå†…å­˜å ç”¨åˆ†æ (cuBLASæ–¹æ¡ˆ)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"å‚æ•°: batch=1, heads=12, head_dim=64, dtype=FP16\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'åºåˆ—é•¿åº¦':<12} {'QKV (GB)':<12} {'S+P (GB)':<12} {'æ€»è®¡ (GB)':<12} {'S+På æ¯”':<12}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for seq_len in [512, 1024, 2048, 4096, 8192, 16384]:\n",
        "    mem = compute_attention_memory(1, 12, seq_len, 64)\n",
        "    sp_total = mem['S (NÃ—N)'] + mem['P (NÃ—N)']\n",
        "    sp_ratio = sp_total / mem['Total'] * 100\n",
        "    print(f\"{seq_len:<12} {mem['QKV']:<12.3f} {sp_total:<12.3f} {mem['Total']:<12.3f} {sp_ratio:<12.1f}%\")\n",
        "\n",
        "print(\"\\nç»“è®º: ä¸­é—´çŸ©é˜µSå’ŒPéšåºåˆ—é•¿åº¦å‘ˆO(NÂ²)å¢é•¿ï¼Œå¾ˆå¿«æˆä¸ºç“¶é¢ˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. cuBLASæ–¹æ¡ˆçš„é—®é¢˜ï¼šHBMè®¿é—®\n",
        "\n",
        "åˆ†ææ ‡å‡†Attentionå„æ­¥éª¤çš„HBMï¼ˆé«˜å¸¦å®½å†…å­˜ï¼‰è®¿é—®é‡ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_hbm_access(N, d, dtype_bytes=2):\n",
        "    \"\"\"\n",
        "    åˆ†ææ ‡å‡†Attention vs FlashAttentionçš„HBMè®¿é—®é‡\n",
        "    \n",
        "    æ ‡å‡†Attention (cuBLAS):\n",
        "    - Step 1: è¯»Q,K â†’ å†™S\n",
        "    - Step 2: è¯»S â†’ å†™P\n",
        "    - Step 3: è¯»P,V â†’ å†™O\n",
        "    \n",
        "    FlashAttention:\n",
        "    - è¯»Q,K,V â†’ å†™O (åˆ†å—å¤„ç†ï¼Œä¸å­˜å‚¨Så’ŒP)\n",
        "    \"\"\"\n",
        "    # æ ‡å‡†Attention\n",
        "    step1_read = 2 * N * d * dtype_bytes  # Q, K\n",
        "    step1_write = N * N * dtype_bytes     # S\n",
        "    step2_read = N * N * dtype_bytes      # S\n",
        "    step2_write = N * N * dtype_bytes     # P\n",
        "    step3_read = N * N * dtype_bytes + N * d * dtype_bytes  # P, V\n",
        "    step3_write = N * d * dtype_bytes     # O\n",
        "    \n",
        "    standard_total = step1_read + step1_write + step2_read + step2_write + step3_read + step3_write\n",
        "    \n",
        "    # FlashAttention\n",
        "    flash_read = 3 * N * d * dtype_bytes  # Q, K, V\n",
        "    flash_write = N * d * dtype_bytes      # O\n",
        "    flash_total = flash_read + flash_write\n",
        "    \n",
        "    return {\n",
        "        'standard': {\n",
        "            'step1': (step1_read + step1_write) / 1e6,\n",
        "            'step2': (step2_read + step2_write) / 1e6,\n",
        "            'step3': (step3_read + step3_write) / 1e6,\n",
        "            'total': standard_total / 1e6\n",
        "        },\n",
        "        'flash': {\n",
        "            'total': flash_total / 1e6\n",
        "        },\n",
        "        'reduction': standard_total / flash_total\n",
        "    }\n",
        "\n",
        "print(\"HBMè®¿é—®é‡åˆ†æ\")\n",
        "print(\"=\"*70)\n",
        "print(f\"å‚æ•°: head_dim=64, dtype=FP16\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'åºåˆ—é•¿åº¦':<12} {'æ ‡å‡† (MB)':<15} {'Flash (MB)':<15} {'å‡å°‘å€æ•°':<12}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for N in [512, 1024, 2048, 4096, 8192]:\n",
        "    result = analyze_hbm_access(N, 64)\n",
        "    print(f\"{N:<12} {result['standard']['total']:<15.1f} {result['flash']['total']:<15.1f} {result['reduction']:<12.1f}x\")\n",
        "\n",
        "print(\"\\nç»“è®º: FlashAttentioné€šè¿‡é¿å…å­˜å‚¨ä¸­é—´çŸ©é˜µï¼Œå¤§å¹…å‡å°‘HBMè®¿é—®ï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. å®éªŒï¼šæ ‡å‡†Attention vs FlashAttention (PyTorch)\n",
        "\n",
        "ä½¿ç”¨PyTorchå¯¹æ¯”æ ‡å‡†å®ç°å’ŒFlashAttentionçš„æ€§èƒ½ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_attention(Q, K, V, scale=None):\n",
        "    \"\"\"\n",
        "    æ ‡å‡†Attentionå®ç°ï¼ˆä½¿ç”¨cuBLASï¼‰\n",
        "    æ¨¡æ‹Ÿæ²¡æœ‰èåˆçš„æƒ…å†µ\n",
        "    \"\"\"\n",
        "    d = Q.shape[-1]\n",
        "    if scale is None:\n",
        "        scale = 1.0 / (d ** 0.5)\n",
        "    \n",
        "    # Step 1: S = QK^T\n",
        "    S = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
        "    \n",
        "    # Step 2: P = softmax(S)\n",
        "    P = torch.softmax(S, dim=-1)\n",
        "    \n",
        "    # Step 3: O = PV\n",
        "    O = torch.matmul(P, V)\n",
        "    \n",
        "    return O\n",
        "\n",
        "def benchmark_attention(batch, heads, seq_len, head_dim, warmup=5, iterations=20):\n",
        "    \"\"\"\n",
        "    å¯¹æ¯”æ ‡å‡†Attentionå’ŒFlashAttentionçš„æ€§èƒ½\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    dtype = torch.float16\n",
        "    \n",
        "    # åˆ›å»ºè¾“å…¥\n",
        "    Q = torch.randn(batch, heads, seq_len, head_dim, dtype=dtype, device=device)\n",
        "    K = torch.randn(batch, heads, seq_len, head_dim, dtype=dtype, device=device)\n",
        "    V = torch.randn(batch, heads, seq_len, head_dim, dtype=dtype, device=device)\n",
        "    \n",
        "    # é¢„çƒ­\n",
        "    for _ in range(warmup):\n",
        "        O_std = standard_attention(Q, K, V)\n",
        "        if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "            O_flash = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
        "    \n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    # æµ‹è¯•æ ‡å‡†Attention\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(iterations):\n",
        "        O_std = standard_attention(Q, K, V)\n",
        "    torch.cuda.synchronize()\n",
        "    std_time = (time.perf_counter() - start) / iterations * 1000\n",
        "    \n",
        "    # æµ‹è¯•FlashAttention (PyTorch 2.0+ SDPA)\n",
        "    flash_time = None\n",
        "    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "        start = time.perf_counter()\n",
        "        for _ in range(iterations):\n",
        "            O_flash = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
        "        torch.cuda.synchronize()\n",
        "        flash_time = (time.perf_counter() - start) / iterations * 1000\n",
        "    \n",
        "    return std_time, flash_time\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Attentionæ€§èƒ½å¯¹æ¯”\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"å‚æ•°: batch=1, heads=12, head_dim=64\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'åºåˆ—é•¿åº¦':<12} {'æ ‡å‡† (ms)':<15} {'SDPA (ms)':<15} {'åŠ é€Ÿæ¯”':<12}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    for seq_len in [256, 512, 1024, 2048]:\n",
        "        try:\n",
        "            std_time, flash_time = benchmark_attention(1, 12, seq_len, 64)\n",
        "            if flash_time:\n",
        "                speedup = std_time / flash_time\n",
        "                print(f\"{seq_len:<12} {std_time:<15.3f} {flash_time:<15.3f} {speedup:<12.2f}x\")\n",
        "            else:\n",
        "                print(f\"{seq_len:<12} {std_time:<15.3f} {'N/A':<15} {'N/A':<12}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{seq_len:<12} Error: {e}\")\n",
        "    \n",
        "    print(\"\\næ³¨æ„: SDPAæ˜¯PyTorch 2.0+å†…ç½®çš„ä¼˜åŒ–Attentionå®ç°\")\n",
        "    print(\"å®é™…çš„FlashAttentionåº“å¯èƒ½æœ‰æ›´å¥½çš„æ€§èƒ½\")\n",
        "else:\n",
        "    print(\"éœ€è¦CUDAç¯å¢ƒè¿è¡Œæ­¤æµ‹è¯•\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. æ“ä½œèåˆçš„é‡è¦æ€§\n",
        "\n",
        "ä¸ºä»€ä¹ˆæ“ä½œèåˆï¼ˆKernel Fusionï¼‰å¯¹FlashAttentionè‡³å…³é‡è¦ï¼Ÿ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_kernel_fusion():\n",
        "    \"\"\"\n",
        "    å¯è§†åŒ–æ“ä½œèåˆçš„æ¦‚å¿µ\n",
        "    \"\"\"\n",
        "    unfused = \"\"\"\n",
        "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "    â•‘                      æ²¡æœ‰èåˆ (cuBLASæ–¹æ¡ˆ)                               â•‘\n",
        "    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘   Kernel 1: GEMM           Kernel 2: Softmax        Kernel 3: GEMM      â•‘\n",
        "    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â•‘\n",
        "    â•‘   â”‚  S = Q@K^T  â”‚ â”€â”€HBMâ†’  â”‚ P = softmax â”‚ â”€â”€HBMâ†’  â”‚   O = P@V   â”‚       â•‘\n",
        "    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â•‘\n",
        "    â•‘          â”‚                       â”‚                       â”‚               â•‘\n",
        "    â•‘          â†“                       â†“                       â†“               â•‘\n",
        "    â•‘      å†™Såˆ°HBM               å†™Påˆ°HBM                å†™Oåˆ°HBM            â•‘\n",
        "    â•‘      (NÃ—NçŸ©é˜µ)              (NÃ—NçŸ©é˜µ)               (NÃ—dçŸ©é˜µ)            â•‘\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘   é—®é¢˜:                                                                  â•‘\n",
        "    â•‘   - æ¯ä¸ªkerneléƒ½è¦è¯»å†™HBM                                               â•‘\n",
        "    â•‘   - ä¸­é—´ç»“æœSå’ŒPéœ€è¦O(NÂ²)å­˜å‚¨                                           â•‘\n",
        "    â•‘   - Kernelå¯åŠ¨å¼€é”€                                                       â•‘\n",
        "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    \"\"\"\n",
        "    \n",
        "    fused = \"\"\"\n",
        "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "    â•‘                      èåˆKernel (FlashAttention)                         â•‘\n",
        "    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘   å•ä¸ªèåˆKernel:                                                        â•‘\n",
        "    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘\n",
        "    â•‘   â”‚                                                                   â”‚  â•‘\n",
        "    â•‘   â”‚   ä»HBMè¯»Qå—   ä»HBMè¯»Kå—   ä»HBMè¯»Vå—                            â”‚  â•‘\n",
        "    â•‘   â”‚       â†“            â†“            â†“                                 â”‚  â•‘\n",
        "    â•‘   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â•‘\n",
        "    â•‘   â”‚   â”‚                    SRAM (å…±äº«å†…å­˜)                       â”‚    â”‚  â•‘\n",
        "    â•‘   â”‚   â”‚   Q_block  K_block  V_block                             â”‚    â”‚  â•‘\n",
        "    â•‘   â”‚   â”‚      â†“        â†“        â†“                                â”‚    â”‚  â•‘\n",
        "    â•‘   â”‚   â”‚   S = Q@K^T â†’ softmax â†’ P@V  (å…¨éƒ¨åœ¨SRAMä¸­!)            â”‚    â”‚  â•‘\n",
        "    â•‘   â”‚   â”‚                 â†“                                        â”‚    â”‚  â•‘\n",
        "    â•‘   â”‚   â”‚            ç´¯åŠ åˆ°O                                       â”‚    â”‚  â•‘\n",
        "    â•‘   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â•‘\n",
        "    â•‘   â”‚                         â†“                                         â”‚  â•‘\n",
        "    â•‘   â”‚                   å†™Oåˆ°HBM                                        â”‚  â•‘\n",
        "    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘   ä¼˜åŠ¿:                                                                  â•‘\n",
        "    â•‘   - ä¸­é—´ç»“æœç•™åœ¨SRAMï¼Œä¸å†™å›HBM                                         â•‘\n",
        "    â•‘   - åªæœ‰ä¸€ä¸ªKernelï¼Œæ— å¯åŠ¨å¼€é”€                                          â•‘\n",
        "    â•‘   - HBMè®¿é—®ä»O(NÂ²)é™åˆ°O(Nd)                                            â•‘\n",
        "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    \"\"\"\n",
        "    \n",
        "    print(unfused)\n",
        "    print(fused)\n",
        "\n",
        "visualize_kernel_fusion()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. CUTLASSçš„å…³é”®èƒ½åŠ›\n",
        "\n",
        "åˆ†æCUTLASSä¸ºFlashAttentionæä¾›çš„å…³é”®èƒ½åŠ›ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_cutlass_capabilities():\n",
        "    \"\"\"\n",
        "    æ€»ç»“CUTLASSä¸ºFlashAttentionæä¾›çš„å…³é”®èƒ½åŠ›\n",
        "    \"\"\"\n",
        "    summary = \"\"\"\n",
        "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "    â•‘                    CUTLASSä¸ºFlashAttentionæä¾›çš„å…³é”®èƒ½åŠ›                 â•‘\n",
        "    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘  1. TiledMMA - Tensor Coreå°è£…                                          â•‘\n",
        "    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘\n",
        "    â•‘  â€¢ å°è£…16x8x16ç­‰MMAæŒ‡ä»¤                                                 â•‘\n",
        "    â•‘  â€¢ è‡ªåŠ¨å¤„ç†çº¿ç¨‹åˆ°æ•°æ®çš„æ˜ å°„                                              â•‘\n",
        "    â•‘  â€¢ æ”¯æŒFP16, BF16, FP8ç­‰ç²¾åº¦                                            â•‘\n",
        "    â•‘  â€¢ FlashAttentionç”¨äº: QK^Tå’ŒPVçš„çŸ©é˜µä¹˜                                 â•‘\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘  2. TiledCopy - é«˜æ•ˆå†…å­˜æ‹·è´                                            â•‘\n",
        "    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘\n",
        "    â•‘  â€¢ æ”¯æŒcp.asyncå¼‚æ­¥æ‹·è´                                                 â•‘\n",
        "    â•‘  â€¢ è‡ªåŠ¨å¤„ç†bank conflict                                                â•‘\n",
        "    â•‘  â€¢ æ”¯æŒ128-bitå‘é‡åŠ è½½                                                  â•‘\n",
        "    â•‘  â€¢ FlashAttentionç”¨äº: HBMâ†’SRAMçš„Q,K,VåŠ è½½                             â•‘\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘  3. Layout - çµæ´»çš„å†…å­˜å¸ƒå±€                                             â•‘\n",
        "    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘\n",
        "    â•‘  â€¢ æ”¯æŒå¤æ‚çš„swizzleæ¨¡å¼                                                â•‘\n",
        "    â•‘  â€¢ é¿å…å…±äº«å†…å­˜bank conflict                                            â•‘\n",
        "    â•‘  â€¢ ç¼–è¯‘æœŸå¸ƒå±€è®¡ç®—ï¼Œé›¶è¿è¡Œæ—¶å¼€é”€                                         â•‘\n",
        "    â•‘  â€¢ FlashAttentionç”¨äº: å…±äº«å†…å­˜å¸ƒå±€ä¼˜åŒ–                                 â•‘\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘  4. è½¯ä»¶æµæ°´çº¿æ”¯æŒ                                                      â•‘\n",
        "    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘\n",
        "    â•‘  â€¢ æ”¯æŒå¤šçº§æµæ°´çº¿                                                       â•‘\n",
        "    â•‘  â€¢ é‡å è®¡ç®—å’Œå†…å­˜è®¿é—®                                                    â•‘\n",
        "    â•‘  â€¢ FlashAttentionç”¨äº: éšè—HBMè®¿é—®å»¶è¿Ÿ                                  â•‘\n",
        "    â•‘                                                                          â•‘\n",
        "    â•‘  5. è‡ªå®šä¹‰Epilogue                                                      â•‘\n",
        "    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘\n",
        "    â•‘  â€¢ å¯åœ¨GEMMåèåˆä»»æ„æ“ä½œ                                               â•‘\n",
        "    â•‘  â€¢ FlashAttentionç”¨äº: èåˆsoftmaxè®¡ç®—                                  â•‘\n",
        "    â•‘                                                                          â•‘\n",
        "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    \"\"\"\n",
        "    print(summary)\n",
        "\n",
        "summarize_cutlass_capabilities()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. å„æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_approaches():\n",
        "    \"\"\"\n",
        "    å¯¹æ¯”ä¸åŒå®ç°æ–¹æ¡ˆ\n",
        "    \"\"\"\n",
        "    comparison = \"\"\"\n",
        "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "    â•‘                              FlashAttentionå®ç°æ–¹æ¡ˆå¯¹æ¯”                                â•‘\n",
        "    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "    â•‘                                                                                        â•‘\n",
        "    â•‘  ç‰¹æ€§                 â”‚ cuBLAS     â”‚ è£¸å†™CUDA   â”‚ Triton     â”‚ CUTLASS    â”‚          â•‘\n",
        "    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â•‘\n",
        "    â•‘  æ“ä½œèåˆ             â”‚ âœ— ä¸æ”¯æŒ   â”‚ âœ“ å¯ä»¥     â”‚ âœ“ å¯ä»¥     â”‚ âœ“ å¯ä»¥     â”‚          â•‘\n",
        "    â•‘  Tensor Core          â”‚ âœ“ è‡ªåŠ¨     â”‚ â–³ éœ€æ‰‹åŠ¨   â”‚ âœ“ è‡ªåŠ¨     â”‚ âœ“ è‡ªåŠ¨     â”‚          â•‘\n",
        "    â•‘  å¼‚æ­¥æ‹·è´             â”‚ âœ— ä¸æ”¯æŒ   â”‚ â–³ éœ€æ‰‹åŠ¨   â”‚ â–³ æœ‰é™     â”‚ âœ“ æ”¯æŒ     â”‚          â•‘\n",
        "    â•‘  è‡ªå®šä¹‰åˆ†å—           â”‚ âœ— ä¸æ”¯æŒ   â”‚ âœ“ å®Œå…¨     â”‚ âœ“ æ”¯æŒ     â”‚ âœ“ æ”¯æŒ     â”‚          â•‘\n",
        "    â•‘  å¼€å‘æ•ˆç‡             â”‚ â­â­â­â­â­  â”‚ â­          â”‚ â­â­â­â­    â”‚ â­â­â­      â”‚          â•‘\n",
        "    â•‘  æ€§èƒ½ä¸Šé™             â”‚ â­â­â­      â”‚ â­â­â­â­â­  â”‚ â­â­â­      â”‚ â­â­â­â­â­  â”‚          â•‘\n",
        "    â•‘  ä»£ç å¯ç»´æŠ¤æ€§         â”‚ â­â­â­â­â­  â”‚ â­          â”‚ â­â­â­â­    â”‚ â­â­â­      â”‚          â•‘\n",
        "    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â•‘\n",
        "    â•‘                                                                                        â•‘\n",
        "    â•‘  ç»“è®º: CUTLASSæ˜¯æ€§èƒ½ä¸å¼€å‘æ•ˆç‡çš„æœ€ä½³å¹³è¡¡ç‚¹ï¼                                          â•‘\n",
        "    â•‘                                                                                        â•‘\n",
        "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    \"\"\"\n",
        "    print(comparison)\n",
        "\n",
        "compare_approaches()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. æ£€æµ‹FlashAttentionæ˜¯å¦å¯ç”¨\n",
        "\n",
        "æ£€æŸ¥å½“å‰ç¯å¢ƒä¸­FlashAttentionçš„å¯ç”¨æ€§ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_flash_attention_availability():\n",
        "    \"\"\"\n",
        "    æ£€æŸ¥FlashAttentionçš„å¯ç”¨æ€§\n",
        "    \"\"\"\n",
        "    print(\"FlashAttentionå¯ç”¨æ€§æ£€æŸ¥\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # 1. PyTorch SDPA\n",
        "    print(\"\\n1. PyTorch Scaled Dot Product Attention:\")\n",
        "    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "        print(\"   âœ“ å¯ç”¨ (PyTorch 2.0+)\")\n",
        "    else:\n",
        "        print(\"   âœ— ä¸å¯ç”¨ (éœ€è¦PyTorch 2.0+)\")\n",
        "    \n",
        "    # 2. flash_attnåº“\n",
        "    print(\"\\n2. flash-attnåº“:\")\n",
        "    try:\n",
        "        import flash_attn\n",
        "        print(f\"   âœ“ å¯ç”¨ (ç‰ˆæœ¬: {flash_attn.__version__})\")\n",
        "    except ImportError:\n",
        "        print(\"   âœ— æœªå®‰è£… (pip install flash-attn)\")\n",
        "    \n",
        "    # 3. GPUæ”¯æŒ\n",
        "    print(\"\\n3. GPUæ”¯æŒ:\")\n",
        "    if torch.cuda.is_available():\n",
        "        props = torch.cuda.get_device_properties(0)\n",
        "        if props.major >= 8:\n",
        "            print(f\"   âœ“ {props.name} (SM{props.major}{props.minor}) - å®Œæ•´æ”¯æŒ\")\n",
        "        elif props.major >= 7:\n",
        "            print(f\"   â–³ {props.name} (SM{props.major}{props.minor}) - éƒ¨åˆ†æ”¯æŒ\")\n",
        "        else:\n",
        "            print(f\"   âœ— {props.name} (SM{props.major}{props.minor}) - ä¸æ”¯æŒTensor Core\")\n",
        "    else:\n",
        "        print(\"   âœ— æ— CUDA GPU\")\n",
        "\n",
        "check_flash_attention_availability()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. æ€»ç»“\n",
        "\n",
        "é€šè¿‡æœ¬notebookï¼Œæˆ‘ä»¬ç†è§£äº†ï¼š\n",
        "\n",
        "1. **cuBLASçš„å±€é™æ€§**ï¼šæ— æ³•èåˆæ“ä½œï¼Œéœ€è¦å­˜å‚¨O(NÂ²)ä¸­é—´ç»“æœ\n",
        "2. **HBMè®¿é—®ä¼˜åŒ–**ï¼šFlashAttentionå°†HBMè®¿é—®ä»O(NÂ²)é™åˆ°O(Nd)\n",
        "3. **æ“ä½œèåˆçš„é‡è¦æ€§**ï¼šå°†å¤šä¸ªæ“ä½œåˆå¹¶åˆ°ä¸€ä¸ªkernelä¸­\n",
        "4. **CUTLASSçš„å…³é”®èƒ½åŠ›**ï¼šTiledMMAã€TiledCopyã€å¼‚æ­¥æ‹·è´ã€è‡ªå®šä¹‰Epilogue\n",
        "5. **æ–¹æ¡ˆé€‰æ‹©**ï¼šCUTLASSæ˜¯æ€§èƒ½ä¸å¼€å‘æ•ˆç‡çš„æœ€ä½³å¹³è¡¡\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š å»¶ä¼¸é˜…è¯»\n",
        "\n",
        "- [FlashAttentionè®ºæ–‡](https://arxiv.org/abs/2205.14135)\n",
        "- [FlashAttention-2è®ºæ–‡](https://arxiv.org/abs/2307.08691)\n",
        "- [FlashAttention GitHub](https://github.com/Dao-AILab/flash-attention)\n",
        "- [CUTLASS GitHub](https://github.com/NVIDIA/cutlass)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
