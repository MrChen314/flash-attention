{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CUTLASSæ¦‚è¿° - å®è·µ\n",
        "\n",
        "> é€šè¿‡å®é™…ä»£ç ç†è§£CUTLASSå’ŒTensor Core\n",
        "\n",
        "---\n",
        "\n",
        "æœ¬notebookå°†é€šè¿‡Pythonä»£ç å¸®åŠ©ä½ ç†è§£ï¼š\n",
        "1. ä¸ºä»€ä¹ˆçŸ©é˜µä¹˜æ³•éœ€è¦ä¼˜åŒ–\n",
        "2. Tensor Coreçš„åŠ é€Ÿæ•ˆæœ\n",
        "3. CUTLASSç›¸æ¯”äºcuBLASçš„çµæ´»æ€§\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç¯å¢ƒæ£€æŸ¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"ç¯å¢ƒä¿¡æ¯\")\n",
        "print(\"=\"*50)\n",
        "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f\"GPU: {props.name}\")\n",
        "    print(f\"è®¡ç®—èƒ½åŠ›: SM{props.major}{props.minor}\")\n",
        "    print(f\"æ˜¾å­˜: {props.total_memory / 1e9:.1f} GB\")\n",
        "    \n",
        "    # æ£€æŸ¥Tensor Coreæ”¯æŒ\n",
        "    if props.major >= 7:\n",
        "        print(\"âœ“ æ”¯æŒTensor Core\")\n",
        "    else:\n",
        "        print(\"âœ— ä¸æ”¯æŒTensor Core (éœ€è¦Voltaæ¶æ„åŠä»¥ä¸Š)\")\n",
        "else:\n",
        "    print(\"è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDAï¼Œéƒ¨åˆ†å®éªŒæ— æ³•è¿è¡Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. çŸ©é˜µä¹˜æ³•æ€§èƒ½åˆ†æ\n",
        "\n",
        "è®©æˆ‘ä»¬é€šè¿‡æµ‹é‡çŸ©é˜µä¹˜æ³•çš„æ€§èƒ½æ¥ç†è§£ä¸ºä»€ä¹ˆéœ€è¦Tensor Coreã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_matmul(M, N, K, dtype=torch.float16, warmup=10, iterations=100):\n",
        "    \"\"\"\n",
        "    æµ‹é‡çŸ©é˜µä¹˜æ³•æ€§èƒ½\n",
        "    C[M,N] = A[M,K] @ B[K,N]\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    # åˆ›å»ºéšæœºçŸ©é˜µ\n",
        "    A = torch.randn(M, K, dtype=dtype, device=device)\n",
        "    B = torch.randn(K, N, dtype=dtype, device=device)\n",
        "    \n",
        "    # é¢„çƒ­\n",
        "    for _ in range(warmup):\n",
        "        C = torch.matmul(A, B)\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    # è®¡æ—¶\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(iterations):\n",
        "        C = torch.matmul(A, B)\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed = time.perf_counter() - start\n",
        "    \n",
        "    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡\n",
        "    avg_time_ms = (elapsed / iterations) * 1000\n",
        "    flops = 2 * M * N * K  # çŸ©é˜µä¹˜æ³•çš„FLOPs\n",
        "    tflops = (flops / (avg_time_ms / 1000)) / 1e12\n",
        "    \n",
        "    return avg_time_ms, tflops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯• (C = A @ B)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"{'Size (M=N=K)':<15} {'FP32 Time (ms)':<18} {'FP16 Time (ms)':<18} {'FP16 TFLOPS':<15}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    sizes = [512, 1024, 2048, 4096]\n",
        "    \n",
        "    for size in sizes:\n",
        "        # FP32 (ä¸ä½¿ç”¨Tensor Core)\n",
        "        time_fp32, _ = benchmark_matmul(size, size, size, dtype=torch.float32, iterations=50)\n",
        "        \n",
        "        # FP16 (ä½¿ç”¨Tensor Core)\n",
        "        time_fp16, tflops_fp16 = benchmark_matmul(size, size, size, dtype=torch.float16, iterations=50)\n",
        "        \n",
        "        speedup = time_fp32 / time_fp16\n",
        "        print(f\"{size:<15} {time_fp32:<18.3f} {time_fp16:<18.3f} {tflops_fp16:<15.1f}\")\n",
        "    \n",
        "    print(\"\\nè¯´æ˜:\")\n",
        "    print(\"- FP16è‡ªåŠ¨ä½¿ç”¨Tensor Core (åœ¨æ”¯æŒçš„GPUä¸Š)\")\n",
        "    print(\"- Tensor Coreå¯ä»¥å¸¦æ¥æ˜¾è‘—çš„åŠ é€Ÿ\")\n",
        "else:\n",
        "    print(\"éœ€è¦CUDAç¯å¢ƒè¿è¡Œæ­¤æµ‹è¯•\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ç†è§£è®¡ç®—å¼ºåº¦\n",
        "\n",
        "è®¡ç®—å¼ºåº¦ï¼ˆArithmetic Intensityï¼‰å†³å®šäº†ç®—æ³•æ˜¯å†…å­˜ç»‘å®šè¿˜æ˜¯è®¡ç®—ç»‘å®šã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_gemm_intensity(M, N, K, dtype_bytes=2):\n",
        "    \"\"\"\n",
        "    åˆ†æGEMMçš„è®¡ç®—å¼ºåº¦\n",
        "    \n",
        "    C[M,N] = A[M,K] @ B[K,N]\n",
        "    \n",
        "    FLOPs = 2 * M * N * K\n",
        "    Memory = (M*K + K*N + M*N) * dtype_bytes\n",
        "    \"\"\"\n",
        "    flops = 2 * M * N * K\n",
        "    memory_bytes = (M*K + K*N + M*N) * dtype_bytes\n",
        "    intensity = flops / memory_bytes\n",
        "    \n",
        "    return flops, memory_bytes, intensity\n",
        "\n",
        "print(\"GEMMè®¡ç®—å¼ºåº¦åˆ†æ\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'M=N=K':<10} {'FLOPs (G)':<15} {'Memory (MB)':<15} {'Intensity':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for size in [256, 512, 1024, 2048, 4096, 8192]:\n",
        "    flops, mem, intensity = analyze_gemm_intensity(size, size, size, dtype_bytes=2)\n",
        "    print(f\"{size:<10} {flops/1e9:<15.2f} {mem/1e6:<15.2f} {intensity:<15.1f}\")\n",
        "\n",
        "print(\"\\nè§£è¯»:\")\n",
        "print(\"- è®¡ç®—å¼ºåº¦ = FLOPs / Memoryè®¿é—®å­—èŠ‚æ•°\")\n",
        "print(\"- A100å¹³è¡¡ç‚¹çº¦ä¸º 156 (312 TFLOPS / 2 TB/s)\")\n",
        "print(\"- å¤§çŸ©é˜µçš„è®¡ç®—å¼ºåº¦æ›´é«˜ï¼Œæ›´èƒ½åˆ©ç”¨Tensor Core\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Attentionçš„è®¡ç®—å¼ºåº¦åˆ†æ\n",
        "\n",
        "è®©æˆ‘ä»¬åˆ†ææ ‡å‡†Attentionå„æ­¥éª¤çš„è®¡ç®—å¼ºåº¦ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_attention_intensity(N, d, dtype_bytes=2):\n",
        "    \"\"\"\n",
        "    åˆ†æAttentionå„æ­¥éª¤çš„è®¡ç®—å¼ºåº¦\n",
        "    \n",
        "    Step 1: S = Q @ K^T   [N,d] @ [d,N] -> [N,N]\n",
        "    Step 2: P = softmax(S)  [N,N] -> [N,N]\n",
        "    Step 3: O = P @ V     [N,N] @ [N,d] -> [N,d]\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Step 1: S = Q @ K^T\n",
        "    flops1 = 2 * N * N * d\n",
        "    mem1 = (N*d + N*d + N*N) * dtype_bytes  # Q + K + S\n",
        "    intensity1 = flops1 / mem1\n",
        "    results.append((\"Q @ K^T\", flops1, mem1, intensity1))\n",
        "    \n",
        "    # Step 2: softmax (approximately)\n",
        "    flops2 = 5 * N * N  # exp, sum, div\n",
        "    mem2 = 2 * N * N * dtype_bytes  # read S, write P\n",
        "    intensity2 = flops2 / mem2\n",
        "    results.append((\"softmax\", flops2, mem2, intensity2))\n",
        "    \n",
        "    # Step 3: O = P @ V\n",
        "    flops3 = 2 * N * d * N\n",
        "    mem3 = (N*N + N*d + N*d) * dtype_bytes  # P + V + O\n",
        "    intensity3 = flops3 / mem3\n",
        "    results.append((\"P @ V\", flops3, mem3, intensity3))\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"æ ‡å‡†Attentionè®¡ç®—å¼ºåº¦åˆ†æ\")\n",
        "print(\"(åºåˆ—é•¿åº¦N=2048, å¤´ç»´åº¦d=64, FP16)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = analyze_attention_intensity(2048, 64)\n",
        "print(f\"{'æ­¥éª¤':<12} {'FLOPs (M)':<15} {'Memory (MB)':<15} {'è®¡ç®—å¼ºåº¦':<15}\")\n",
        "print(\"-\"*70)\n",
        "for step, flops, mem, intensity in results:\n",
        "    print(f\"{step:<12} {flops/1e6:<15.1f} {mem/1e6:<15.1f} {intensity:<15.1f}\")\n",
        "\n",
        "print(\"\\nå…³é”®æ´å¯Ÿ:\")\n",
        "print(\"- softmaxçš„è®¡ç®—å¼ºåº¦æä½ (~2.5)ï¼Œä¸¥é‡å†…å­˜ç»‘å®š\")\n",
        "print(\"- çŸ©é˜µä¹˜æ³•çš„è®¡ç®—å¼ºåº¦ (~64) ä¹Ÿä½äºGPUå¹³è¡¡ç‚¹ (~156)\")\n",
        "print(\"- è¿™å°±æ˜¯ä¸ºä»€ä¹ˆFlashAttentionéœ€è¦èåˆè¿™äº›æ“ä½œï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. æ¨¡æ‹ŸCUTLASSçš„Tileåˆ†å—\n",
        "\n",
        "CUTLASSå°†å¤§çŸ©é˜µåˆ†æˆå°å—(Tile)åœ¨SRAMä¸­è®¡ç®—ã€‚è®©æˆ‘ä»¬æ¨¡æ‹Ÿè¿™ä¸ªè¿‡ç¨‹ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tiled_matmul_simulation(M, N, K, tile_m=64, tile_n=64, tile_k=32):\n",
        "    \"\"\"\n",
        "    æ¨¡æ‹ŸCUTLASSçš„åˆ†å—çŸ©é˜µä¹˜æ³•\n",
        "    \n",
        "    å±•ç¤ºå¦‚ä½•å°†å¤§çŸ©é˜µåˆ†æˆå°å—å¤„ç†\n",
        "    \"\"\"\n",
        "    print(f\"çŸ©é˜µå¤§å°: A[{M}Ã—{K}] @ B[{K}Ã—{N}] = C[{M}Ã—{N}]\")\n",
        "    print(f\"Tileå¤§å°: {tile_m}Ã—{tile_n}Ã—{tile_k}\")\n",
        "    print()\n",
        "    \n",
        "    # è®¡ç®—éœ€è¦çš„tileæ•°é‡\n",
        "    num_tiles_m = (M + tile_m - 1) // tile_m\n",
        "    num_tiles_n = (N + tile_n - 1) // tile_n\n",
        "    num_tiles_k = (K + tile_k - 1) // tile_k\n",
        "    \n",
        "    total_tiles = num_tiles_m * num_tiles_n\n",
        "    inner_iterations = num_tiles_k\n",
        "    \n",
        "    print(f\"è¾“å‡ºTileæ•°é‡: {num_tiles_m} Ã— {num_tiles_n} = {total_tiles}\")\n",
        "    print(f\"æ¯ä¸ªè¾“å‡ºTileéœ€è¦Kæ–¹å‘è¿­ä»£: {inner_iterations}æ¬¡\")\n",
        "    print()\n",
        "    \n",
        "    # è®¡ç®—æ¯ä¸ªTileéœ€è¦çš„SRAM\n",
        "    tile_a_size = tile_m * tile_k * 2  # FP16\n",
        "    tile_b_size = tile_k * tile_n * 2\n",
        "    tile_c_size = tile_m * tile_n * 4  # FP32ç´¯åŠ \n",
        "    total_sram = tile_a_size + tile_b_size + tile_c_size\n",
        "    \n",
        "    print(f\"æ¯ä¸ªTileçš„SRAMéœ€æ±‚:\")\n",
        "    print(f\"  A tile: {tile_a_size/1024:.1f} KB\")\n",
        "    print(f\"  B tile: {tile_b_size/1024:.1f} KB\")\n",
        "    print(f\"  C tile: {tile_c_size/1024:.1f} KB\")\n",
        "    print(f\"  æ€»è®¡: {total_sram/1024:.1f} KB\")\n",
        "    print()\n",
        "    \n",
        "    # A100çš„å…±äº«å†…å­˜å¤§å°\n",
        "    a100_smem = 164 * 1024  # 164 KB\n",
        "    print(f\"A100å…±äº«å†…å­˜: {a100_smem/1024:.0f} KB\")\n",
        "    print(f\"åˆ©ç”¨ç‡: {total_sram/a100_smem*100:.1f}%\")\n",
        "\n",
        "# æ¨¡æ‹ŸFlashAttentionä¸­çš„åˆ†å—\n",
        "print(\"=\"*60)\n",
        "print(\"FlashAttentioné£æ ¼çš„åˆ†å— (åºåˆ—é•¿åº¦2048, å¤´ç»´åº¦64)\")\n",
        "print(\"=\"*60)\n",
        "tiled_matmul_simulation(2048, 64, 2048, tile_m=128, tile_n=64, tile_k=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. å¯è§†åŒ–åˆ†å—è¿‡ç¨‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_tiling():\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨ASCIIè‰ºæœ¯å¯è§†åŒ–åˆ†å—è¿‡ç¨‹\n",
        "    \"\"\"\n",
        "    diagram = \"\"\"\n",
        "    CUTLASSåˆ†å—çŸ©é˜µä¹˜æ³•ç¤ºæ„å›¾\n",
        "    ========================\n",
        "    \n",
        "    çŸ©é˜µA [MÃ—K]              çŸ©é˜µB [KÃ—N]              çŸ©é˜µC [MÃ—N]\n",
        "    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
        "    â”‚ A00 â”‚ A01 â”‚ A02 â”‚     â”‚ B00 â”‚ B01 â”‚ B02 â”‚     â”‚ C00 â”‚ C01 â”‚ C02 â”‚\n",
        "    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤\n",
        "    â”‚ A10 â”‚ A11 â”‚ A12 â”‚  @  â”‚ B10 â”‚ B11 â”‚ B12 â”‚  =  â”‚ C10 â”‚ C11 â”‚ C12 â”‚\n",
        "    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤\n",
        "    â”‚ A20 â”‚ A21 â”‚ A22 â”‚     â”‚ B20 â”‚ B21 â”‚ B22 â”‚     â”‚ C20 â”‚ C21 â”‚ C22 â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n",
        "    \n",
        "    è®¡ç®—C11çš„è¿‡ç¨‹:\n",
        "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    \n",
        "    C11 = A10 @ B01 + A11 @ B11 + A12 @ B21\n",
        "    \n",
        "    æ­¥éª¤1: åŠ è½½A10, B01åˆ°SRAM â†’ è®¡ç®—A10 @ B01 â†’ ç´¯åŠ åˆ°C11\n",
        "    æ­¥éª¤2: åŠ è½½A11, B11åˆ°SRAM â†’ è®¡ç®—A11 @ B11 â†’ ç´¯åŠ åˆ°C11  \n",
        "    æ­¥éª¤3: åŠ è½½A12, B21åˆ°SRAM â†’ è®¡ç®—A12 @ B21 â†’ ç´¯åŠ åˆ°C11\n",
        "    \n",
        "    å…³é”®ä¼˜åŒ–:\n",
        "    â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    1. æ¯ä¸ªTileåªä»HBMåŠ è½½ä¸€æ¬¡\n",
        "    2. æ‰€æœ‰è®¡ç®—åœ¨SRAMä¸­å®Œæˆ\n",
        "    3. ä½¿ç”¨å¼‚æ­¥æ‹·è´éšè—å»¶è¿Ÿ\n",
        "    4. Tensor CoreåŠ é€ŸTileå†…çš„çŸ©é˜µä¹˜æ³•\n",
        "    \"\"\"\n",
        "    print(diagram)\n",
        "\n",
        "visualize_tiling()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. CUTLASS vs æœ´ç´ å®ç°çš„å¯¹æ¯”\n",
        "\n",
        "è®©æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸ªæœ´ç´ çš„åˆ†å—å®ç°å’ŒPyTorchï¼ˆä½¿ç”¨cuBLAS/Tensor Coreï¼‰çš„æ€§èƒ½å·®è·ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def naive_tiled_matmul(A, B, tile_size=64):\n",
        "    \"\"\"\n",
        "    æœ´ç´ çš„åˆ†å—çŸ©é˜µä¹˜æ³•ï¼ˆçº¯Python/PyTorchï¼Œä¸ä½¿ç”¨ä¼˜åŒ–ï¼‰\n",
        "    ç”¨äºæ¼”ç¤ºåˆ†å—çš„æ¦‚å¿µ\n",
        "    \"\"\"\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "    \n",
        "    C = torch.zeros(M, N, dtype=A.dtype, device=A.device)\n",
        "    \n",
        "    # åˆ†å—éå†\n",
        "    for i in range(0, M, tile_size):\n",
        "        for j in range(0, N, tile_size):\n",
        "            for k in range(0, K, tile_size):\n",
        "                # è·å–tile\n",
        "                i_end = min(i + tile_size, M)\n",
        "                j_end = min(j + tile_size, N)\n",
        "                k_end = min(k + tile_size, K)\n",
        "                \n",
        "                # Tileä¹˜æ³•å¹¶ç´¯åŠ \n",
        "                C[i:i_end, j:j_end] += A[i:i_end, k:k_end] @ B[k:k_end, j:j_end]\n",
        "    \n",
        "    return C\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"åˆ†å—çŸ©é˜µä¹˜æ³•æ¼”ç¤º\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    M, N, K = 512, 512, 512\n",
        "    A = torch.randn(M, K, dtype=torch.float16, device='cuda')\n",
        "    B = torch.randn(K, N, dtype=torch.float16, device='cuda')\n",
        "    \n",
        "    # æ­£ç¡®æ€§éªŒè¯\n",
        "    C_torch = A @ B\n",
        "    C_tiled = naive_tiled_matmul(A, B, tile_size=64)\n",
        "    \n",
        "    max_diff = (C_torch - C_tiled).abs().max().item()\n",
        "    print(f\"æœ€å¤§è¯¯å·®: {max_diff:.6f}\")\n",
        "    print(f\"ç»“æœæ­£ç¡®: {max_diff < 0.01}\")\n",
        "    print()\n",
        "    print(\"æ³¨æ„: è¿™ä¸ªæœ´ç´ å®ç°åªæ˜¯æ¼”ç¤ºåˆ†å—æ¦‚å¿µ\")\n",
        "    print(\"CUTLASSçš„å®ç°è¦å¤æ‚å¾—å¤šï¼ŒåŒ…å«:\")\n",
        "    print(\"  - å…±äº«å†…å­˜ä¼˜åŒ–\")\n",
        "    print(\"  - Tensor Coreè°ƒç”¨\")\n",
        "    print(\"  - å¼‚æ­¥æ‹·è´\")\n",
        "    print(\"  - è½¯ä»¶æµæ°´çº¿\")\n",
        "else:\n",
        "    print(\"éœ€è¦CUDAç¯å¢ƒè¿è¡Œæ­¤æµ‹è¯•\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. æ€»ç»“\n",
        "\n",
        "é€šè¿‡è¿™ä¸ªnotebookï¼Œæˆ‘ä»¬ç†è§£äº†ï¼š\n",
        "\n",
        "1. **Tensor Coreçš„é‡è¦æ€§**: FP16çŸ©é˜µä¹˜æ³•å¯ä»¥è·å¾—æ˜¾è‘—åŠ é€Ÿ\n",
        "2. **è®¡ç®—å¼ºåº¦**: Attentionæ“ä½œæ˜¯å†…å­˜ç»‘å®šçš„ï¼Œéœ€è¦ä¼˜åŒ–\n",
        "3. **åˆ†å—ç­–ç•¥**: å°†å¤§çŸ©é˜µåˆ†æˆé€‚åˆSRAMçš„å°å—\n",
        "4. **CUTLASSçš„ä»·å€¼**: æä¾›äº†çµæ´»çš„åˆ†å—å’ŒTensor Coreå°è£…\n",
        "\n",
        "ä¸‹ä¸€æ­¥ï¼šå­¦ä¹ cuteå¼ é‡æŠ½è±¡å±‚ï¼Œäº†è§£å¦‚ä½•ç”¨ç°ä»£æ–¹å¼è¡¨è¾¾è¿™äº›æ“ä½œã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š å»¶ä¼¸é˜…è¯»\n",
        "\n",
        "- [CUTLASS Profiler](https://github.com/NVIDIA/cutlass/blob/main/tools/profiler/README.md): ç”¨äºæ€§èƒ½åˆ†æ\n",
        "- [CUTLASS Examples](https://github.com/NVIDIA/cutlass/tree/main/examples): å®˜æ–¹ç¤ºä¾‹ä»£ç \n",
        "- [Nsight Compute](https://developer.nvidia.com/nsight-compute): NVIDIAæ€§èƒ½åˆ†æå·¥å…·\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
