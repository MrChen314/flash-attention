{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPU架构基础 - 实践篇\n",
        "\n",
        "本notebook通过实际代码帮助你理解GPU架构的核心概念。\n",
        "\n",
        "**学习目标：**\n",
        "- 使用CUDA API查询GPU硬件信息\n",
        "- 理解SM、Warp、Thread的实际数值\n",
        "- 观察Warp内线程的行为特性\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n",
        "\n",
        "首先加载nvcc4jupyter扩展，使我们能够在Jupyter中编写CUDA C++代码。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载nvcc4jupyter扩展\n",
        "%load_ext nvcc4jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 查询GPU设备信息\n",
        "\n",
        "使用 `cudaGetDeviceProperties` API 获取GPU的详细硬件信息。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main() {\n",
        "    int deviceCount;\n",
        "    cudaGetDeviceCount(&deviceCount);\n",
        "    \n",
        "    printf(\"==========================================\\n\");\n",
        "    printf(\"         GPU 设备信息查询\\n\");\n",
        "    printf(\"==========================================\\n\\n\");\n",
        "    printf(\"检测到 %d 个CUDA设备\\n\\n\", deviceCount);\n",
        "    \n",
        "    for (int i = 0; i < deviceCount; i++) {\n",
        "        cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, i);\n",
        "        \n",
        "        printf(\"设备 %d: %s\\n\", i, prop.name);\n",
        "        printf(\"------------------------------------------\\n\");\n",
        "        \n",
        "        // 计算能力\n",
        "        printf(\"计算能力: %d.%d\\n\", prop.major, prop.minor);\n",
        "        \n",
        "        // SM相关\n",
        "        printf(\"\\n【SM信息】\\n\");\n",
        "        printf(\"  SM数量: %d\\n\", prop.multiProcessorCount);\n",
        "        printf(\"  每SM最大线程数: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
        "        printf(\"  每SM最大Block数: %d\\n\", prop.maxBlocksPerMultiProcessor);\n",
        "        \n",
        "        // Warp相关\n",
        "        printf(\"\\n【Warp信息】\\n\");\n",
        "        printf(\"  Warp大小: %d 线程\\n\", prop.warpSize);\n",
        "        int maxWarpsPerSM = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n",
        "        printf(\"  每SM最大Warp数: %d\\n\", maxWarpsPerSM);\n",
        "        \n",
        "        // Block/Thread相关\n",
        "        printf(\"\\n【Block/Thread限制】\\n\");\n",
        "        printf(\"  每Block最大线程数: %d\\n\", prop.maxThreadsPerBlock);\n",
        "        printf(\"  Block维度限制: (%d, %d, %d)\\n\", \n",
        "               prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2]);\n",
        "        printf(\"  Grid维度限制: (%d, %d, %d)\\n\", \n",
        "               prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]);\n",
        "        \n",
        "        // 内存相关\n",
        "        printf(\"\\n【内存信息】\\n\");\n",
        "        printf(\"  全局内存: %.2f GB\\n\", prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));\n",
        "        printf(\"  每Block共享内存: %zu KB\\n\", prop.sharedMemPerBlock / 1024);\n",
        "        printf(\"  每SM共享内存: %zu KB\\n\", prop.sharedMemPerMultiprocessor / 1024);\n",
        "        printf(\"  每Block寄存器数: %d\\n\", prop.regsPerBlock);\n",
        "        printf(\"  每SM寄存器数: %d\\n\", prop.regsPerMultiprocessor);\n",
        "        \n",
        "        // 性能特性\n",
        "        printf(\"\\n【性能特性】\\n\");\n",
        "        printf(\"  GPU时钟频率: %.2f GHz\\n\", prop.clockRate / 1e6);\n",
        "        printf(\"  内存时钟频率: %.2f GHz\\n\", prop.memoryClockRate / 1e6);\n",
        "        printf(\"  内存总线宽度: %d bit\\n\", prop.memoryBusWidth);\n",
        "        printf(\"  L2缓存大小: %d KB\\n\", prop.l2CacheSize / 1024);\n",
        "        \n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    \n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 观察Warp内线程行为\n",
        "\n",
        "Warp内的32个线程是同步执行的。让我们通过Warp级别的投票函数来验证这一点。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// 使用Warp级别的投票函数\n",
        "__global__ void warpVoteDemo() {\n",
        "    int tid = threadIdx.x;\n",
        "    int warpId = tid / 32;\n",
        "    int laneId = tid % 32;  // 线程在Warp内的位置\n",
        "    \n",
        "    // 条件：线程ID是否为偶数\n",
        "    bool isEven = (tid % 2 == 0);\n",
        "    \n",
        "    // __all_sync: 如果Warp内所有线程的条件都为true，返回1\n",
        "    int allEven = __all_sync(0xFFFFFFFF, isEven);\n",
        "    \n",
        "    // __any_sync: 如果Warp内任意线程的条件为true，返回1\n",
        "    int anyEven = __any_sync(0xFFFFFFFF, isEven);\n",
        "    \n",
        "    // __ballot_sync: 返回一个32位掩码，每个bit对应一个线程的条件值\n",
        "    unsigned int ballot = __ballot_sync(0xFFFFFFFF, isEven);\n",
        "    \n",
        "    // 只让每个Warp的第一个线程打印\n",
        "    if (laneId == 0) {\n",
        "        printf(\"Warp %d: all_even=%d, any_even=%d, ballot=0x%08X\\n\", \n",
        "               warpId, allEven, anyEven, ballot);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"==========================================\\n\");\n",
        "    printf(\"      Warp级别投票函数演示\\n\");\n",
        "    printf(\"==========================================\\n\\n\");\n",
        "    \n",
        "    printf(\"启动配置: 1个Block, 64个线程 (2个Warp)\\n\\n\");\n",
        "    \n",
        "    warpVoteDemo<<<1, 64>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "    printf(\"\\n解释:\\n\");\n",
        "    printf(\"- ballot = 0x55555555 表示bit 0,2,4,6,...为1 (偶数线程)\\n\");\n",
        "    printf(\"- 二进制: 0101 0101 0101 0101 0101 0101 0101 0101\\n\");\n",
        "    \n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Warp分化演示\n",
        "\n",
        "当Warp内的线程执行不同分支时，会导致串行执行，降低性能。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// 演示Warp分化 (糟糕的写法)\n",
        "__global__ void warpDivergenceDemo() {\n",
        "    int tid = threadIdx.x;\n",
        "    int laneId = tid % 32;\n",
        "    \n",
        "    volatile int result = 0;\n",
        "    \n",
        "    // Warp分化: 前16个线程和后16个线程执行不同分支\n",
        "    if (laneId < 16) {\n",
        "        for (int i = 0; i < 100; i++) result += i;\n",
        "    } else {\n",
        "        for (int i = 0; i < 100; i++) result -= i;\n",
        "    }\n",
        "}\n",
        "\n",
        "// 无Warp分化 (好的写法)\n",
        "__global__ void noDivergenceDemo() {\n",
        "    int tid = threadIdx.x;\n",
        "    int warpId = tid / 32;\n",
        "    \n",
        "    volatile int result = 0;\n",
        "    \n",
        "    // 整个Warp执行同一分支\n",
        "    if (warpId == 0) {\n",
        "        for (int i = 0; i < 100; i++) result += i;\n",
        "    } else {\n",
        "        for (int i = 0; i < 100; i++) result -= i;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"==========================================\\n\");\n",
        "    printf(\"        Warp分化性能对比\\n\");\n",
        "    printf(\"==========================================\\n\\n\");\n",
        "    \n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    float time1, time2;\n",
        "    \n",
        "    // 预热\n",
        "    warpDivergenceDemo<<<1000, 64>>>();\n",
        "    noDivergenceDemo<<<1000, 64>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "    // 测试有分化的情况\n",
        "    cudaEventRecord(start);\n",
        "    for (int i = 0; i < 1000; i++) {\n",
        "        warpDivergenceDemo<<<1000, 64>>>();\n",
        "    }\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time1, start, stop);\n",
        "    \n",
        "    // 测试无分化的情况\n",
        "    cudaEventRecord(start);\n",
        "    for (int i = 0; i < 1000; i++) {\n",
        "        noDivergenceDemo<<<1000, 64>>>();\n",
        "    }\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time2, start, stop);\n",
        "    \n",
        "    printf(\"有Warp分化: %.3f ms\\n\", time1);\n",
        "    printf(\"无Warp分化: %.3f ms\\n\", time2);\n",
        "    printf(\"性能提升: %.2fx\\n\", time1 / time2);\n",
        "    \n",
        "    printf(\"\\n结论: 避免Warp内的分支分化可以显著提升性能!\\n\");\n",
        "    \n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    \n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 线程与Warp映射关系\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void threadWarpMapping() {\n",
        "    // 计算线程的线性ID\n",
        "    int tid = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y;\n",
        "    int warpId = tid / 32;\n",
        "    int laneId = tid % 32;\n",
        "    \n",
        "    // 只打印每个Warp的第一个和最后一个线程\n",
        "    if (laneId == 0 || laneId == 31) {\n",
        "        printf(\"Thread (%d,%d,%d) -> 线性ID=%d, Warp=%d, Lane=%d\\n\",\n",
        "               threadIdx.x, threadIdx.y, threadIdx.z,\n",
        "               tid, warpId, laneId);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"==========================================\\n\");\n",
        "    printf(\"      线程与Warp映射关系\\n\");\n",
        "    printf(\"==========================================\\n\\n\");\n",
        "    \n",
        "    printf(\"Block配置: (8, 4, 2) = 64线程 = 2个Warp\\n\\n\");\n",
        "    \n",
        "    dim3 blockDim(8, 4, 2);  // 8*4*2 = 64 threads\n",
        "    threadWarpMapping<<<1, blockDim>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "    printf(\"\\n说明: 线程按照 x->y->z 的顺序线性化后分配到Warp\\n\");\n",
        "    \n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "\n",
        "1. **SM (Streaming Multiprocessor)**\n",
        "   - GPU的基本计算单元\n",
        "   - 包含多个CUDA核心、共享内存、寄存器等资源\n",
        "   \n",
        "2. **Warp**\n",
        "   - 32个线程组成的执行单位\n",
        "   - 同一Warp内的线程执行相同指令\n",
        "   - Warp分化会导致性能下降\n",
        "   \n",
        "3. **关键数值**\n",
        "   - Warp大小固定为32\n",
        "   - 每SM的最大线程数、Warp数取决于GPU架构\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 修改代码，使用`__shfl_sync`在Warp内交换数据\n",
        "2. 编写一个kernel统计每个SM上运行的Block数量\n",
        "3. 尝试不同的Block大小，观察Occupancy的变化\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
