# GPU内存层次结构

> 理解内存层次是优化CUDA程序的关键

---

## 1. 内存层次概述

GPU拥有多层内存，每层在容量、延迟、带宽和作用域上各有不同。

### 1.1 内存层次金字塔

```
                    ┌──────────┐
                    │ Registers│  ← 最快，线程私有
                    │  (~1周期) │
                   ┌┴──────────┴┐
                   │   Shared   │  ← Block内共享
                   │  Memory    │
                   │ (~20周期)  │
                  ┌┴────────────┴┐
                  │   L1 Cache   │  ← SM本地
                  │  (~30周期)   │
                 ┌┴──────────────┴┐
                 │    L2 Cache    │  ← 全局共享
                 │   (~200周期)   │
                ┌┴────────────────┴┐
                │   Global Memory  │  ← 主显存(HBM)
                │   (VRAM/HBM)     │
                │   (~400周期)     │
               ┌┴──────────────────┴┐
               │    Host Memory     │  ← CPU内存
               │   (System RAM)     │
               └────────────────────┘
                        ↓
                    容量增大
                    延迟增加
```

### 1.2 各层内存对比

| 内存类型 | 容量 | 延迟 | 带宽 | 作用域 | 生命周期 |
|----------|------|------|------|--------|----------|
| Registers | ~256KB/SM | 1周期 | 极高 | 线程 | 线程 |
| Shared Memory | 最大164KB/SM | ~20周期 | ~10TB/s | Block | Block |
| L1 Cache | 最大192KB/SM | ~30周期 | ~8TB/s | SM | 自动管理 |
| L2 Cache | 数MB | ~200周期 | ~4TB/s | 全局 | 自动管理 |
| Global Memory | 数GB-数十GB | ~400周期 | ~2TB/s | 全局 | 应用 |
| Constant Memory | 64KB | ~4周期(缓存命中) | 高 | 全局 | 应用 |
| Texture Memory | 取决于配置 | ~100周期 | 高 | 全局 | 应用 |

---

## 2. Registers（寄存器）

### 2.1 特点

- **最快的存储**：1个时钟周期访问
- **线程私有**：每个线程独享
- **容量有限**：每个线程最多255个32位寄存器
- **隐式使用**：由编译器自动分配

### 2.2 使用方式

```cpp
__global__ void kernel() {
    // 这些局部变量通常存储在寄存器中
    int a = 1;
    float b = 2.0f;
    float result = a + b;
}
```

### 2.3 寄存器溢出（Register Spilling）

当使用的寄存器超过限制时，会溢出到Local Memory（实际是Global Memory），性能大幅下降。

```cpp
// 可能导致寄存器溢出的情况
__global__ void heavyKernel() {
    float arr[100];  // 大数组可能导致溢出
    // ...
}
```

### 2.4 影响Occupancy

每个线程使用的寄存器越多，SM能同时运行的线程越少：

```
假设SM有65536个寄存器，每Block 256线程：
- 每线程32个寄存器 → 每SM可运行 65536/(256*32) = 8 Blocks
- 每线程64个寄存器 → 每SM可运行 65536/(256*64) = 4 Blocks
```

---

## 3. Shared Memory（共享内存）

### 3.1 特点

- **Block内所有线程共享**
- **低延迟**：约20个时钟周期
- **手动管理**：需要显式声明和使用
- **容量有限**：最大约164KB/SM（Ampere架构）

### 3.2 声明方式

**静态分配：**
```cpp
__global__ void kernel() {
    __shared__ float sharedData[256];  // 编译时确定大小
    // ...
}
```

**动态分配：**
```cpp
__global__ void kernel() {
    extern __shared__ float sharedData[];  // 运行时确定大小
    // ...
}

// 调用时指定大小
kernel<<<grid, block, sharedMemSize>>>(args);
```

### 3.3 典型使用模式

```cpp
__global__ void sharedMemExample(float* input, float* output, int n) {
    __shared__ float sdata[256];
    
    int tid = threadIdx.x;
    int gid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // 1. 从Global Memory加载到Shared Memory
    if (gid < n) {
        sdata[tid] = input[gid];
    }
    
    // 2. 同步，确保所有线程都完成加载
    __syncthreads();
    
    // 3. 在Shared Memory中进行计算（快速）
    // 例如：访问相邻元素
    float result = sdata[tid];
    if (tid > 0) result += sdata[tid - 1];
    if (tid < 255) result += sdata[tid + 1];
    
    // 4. 写回Global Memory
    if (gid < n) {
        output[gid] = result;
    }
}
```

### 3.4 Bank冲突（Bank Conflict）

共享内存被分成32个bank（对应32个线程的Warp）。

**无冲突情况：**
```cpp
// 每个线程访问不同bank - 最优
sdata[threadIdx.x];  // 线程0访问bank0, 线程1访问bank1, ...
```

**有冲突情况：**
```cpp
// 多个线程访问同一bank - 串行化
sdata[threadIdx.x * 32];  // 所有线程访问bank0！
```

**Bank冲突可视化：**
```
Bank:    0   1   2   3   4   5   6   7  ... 31
Thread:  0   1   2   3   4   5   6   7  ... 31
         ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓      ↓
Access: [0] [1] [2] [3] [4] [5] [6] [7] ...[31]  ← 无冲突

Bank:    0   0   0   0   0   0   0   0  ... 0
Thread:  0   1   2   3   4   5   6   7  ... 31
         ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓      ↓
Access: [0] [32][64][96]...                     ← 32路冲突！
```

---

## 4. Global Memory（全局内存）

### 4.1 特点

- **容量最大**：GB到数十GB（显存/HBM）
- **所有线程可访问**
- **延迟最高**：~400个时钟周期
- **持久化**：在整个应用生命周期有效

### 4.2 声明与分配

```cpp
// 在Host代码中
float* d_data;
cudaMalloc(&d_data, size);  // 分配
cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);  // 拷贝
cudaFree(d_data);  // 释放

// 在Kernel中直接访问
__global__ void kernel(float* data) {
    data[idx] = ...;
}
```

### 4.3 合并访问（Coalesced Access）

**最重要的优化原则**：相邻线程访问相邻内存地址。

**好的访问模式（合并访问）：**
```cpp
// 线程0访问data[0], 线程1访问data[1], ...
data[threadIdx.x] = value;  // ✓ 合并为一次内存事务
```

**差的访问模式（非合并访问）：**
```cpp
// 线程0访问data[0], 线程1访问data[1024], ...
data[threadIdx.x * stride] = value;  // ✗ 多次内存事务
```

**访问模式图解：**
```
合并访问 (Coalesced):
Thread: 0   1   2   3   4   5   6   7
        ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓
Memory: [0] [1] [2] [3] [4] [5] [6] [7]  ← 一次128字节事务

非合并访问 (Strided):
Thread: 0       1       2       3
        ↓       ↓       ↓       ↓
Memory: [0] ... [32] ...[64] ...[96]    ← 多次32字节事务
```

---

## 5. 其他内存类型

### 5.1 Constant Memory

- **只读**：由Host写入，Kernel只能读取
- **广播优化**：当所有线程读取相同地址时，一次读取广播到所有线程
- **大小限制**：64KB

```cpp
__constant__ float constData[1024];

// Host端写入
cudaMemcpyToSymbol(constData, hostData, size);

// Kernel中读取
__global__ void kernel() {
    float val = constData[idx];  // 所有线程读同一地址时最优
}
```

### 5.2 Texture Memory

- **专门针对2D/3D空间局部性优化**
- **硬件插值**：支持线性插值
- **边界处理**：自动处理越界访问
- **适用场景**：图像处理、物理模拟

### 5.3 Local Memory

- **线程私有**：当寄存器不够时使用
- **实际位于Global Memory**：延迟高
- **编译器自动使用**：当局部变量太大或寄存器溢出时

---

## 6. 内存优化策略

### 6.1 使用Shared Memory减少Global Memory访问

```cpp
// 优化前：多次Global Memory访问
__global__ void before(float* data) {
    for (int i = 0; i < 10; i++) {
        result += data[idx];  // 每次循环都访问Global Memory
    }
}

// 优化后：先加载到Shared Memory
__global__ void after(float* data) {
    __shared__ float sdata[256];
    sdata[tid] = data[gid];
    __syncthreads();
    
    for (int i = 0; i < 10; i++) {
        result += sdata[tid];  // 在Shared Memory中访问
    }
}
```

### 6.2 确保合并访问

```cpp
// 差：strided访问（AoS布局）
struct Particle {
    float x, y, z;
};
Particle particles[N];
// particles[tid].x, particles[tid].y, particles[tid].z

// 好：合并访问（SoA布局）
float x[N], y[N], z[N];
// x[tid], y[tid], z[tid]
```

### 6.3 避免Bank冲突

```cpp
// 原始：有冲突
__shared__ float sdata[32][32];
sdata[threadIdx.x][threadIdx.y];  // 可能有冲突

// 优化：填充一列避免冲突
__shared__ float sdata[32][33];  // 加一列padding
sdata[threadIdx.x][threadIdx.y];  // 无冲突
```

---

## 7. 与FlashAttention的关联

FlashAttention的核心优化就是**内存层次的高效利用**：

1. **Shared Memory作为Tile缓存**
   - Q、K、V的块加载到Shared Memory
   - 在Shared Memory中进行矩阵运算

2. **避免Global Memory中间结果**
   - 不存储完整的N×N注意力矩阵
   - 使用Online Softmax在片上计算

3. **利用异步拷贝**
   - 使用`cp.async`重叠数据加载和计算

```cpp
// FlashAttention中的典型模式
__shared__ float sQ[kBlockM][kHeadDim];
__shared__ float sK[kBlockN][kHeadDim];
__shared__ float sV[kBlockN][kHeadDim];

// 加载Q到Shared Memory
// 循环处理K/V块
for (int n_block = ...) {
    // 加载K/V块到Shared Memory
    // 在Shared Memory中计算 S = Q @ K^T
    // 计算Softmax
    // 计算 O += Softmax(S) @ V
}
```

---

## 8. 总结

| 内存类型 | 何时使用 | 关键优化点 |
|----------|----------|------------|
| Registers | 临时变量、循环计数器 | 避免溢出 |
| Shared Memory | Block内数据共享、减少Global访问 | 避免Bank冲突 |
| L1/L2 Cache | 自动管理 | 合并访问提高命中率 |
| Global Memory | 大规模数据存储 | 合并访问 |
| Constant Memory | 只读常量 | 相同地址广播 |

---

## 📚 延伸阅读

- [CUDA C++ Programming Guide - Memory Hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy)
- [CUDA Best Practices - Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)

