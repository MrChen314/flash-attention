{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPU内存层次结构 - 实践篇\n",
        "\n",
        "本notebook通过实际代码演示GPU内存层次的使用和优化技巧。\n",
        "\n",
        "**学习目标：**\n",
        "- 对比Global Memory和Shared Memory的性能\n",
        "- 掌握Shared Memory的使用方法\n",
        "- 理解合并访问和Bank冲突的影响\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext nvcc4jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 矩阵转置：Global Memory vs Shared Memory\n",
        "\n",
        "矩阵转置是展示内存优化效果的经典示例。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define TILE_DIM 32\n",
        "#define BLOCK_ROWS 8\n",
        "\n",
        "// 朴素版本：直接转置，非合并写入\n",
        "__global__ void transposeNaive(float* out, float* in, int width, int height) {\n",
        "    int xIndex = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "    int yIndex = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "    \n",
        "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
        "        if (xIndex < width && (yIndex + j) < height) {\n",
        "            // 读取合并，写入非合并（跨越stride）\n",
        "            out[xIndex * height + (yIndex + j)] = in[(yIndex + j) * width + xIndex];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// 使用Shared Memory优化\n",
        "__global__ void transposeShared(float* out, float* in, int width, int height) {\n",
        "    __shared__ float tile[TILE_DIM][TILE_DIM + 1];  // +1避免bank冲突\n",
        "    \n",
        "    int xIndex = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "    int yIndex = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "    \n",
        "    // 合并读取到Shared Memory\n",
        "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
        "        if (xIndex < width && (yIndex + j) < height) {\n",
        "            tile[threadIdx.y + j][threadIdx.x] = in[(yIndex + j) * width + xIndex];\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    __syncthreads();\n",
        "    \n",
        "    // 交换坐标\n",
        "    xIndex = blockIdx.y * TILE_DIM + threadIdx.x;\n",
        "    yIndex = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "    \n",
        "    // 合并写入Global Memory\n",
        "    for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS) {\n",
        "        if (xIndex < height && (yIndex + j) < width) {\n",
        "            out[(yIndex + j) * height + xIndex] = tile[threadIdx.x][threadIdx.y + j];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"==========================================\\n\");\n",
        "    printf(\"    矩阵转置: Shared Memory优化对比\\n\");\n",
        "    printf(\"==========================================\\n\\n\");\n",
        "    \n",
        "    int width = 4096;\n",
        "    int height = 4096;\n",
        "    size_t size = width * height * sizeof(float);\n",
        "    \n",
        "    float *h_in = (float*)malloc(size);\n",
        "    float *h_out = (float*)malloc(size);\n",
        "    \n",
        "    // 初始化\n",
        "    for (int i = 0; i < width * height; i++) {\n",
        "        h_in[i] = (float)i;\n",
        "    }\n",
        "    \n",
        "    float *d_in, *d_out;\n",
        "    cudaMalloc(&d_in, size);\n",
        "    cudaMalloc(&d_out, size);\n",
        "    cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);\n",
        "    \n",
        "    dim3 grid(width / TILE_DIM, height / TILE_DIM);\n",
        "    dim3 block(TILE_DIM, BLOCK_ROWS);\n",
        "    \n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    float time_naive, time_shared;\n",
        "    \n",
        "    // 预热\n",
        "    transposeNaive<<<grid, block>>>(d_out, d_in, width, height);\n",
        "    transposeShared<<<grid, block>>>(d_out, d_in, width, height);\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "    // 测试朴素版本\n",
        "    cudaEventRecord(start);\n",
        "    for (int i = 0; i < 100; i++) {\n",
        "        transposeNaive<<<grid, block>>>(d_out, d_in, width, height);\n",
        "    }\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time_naive, start, stop);\n",
        "    \n",
        "    // 测试Shared Memory版本\n",
        "    cudaEventRecord(start);\n",
        "    for (int i = 0; i < 100; i++) {\n",
        "        transposeShared<<<grid, block>>>(d_out, d_in, width, height);\n",
        "    }\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&time_shared, start, stop);\n",
        "    \n",
        "    printf(\"矩阵大小: %d × %d\\n\", width, height);\n",
        "    printf(\"Block大小: (%d, %d)\\n\\n\", TILE_DIM, BLOCK_ROWS);\n",
        "    \n",
        "    printf(\"朴素版本: %.3f ms\\n\", time_naive / 100);\n",
        "    printf(\"Shared Memory版本: %.3f ms\\n\", time_shared / 100);\n",
        "    printf(\"加速比: %.2fx\\n\", time_naive / time_shared);\n",
        "    \n",
        "    // 计算带宽\n",
        "    float gb = 2.0f * width * height * sizeof(float) / 1e9;  // 读+写\n",
        "    printf(\"\\n有效带宽:\\n\");\n",
        "    printf(\"  朴素版本: %.2f GB/s\\n\", gb / (time_naive / 100 / 1000));\n",
        "    printf(\"  Shared Memory版本: %.2f GB/s\\n\", gb / (time_shared / 100 / 1000));\n",
        "    \n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "    free(h_in);\n",
        "    free(h_out);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    \n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Shared Memory基础用法\n",
        "\n",
        "演示如何声明和使用共享内存。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// 使用静态分配的Shared Memory\n",
        "__global__ void staticSharedDemo(float* out, float* in, int n) {\n",
        "    __shared__ float sdata[256];  // 静态分配：编译时确定大小\n",
        "    \n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    \n",
        "    // 加载到共享内存\n",
        "    if (gid < n) {\n",
        "        sdata[tid] = in[gid];\n",
        "    }\n",
        "    \n",
        "    __syncthreads();  // 确保所有线程完成加载\n",
        "    \n",
        "    // 在共享内存中进行计算（示例：与相邻元素求平均）\n",
        "    float result = sdata[tid];\n",
        "    if (tid > 0) result = (result + sdata[tid - 1]) / 2.0f;\n",
        "    \n",
        "    if (gid < n) {\n",
        "        out[gid] = result;\n",
        "    }\n",
        "}\n",
        "\n",
        "// 使用动态分配的Shared Memory\n",
        "__global__ void dynamicSharedDemo(float* out, float* in, int n) {\n",
        "    extern __shared__ float sdata[];  // 动态分配：运行时确定大小\n",
        "    \n",
        "    int tid = threadIdx.x;\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    \n",
        "    if (gid < n) {\n",
        "        sdata[tid] = in[gid];\n",
        "    }\n",
        "    \n",
        "    __syncthreads();\n",
        "    \n",
        "    float result = sdata[tid];\n",
        "    if (tid > 0) result = (result + sdata[tid - 1]) / 2.0f;\n",
        "    \n",
        "    if (gid < n) {\n",
        "        out[gid] = result;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"==========================================\\n\");\n",
        "    printf(\"        Shared Memory 基础用法\\n\");\n",
        "    printf(\"==========================================\\n\\n\");\n",
        "    \n",
        "    int n = 16;\n",
        "    size_t size = n * sizeof(float);\n",
        "    \n",
        "    float *h_in = (float*)malloc(size);\n",
        "    float *h_out = (float*)malloc(size);\n",
        "    \n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_in[i] = (float)i;\n",
        "    }\n",
        "    \n",
        "    float *d_in, *d_out;\n",
        "    cudaMalloc(&d_in, size);\n",
        "    cudaMalloc(&d_out, size);\n",
        "    cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);\n",
        "    \n",
        "    printf(\"输入数据:\\n  \");\n",
        "    for (int i = 0; i < n; i++) printf(\"%.0f \", h_in[i]);\n",
        "    printf(\"\\n\\n\");\n",
        "    \n",
        "    // 静态Shared Memory\n",
        "    staticSharedDemo<<<1, 256>>>(d_out, d_in, n);\n",
        "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
        "    printf(\"静态Shared Memory结果:\\n  \");\n",
        "    for (int i = 0; i < n; i++) printf(\"%.1f \", h_out[i]);\n",
        "    printf(\"\\n\\n\");\n",
        "    \n",
        "    // 动态Shared Memory - 注意第三个参数指定大小\n",
        "    int sharedMemSize = 256 * sizeof(float);\n",
        "    dynamicSharedDemo<<<1, 256, sharedMemSize>>>(d_out, d_in, n);\n",
        "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
        "    printf(\"动态Shared Memory结果:\\n  \");\n",
        "    for (int i = 0; i < n; i++) printf(\"%.1f \", h_out[i]);\n",
        "    printf(\"\\n\");\n",
        "    \n",
        "    printf(\"\\n说明: 每个元素与其前一个元素取平均\\n\");\n",
        "    \n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "    free(h_in);\n",
        "    free(h_out);\n",
        "    \n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 合并访问 vs 非合并访问\n",
        "\n",
        "演示合并访问对Global Memory性能的影响。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// 合并访问：相邻线程访问相邻内存\n",
        "__global__ void coalescedAccess(float* data, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        data[idx] = data[idx] * 2.0f;  // 线程0访问[0], 线程1访问[1]...\n",
        "    }\n",
        "}\n",
        "\n",
        "// 非合并访问：跨步访问\n",
        "__global__ void stridedAccess(float* data, int n, int stride) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int actualIdx = idx * stride;  // 跨步访问\n",
        "    if (actualIdx < n) {\n",
        "        data[actualIdx] = data[actualIdx] * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"==========================================\\n\");\n",
        "    printf(\"      合并访问 vs 非合并访问对比\\n\");\n",
        "    printf(\"==========================================\\n\\n\");\n",
        "    \n",
        "    int n = 32 * 1024 * 1024;  // 32M elements\n",
        "    size_t size = n * sizeof(float);\n",
        "    \n",
        "    float *d_data;\n",
        "    cudaMalloc(&d_data, size);\n",
        "    \n",
        "    int threadsPerBlock = 256;\n",
        "    \n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    \n",
        "    printf(\"数据大小: %d MB\\n\\n\", (int)(size / (1024 * 1024)));\n",
        "    \n",
        "    // 预热\n",
        "    coalescedAccess<<<n / threadsPerBlock, threadsPerBlock>>>(d_data, n);\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "    // 测试合并访问\n",
        "    cudaEventRecord(start);\n",
        "    for (int i = 0; i < 100; i++) {\n",
        "        coalescedAccess<<<n / threadsPerBlock, threadsPerBlock>>>(d_data, n);\n",
        "    }\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    \n",
        "    float time_coalesced;\n",
        "    cudaEventElapsedTime(&time_coalesced, start, stop);\n",
        "    \n",
        "    // 测试不同stride的非合并访问\n",
        "    printf(\"访问模式             时间(ms)    带宽(GB/s)\\n\");\n",
        "    printf(\"------------------------------------------------\\n\");\n",
        "    \n",
        "    float gb = 2.0f * n * sizeof(float) / 1e9;  // 读+写\n",
        "    printf(\"合并访问 (stride=1)   %7.3f     %7.2f\\n\", \n",
        "           time_coalesced / 100, gb / (time_coalesced / 100 / 1000));\n",
        "    \n",
        "    int strides[] = {2, 4, 8, 16, 32};\n",
        "    for (int s = 0; s < 5; s++) {\n",
        "        int stride = strides[s];\n",
        "        int numElements = n / stride;\n",
        "        int numBlocks = (numElements + threadsPerBlock - 1) / threadsPerBlock;\n",
        "        \n",
        "        // 预热\n",
        "        stridedAccess<<<numBlocks, threadsPerBlock>>>(d_data, n, stride);\n",
        "        cudaDeviceSynchronize();\n",
        "        \n",
        "        cudaEventRecord(start);\n",
        "        for (int i = 0; i < 100; i++) {\n",
        "            stridedAccess<<<numBlocks, threadsPerBlock>>>(d_data, n, stride);\n",
        "        }\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "        \n",
        "        float time_strided;\n",
        "        cudaEventElapsedTime(&time_strided, start, stop);\n",
        "        \n",
        "        float gb_strided = 2.0f * numElements * sizeof(float) / 1e9;\n",
        "        printf(\"非合并 (stride=%2d)    %7.3f     %7.2f\\n\", \n",
        "               stride, time_strided / 100, gb_strided / (time_strided / 100 / 1000));\n",
        "    }\n",
        "    \n",
        "    printf(\"\\n结论: stride越大，有效带宽越低！\\n\");\n",
        "    \n",
        "    cudaFree(d_data);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    \n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "**关键要点：**\n",
        "\n",
        "1. **内存层次**\n",
        "   - Registers > Shared Memory > L1/L2 Cache > Global Memory\n",
        "   - 越靠近计算单元，速度越快，容量越小\n",
        "\n",
        "2. **Shared Memory使用**\n",
        "   - 声明: `__shared__ float data[SIZE];`\n",
        "   - 必须配合 `__syncthreads()` 同步\n",
        "\n",
        "3. **合并访问**\n",
        "   - 相邻线程访问相邻地址 = 高带宽\n",
        "   - 跨步访问 = 低带宽\n",
        "\n",
        "4. **优化策略**\n",
        "   - 将频繁访问的数据加载到Shared Memory\n",
        "   - 保持内存访问的合并性\n",
        "   - 避免Shared Memory的Bank冲突\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 修改矩阵转置代码，去掉`+1`的padding，观察Bank冲突对性能的影响\n",
        "2. 实现一个使用Shared Memory的矩阵乘法（分块算法）\n",
        "3. 尝试不同的TILE_DIM值，找到最优配置\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
