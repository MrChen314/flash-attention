# å¼‚æ­¥å†…å­˜æ‹·è´

> å®ç°è®¡ç®—ä¸æ•°æ®ä¼ è¾“çš„é‡å ï¼Œæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡

---

## 1. åŒæ­¥ vs å¼‚æ­¥

### 1.1 åŒæ­¥æ‹·è´çš„é—®é¢˜

ä¼ ç»Ÿçš„`cudaMemcpy`æ˜¯**é˜»å¡è°ƒç”¨**ï¼š

```cpp
cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);
// CPUåœ¨è¿™é‡Œç­‰å¾…æ‹·è´å®Œæˆ
kernel<<<grid, block>>>(d_data);
// GPUå¼€å§‹æ‰§è¡Œkernel
```

**æ—¶é—´çº¿ï¼š**
```
CPU:  [  ç­‰å¾…æ‹·è´  ] [  ç­‰å¾…kernel  ]
GPU:  [ æ‹·è´æ•°æ® ] [  æ‰§è¡Œkernel  ]
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ—¶é—´
                    ä¸²è¡Œæ‰§è¡Œï¼Œæµªè´¹æ—¶é—´
```

### 1.2 å¼‚æ­¥æ‹·è´çš„ä¼˜åŠ¿

ä½¿ç”¨CUDA Streamå®ç°å¼‚æ­¥æ“ä½œï¼š

```cpp
cudaStream_t stream;
cudaStreamCreate(&stream);

cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream);
kernel<<<grid, block, 0, stream>>>(d_data);
// CPUç»§ç»­æ‰§è¡Œå…¶ä»–å·¥ä½œ
```

**æ—¶é—´çº¿ï¼š**
```
CPU:  [æäº¤ä»»åŠ¡] [å…¶ä»–å·¥ä½œ...] [åŒæ­¥]
GPU:  [ æ‹·è´ ] [kernel]
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ—¶é—´
           CPUå’ŒGPUå¹¶è¡Œå·¥ä½œ
```

---

## 2. CUDA Stream

### 2.1 ä»€ä¹ˆæ˜¯Stream

Streamæ˜¯GPUä¸Šçš„**å‘½ä»¤é˜Ÿåˆ—**ï¼š
- åŒä¸€Streamå†…çš„æ“ä½œ**é¡ºåºæ‰§è¡Œ**
- ä¸åŒStreamä¹‹é—´çš„æ“ä½œ**å¯ä»¥å¹¶è¡Œ**

### 2.2 åˆ›å»ºå’Œä½¿ç”¨Stream

```cpp
// åˆ›å»ºStream
cudaStream_t stream;
cudaStreamCreate(&stream);

// å¼‚æ­¥å†…å­˜æ‹·è´
cudaMemcpyAsync(dst, src, size, cudaMemcpyHostToDevice, stream);

// åœ¨æŒ‡å®šStreamä¸Šæ‰§è¡Œkernel
kernel<<<grid, block, sharedMem, stream>>>(args);

// ç­‰å¾…Streamå®Œæˆ
cudaStreamSynchronize(stream);

// é”€æ¯Stream
cudaStreamDestroy(stream);
```

### 2.3 é»˜è®¤Stream

å¦‚æœä¸æŒ‡å®šStreamï¼Œä½¿ç”¨**é»˜è®¤Streamï¼ˆNULL Streamï¼‰**ï¼š
- `cudaMemcpy` = åŒæ­¥æ‹·è´ï¼ˆé˜»å¡ï¼‰
- `cudaMemcpyAsync` + NULL = å¼‚æ­¥ä½†ä¸é»˜è®¤StreamåŒæ­¥
- kernel<<<...>>> = åœ¨é»˜è®¤Streamæ‰§è¡Œ

---

## 3. å¤šStreamå¹¶è¡Œ

### 3.1 ä½¿ç”¨å¤šä¸ªStream

```cpp
const int numStreams = 4;
cudaStream_t streams[numStreams];

for (int i = 0; i < numStreams; i++) {
    cudaStreamCreate(&streams[i]);
}

// å°†å·¥ä½œåˆ†é…åˆ°å¤šä¸ªStream
for (int i = 0; i < numStreams; i++) {
    int offset = i * chunkSize;
    cudaMemcpyAsync(d_data + offset, h_data + offset, 
                    chunkSize * sizeof(float), 
                    cudaMemcpyHostToDevice, streams[i]);
    kernel<<<grid, block, 0, streams[i]>>>(d_data + offset);
    cudaMemcpyAsync(h_result + offset, d_result + offset,
                    chunkSize * sizeof(float),
                    cudaMemcpyDeviceToHost, streams[i]);
}

// ç­‰å¾…æ‰€æœ‰Streamå®Œæˆ
cudaDeviceSynchronize();
```

### 3.2 å¤šStreamå¹¶è¡Œç¤ºæ„å›¾

```
Stream 0: [H2Dæ‹·è´0] [Kernel0] [D2Hæ‹·è´0]
Stream 1:    [H2Dæ‹·è´1] [Kernel1] [D2Hæ‹·è´1]
Stream 2:       [H2Dæ‹·è´2] [Kernel2] [D2Hæ‹·è´2]
Stream 3:          [H2Dæ‹·è´3] [Kernel3] [D2Hæ‹·è´3]
          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ—¶é—´
                    æ“ä½œå¯ä»¥é‡å æ‰§è¡Œ
```

### 3.3 Pinned Memoryï¼ˆé¡µé”å®šå†…å­˜ï¼‰

å¼‚æ­¥æ‹·è´éœ€è¦ä½¿ç”¨**Pinned Memory**ï¼š

```cpp
// åˆ†é…Pinned Memoryï¼ˆé¡µé”å®šå†…å­˜ï¼‰
float* h_data;
cudaMallocHost(&h_data, size);  // æˆ– cudaHostAlloc

// ä½¿ç”¨...

// é‡Šæ”¾
cudaFreeHost(h_data);
```

**ä¸ºä»€ä¹ˆéœ€è¦Pinned Memoryï¼Ÿ**
- æ™®é€šå†…å­˜å¯èƒ½è¢«æ“ä½œç³»ç»Ÿæ¢é¡µï¼ˆswapï¼‰
- DMAä¼ è¾“éœ€è¦ç‰©ç†åœ°å€å›ºå®š
- Pinned Memoryä¿è¯å†…å­˜ä¸ä¼šè¢«æ¢å‡º

---

## 4. cp.asyncï¼šç¡¬ä»¶å¼‚æ­¥æ‹·è´

### 4.1 ä»€ä¹ˆæ˜¯cp.async

ä»Ampereæ¶æ„ï¼ˆSM 8.0ï¼‰å¼€å§‹ï¼ŒGPUæ”¯æŒ**ç¡¬ä»¶çº§åˆ«çš„å¼‚æ­¥æ‹·è´**ï¼š

- `cp.async`ï¼šä»Global Memoryåˆ°Shared Memoryçš„å¼‚æ­¥æ‹·è´
- ç»•è¿‡L1 Cacheå’Œå¯„å­˜å™¨
- å…è®¸è®¡ç®—ä¸å†…å­˜åŠ è½½é‡å 

### 4.2 è½¯ä»¶ç®¡ç†çš„å¼‚æ­¥æ‹·è´

```
ä¼ ç»Ÿæ–¹å¼ï¼š
Global Memory â†’ Registers â†’ Shared Memory
                  â†“
              çº¿ç¨‹ç­‰å¾…

cp.asyncæ–¹å¼ï¼š
Global Memory â”€â”€â”€â”€â”€â”€â”€â”€â†’ Shared Memory
                  â†“
              çº¿ç¨‹ç»§ç»­è®¡ç®—
```

### 4.3 ä½¿ç”¨cp.async

```cpp
#include <cuda/pipeline>

__global__ void kernel(float* gData) {
    __shared__ float sData[256];
    
    // åˆ›å»ºå¼‚æ­¥æ‹·è´ç®¡çº¿
    __shared__ cuda::pipeline_shared_state<cuda::thread_scope_block, 2> pipe_state;
    auto pipe = cuda::make_pipeline(threadBlock, &pipe_state);
    
    // å‘èµ·å¼‚æ­¥æ‹·è´
    pipe.producer_acquire();
    cuda::memcpy_async(&sData[threadIdx.x], &gData[idx], sizeof(float), pipe);
    pipe.producer_commit();
    
    // åœ¨ç­‰å¾…æ•°æ®çš„åŒæ—¶å¯ä»¥åšå…¶ä»–è®¡ç®—
    doOtherWork();
    
    // ç­‰å¾…æ‹·è´å®Œæˆ
    pipe.consumer_wait();
    
    // ç°åœ¨å¯ä»¥ä½¿ç”¨sData
    float val = sData[threadIdx.x];
    
    pipe.consumer_release();
}
```

### 4.4 ä½¿ç”¨PTXå†…è”æ±‡ç¼–

æ›´åº•å±‚çš„æ–¹å¼ï¼š

```cpp
__device__ void cp_async_cg(void* smem_ptr, const void* gmem_ptr) {
    uint32_t smem_addr = __cvta_generic_to_shared(smem_ptr);
    asm volatile(
        "cp.async.cg.shared.global [%0], [%1], 16;\n"
        :
        : "r"(smem_addr), "l"(gmem_ptr)
    );
}

// ç­‰å¾…æ‰€æœ‰cp.asyncå®Œæˆ
__device__ void cp_async_wait_all() {
    asm volatile("cp.async.wait_all;\n");
}

// ç­‰å¾…é™¤äº†æœ€è¿‘Nä¸ªç»„å¤–çš„æ‰€æœ‰cp.asyncå®Œæˆ
__device__ void cp_async_wait_group(int n) {
    asm volatile("cp.async.wait_group %0;\n" : : "n"(n));
}
```

---

## 5. åŒç¼“å†²ï¼ˆDouble Bufferingï¼‰

### 5.1 åŸç†

ä½¿ç”¨ä¸¤ä¸ªç¼“å†²åŒºäº¤æ›¿å·¥ä½œï¼š
- ç¼“å†²åŒºAï¼šæ­£åœ¨è¢«è®¡ç®—ä½¿ç”¨
- ç¼“å†²åŒºBï¼šæ­£åœ¨åŠ è½½ä¸‹ä¸€æ‰¹æ•°æ®

### 5.2 å®ç°

```cpp
__global__ void doubleBufferKernel(float* input, float* output, int n) {
    __shared__ float buffer[2][TILE_SIZE];
    
    int tile = 0;
    
    // é¢„åŠ è½½ç¬¬ä¸€ä¸ªtile
    loadTile(buffer[0], input, 0);
    __syncthreads();
    
    for (int i = 0; i < numTiles; i++) {
        // å¼‚æ­¥åŠ è½½ä¸‹ä¸€ä¸ªtileåˆ°å¦ä¸€ä¸ªbuffer
        if (i + 1 < numTiles) {
            loadTileAsync(buffer[1 - tile], input, i + 1);
        }
        
        // å¤„ç†å½“å‰tile
        processTile(buffer[tile]);
        
        // ç­‰å¾…å¼‚æ­¥åŠ è½½å®Œæˆ
        __syncthreads();
        
        // åˆ‡æ¢buffer
        tile = 1 - tile;
    }
}
```

### 5.3 æ—¶é—´çº¿å¯¹æ¯”

**æ— åŒç¼“å†²ï¼š**
```
[Load0][Compute0][Load1][Compute1][Load2][Compute2]
```

**æœ‰åŒç¼“å†²ï¼š**
```
[Load0][Compute0][Compute1][Compute2]
        [Load1  ][Load2  ]
         â†‘ é‡å æ‰§è¡Œ
```

---

## 6. ä¸FlashAttentionçš„å…³è”

FlashAttentionå¤§é‡ä½¿ç”¨å¼‚æ­¥æ‹·è´ä¼˜åŒ–ï¼š

### 6.1 æ•°æ®é¢„å–

```cpp
// FlashAttentionä¸­çš„å…¸å‹æ¨¡å¼
for (int n_block = 0; n_block < num_blocks; n_block++) {
    // å¼‚æ­¥åŠ è½½ä¸‹ä¸€å—K/Våˆ°ç¼“å†²åŒº
    if (n_block + 1 < num_blocks) {
        flash::copy_async(gK_next, sK_next);
        flash::copy_async(gV_next, sV_next);
    }
    
    // å¤„ç†å½“å‰å—
    flash::gemm(acc_s, sQ, sK);  // S = Q @ K^T
    flash::softmax(acc_s);
    flash::gemm(acc_o, acc_s, sV);  // O += Softmax(S) @ V
    
    // ç­‰å¾…å¼‚æ­¥åŠ è½½å®Œæˆ
    flash::cp_async_wait();
    __syncthreads();
    
    // äº¤æ¢ç¼“å†²åŒº
    swap(sK, sK_next);
    swap(sV, sV_next);
}
```

### 6.2 FlashAttention-2çš„ä¼˜åŒ–

```cpp
// ä½¿ç”¨cp.asyncå®ç°è®¡ç®—ä¸åŠ è½½é‡å 
// ä¼ªä»£ç 
for (int n_block = 0; n_block < num_blocks; n_block++) {
    // Stage 1: å‘èµ·å¼‚æ­¥æ‹·è´
    cp_async_copy(sK[next], gK[n_block + 1]);
    cp_async_copy(sV[next], gV[n_block + 1]);
    cp_async_commit();
    
    // Stage 2: è®¡ç®—S = Q @ K^Tï¼ˆä½¿ç”¨å½“å‰ç¼“å†²åŒºçš„æ•°æ®ï¼‰
    gemm(S, Q, K[current]);
    
    // Stage 3: ç­‰å¾…KåŠ è½½å®Œæˆï¼Œè®¡ç®—Softmax
    cp_async_wait<1>();  // ç­‰å¾…é™¤æœ€å1ç»„å¤–çš„æ‰€æœ‰æ‹·è´
    softmax(S);
    
    // Stage 4: è®¡ç®—O += Softmax(S) @ V
    gemm(O, S, V[current]);
    
    // Stage 5: ç­‰å¾…VåŠ è½½å®Œæˆ
    cp_async_wait<0>();
    
    // äº¤æ¢ç¼“å†²åŒº
    swap(current, next);
}
```

---

## 7. æ€»ç»“

| æŠ€æœ¯ | é€‚ç”¨åœºæ™¯ | å…³é”®ç‚¹ |
|------|----------|--------|
| cudaMemcpyAsync | Host-Deviceå¼‚æ­¥ä¼ è¾“ | éœ€è¦Pinned Memory |
| CUDA Stream | å¤šä»»åŠ¡å¹¶è¡Œ | åˆç†åˆ†é…å·¥ä½œåˆ°å¤šä¸ªStream |
| cp.async | Global-Sharedå¼‚æ­¥æ‹·è´ | Ampereæ¶æ„åŠä»¥å |
| åŒç¼“å†² | è®¡ç®—ä¸åŠ è½½é‡å  | ç©ºé—´æ¢æ—¶é—´ |

**æœ€ä½³å®è·µï¼š**
1. ä½¿ç”¨Pinned Memoryè¿›è¡Œå¼‚æ­¥ä¼ è¾“
2. å°†å·¥ä½œåˆ†å—ï¼Œåˆ†é…åˆ°å¤šä¸ªStream
3. ä½¿ç”¨cp.asyncå®ç°ç»†ç²’åº¦çš„è®¡ç®—-åŠ è½½é‡å 
4. åŒç¼“å†²éšè—å†…å­˜å»¶è¿Ÿ

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [CUDA C++ Programming Guide - Asynchronous Concurrent Execution](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution)
- [CUDA C++ Programming Guide - Asynchronous Data Copies](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies)
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)

