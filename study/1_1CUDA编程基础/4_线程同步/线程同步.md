# çº¿ç¨‹åŒæ­¥

> ç¡®ä¿å¹¶è¡Œè®¡ç®—çš„æ­£ç¡®æ€§

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦åŒæ­¥

### 1.1 ç«äº‰æ¡ä»¶ï¼ˆRace Conditionï¼‰

å½“å¤šä¸ªçº¿ç¨‹åŒæ—¶è®¿é—®åŒä¸€å†…å­˜ä½ç½®ï¼Œä¸”è‡³å°‘æœ‰ä¸€ä¸ªæ˜¯å†™æ“ä½œæ—¶ï¼Œå°±å¯èƒ½å‘ç”Ÿ**ç«äº‰æ¡ä»¶**ã€‚

```cpp
// å±é™©ï¼ç«äº‰æ¡ä»¶
__global__ void raceCondition(int* counter) {
    // å¤šä¸ªçº¿ç¨‹åŒæ—¶æ‰§è¡Œè¿™è¡Œ
    (*counter)++;  // è¯»å– -> åŠ 1 -> å†™å›
}
```

**é—®é¢˜åˆ†æï¼š**
```
çº¿ç¨‹A:  è¯»å–counter=0 â†’ è®¡ç®—0+1=1 â†’ å†™å›1
çº¿ç¨‹B:  è¯»å–counter=0 â†’ è®¡ç®—0+1=1 â†’ å†™å›1
ç»“æœ: counter=1 (æœŸæœ›å€¼=2)
```

### 1.2 æ•°æ®ä¾èµ–

å½“è®¡ç®—éœ€è¦å…¶ä»–çº¿ç¨‹çš„ç»“æœæ—¶ï¼Œå¿…é¡»ç­‰å¾…ï¼š

```cpp
__global__ void dataDependency() {
    __shared__ float sdata[256];
    
    sdata[threadIdx.x] = compute();  // æ¯ä¸ªçº¿ç¨‹å†™å…¥è‡ªå·±çš„ä½ç½®
    
    // âŒ é”™è¯¯ï¼šå¯èƒ½è¯»åˆ°æœªåˆå§‹åŒ–çš„æ•°æ®
    float neighbor = sdata[threadIdx.x + 1];
}
```

---

## 2. `__syncthreads()` è¯¦è§£

### 2.1 åŠŸèƒ½

`__syncthreads()` æ˜¯Blockå†…çš„**å±éšœåŒæ­¥ï¼ˆBarrier Synchronizationï¼‰**ï¼š

- Blockå†…æ‰€æœ‰çº¿ç¨‹å¿…é¡»åˆ°è¾¾æ­¤ç‚¹æ‰èƒ½ç»§ç»­
- ç¡®ä¿ä¹‹å‰çš„å†…å­˜æ“ä½œå¯¹æ‰€æœ‰çº¿ç¨‹å¯è§
- **åªåœ¨Blockå†…æœ‰æ•ˆ**ï¼Œä¸åŒBlockä¹‹é—´æ— æ³•åŒæ­¥

### 2.2 å…¸å‹ç”¨æ³•

```cpp
__global__ void correctUsage() {
    __shared__ float sdata[256];
    
    // é˜¶æ®µ1: æ‰€æœ‰çº¿ç¨‹å†™å…¥
    sdata[threadIdx.x] = compute();
    
    __syncthreads();  // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆå†™å…¥
    
    // é˜¶æ®µ2: ç°åœ¨å¯ä»¥å®‰å…¨è¯»å–ä»»æ„ä½ç½®
    float neighbor = sdata[(threadIdx.x + 1) % 256];
}
```

### 2.3 ç¤ºæ„å›¾

```
æ—¶é—´ â†’
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Thread 0: â”‚ å†™å…¥ â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“ â”‚ è¯»å– â”‚
          â”‚sdata[0]â”‚  ç­‰å¾…åŒæ­¥   â”‚      â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Thread 1: â”‚ å†™å…¥   â”‚ â–“â–“â–“â–“â–“â–“â–“ â”‚ è¯»å– â”‚
          â”‚sdata[1]â”‚ ç­‰å¾…åŒæ­¥ â”‚      â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Thread 2: â”‚ å†™å…¥      â”‚ â–“â–“â–“â–“ â”‚ è¯»å– â”‚
          â”‚sdata[2]   â”‚ç­‰å¾…åŒæ­¥â”‚      â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                       â†‘
                 __syncthreads()
                 æ‰€æœ‰çº¿ç¨‹åœ¨æ­¤æ±‡åˆ
```

---

## 3. åŒæ­¥çš„æ³¨æ„äº‹é¡¹

### 3.1 æ¡ä»¶åˆ†æ”¯ä¸­çš„åŒæ­¥

**å±é™©æ¨¡å¼**ï¼šä¸æ˜¯æ‰€æœ‰çº¿ç¨‹éƒ½æ‰§è¡Œ`__syncthreads()`

```cpp
// âŒ æ­»é”ï¼
if (threadIdx.x < 128) {
    __syncthreads();  // åªæœ‰éƒ¨åˆ†çº¿ç¨‹æ‰§è¡Œ
}

// âœ“ æ­£ç¡®
if (threadIdx.x < 128) {
    doWork();
}
__syncthreads();  // æ‰€æœ‰çº¿ç¨‹éƒ½å¿…é¡»æ‰§è¡Œ
```

### 3.2 å¾ªç¯ä¸­çš„åŒæ­¥

```cpp
// âœ“ æ­£ç¡®ï¼šæ‰€æœ‰çº¿ç¨‹æ‰§è¡Œç›¸åŒæ¬¡æ•°çš„åŒæ­¥
for (int i = 0; i < N; i++) {
    sdata[threadIdx.x] = data[i * blockDim.x + threadIdx.x];
    __syncthreads();
    // ä½¿ç”¨sdata
    __syncthreads();
}
```

### 3.3 åŒæ­¥çš„å¼€é”€

`__syncthreads()` ä¸æ˜¯å…è´¹çš„ï¼š
- éœ€è¦ç­‰å¾…æœ€æ…¢çš„çº¿ç¨‹
- é˜»æ­¢æŒ‡ä»¤çº§å¹¶è¡Œ
- åˆç†ä½¿ç”¨ï¼Œä¸è¦è¿‡åº¦åŒæ­¥

---

## 4. Warpçº§åŒæ­¥

### 4.1 Warpå†…éšå¼åŒæ­¥

åŒä¸€Warpå†…çš„32ä¸ªçº¿ç¨‹**å¤©ç„¶åŒæ­¥**ï¼ˆæ‰§è¡Œç›¸åŒæŒ‡ä»¤ï¼‰ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥é¿å…`__syncthreads()`ï¼š

```cpp
// Warpå†…æ“ä½œä¸éœ€è¦__syncthreads()
// ä½†éœ€è¦ä½¿ç”¨__syncwarp()ä¿è¯å¯è§æ€§
__syncwarp();
```

### 4.2 Warpçº§åŸè¯­

```cpp
// Warpå†…æŠ•ç¥¨
bool result = __all_sync(0xFFFFFFFF, condition);  // æ‰€æœ‰çº¿ç¨‹æ»¡è¶³æ¡ä»¶
bool result = __any_sync(0xFFFFFFFF, condition);  // ä»»ä¸€çº¿ç¨‹æ»¡è¶³æ¡ä»¶
unsigned mask = __ballot_sync(0xFFFFFFFF, condition);  // æ¡ä»¶æ©ç 

// Warpå†…æ•°æ®äº¤æ¢
float value = __shfl_sync(0xFFFFFFFF, var, srcLane);  // ä»æŒ‡å®šlaneè·å–å€¼
float value = __shfl_up_sync(0xFFFFFFFF, var, delta); // ä»ä¸‹æ–¹laneè·å–
float value = __shfl_down_sync(0xFFFFFFFF, var, delta); // ä»ä¸Šæ–¹laneè·å–
float value = __shfl_xor_sync(0xFFFFFFFF, var, laneMask); // XORäº¤æ¢
```

---

## 5. åŸå­æ“ä½œ

### 5.1 ä»€ä¹ˆæ˜¯åŸå­æ“ä½œ

åŸå­æ“ä½œæ˜¯**ä¸å¯åˆ†å‰²çš„æ“ä½œ**ï¼Œä¿è¯åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„æ­£ç¡®æ€§ã€‚

```cpp
// ä½¿ç”¨åŸå­åŠ æ³•
atomicAdd(address, value);
```

### 5.2 å¸¸ç”¨åŸå­æ“ä½œ

| å‡½æ•° | åŠŸèƒ½ | æ”¯æŒç±»å‹ |
|------|------|----------|
| `atomicAdd` | åŠ æ³• | int, float, double |
| `atomicSub` | å‡æ³• | int |
| `atomicMax` | æœ€å¤§å€¼ | int |
| `atomicMin` | æœ€å°å€¼ | int |
| `atomicExch` | äº¤æ¢ | int, float |
| `atomicCAS` | æ¯”è¾ƒå¹¶äº¤æ¢ | int, unsigned |
| `atomicAnd/Or/Xor` | ä½è¿ç®— | int |

### 5.3 åŸå­æ“ä½œç¤ºä¾‹

```cpp
__global__ void histogram(int* data, int* hist, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        int bin = data[idx];
        atomicAdd(&hist[bin], 1);  // åŸå­é€’å¢
    }
}
```

### 5.4 åŸå­æ“ä½œçš„ä»£ä»·

- **æ€§èƒ½å¼€é”€å¤§**ï¼šéœ€è¦ä¸²è¡ŒåŒ–å†²çªçš„è®¿é—®
- **åº”å°½é‡å‡å°‘ä½¿ç”¨**ï¼š
  - å…ˆåœ¨Shared Memoryä¸­å±€éƒ¨ç´¯åŠ 
  - æœ€åä¸€æ¬¡åŸå­æ“ä½œå†™å…¥Global Memory

```cpp
// ä¼˜åŒ–çš„ç›´æ–¹å›¾
__global__ void histogramOptimized(int* data, int* hist, int n) {
    __shared__ int localHist[256];
    
    // åˆå§‹åŒ–å±€éƒ¨ç›´æ–¹å›¾
    if (threadIdx.x < 256) localHist[threadIdx.x] = 0;
    __syncthreads();
    
    // åœ¨Shared Memoryä¸­ç´¯åŠ 
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        atomicAdd(&localHist[data[idx]], 1);
    }
    __syncthreads();
    
    // åˆå¹¶åˆ°å…¨å±€ç›´æ–¹å›¾
    if (threadIdx.x < 256) {
        atomicAdd(&hist[threadIdx.x], localHist[threadIdx.x]);
    }
}
```

---

## 6. ç»å…¸æ¨¡å¼ï¼šå¹¶è¡Œè§„çº¦ï¼ˆReductionï¼‰

è§„çº¦æ˜¯å±•ç¤ºåŒæ­¥é‡è¦æ€§çš„ç»å…¸ä¾‹å­ã€‚

### 6.1 é—®é¢˜ï¼šæ±‚æ•°ç»„å…ƒç´ ä¹‹å’Œ

```cpp
// ä¸²è¡Œç‰ˆæœ¬
int sum = 0;
for (int i = 0; i < n; i++) {
    sum += data[i];
}
```

### 6.2 å¹¶è¡Œè§„çº¦ç®—æ³•

```
åˆå§‹:     [1] [2] [3] [4] [5] [6] [7] [8]
Step 1:   [3]     [7]     [11]    [15]    (ç›¸é‚»ç›¸åŠ )
Step 2:   [10]            [26]            
Step 3:   [36]                            (æœ€ç»ˆç»“æœ)
```

### 6.3 å®ç°

```cpp
__global__ void reduce(float* input, float* output, int n) {
    __shared__ float sdata[256];
    
    int tid = threadIdx.x;
    int gid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // åŠ è½½åˆ°å…±äº«å†…å­˜
    sdata[tid] = (gid < n) ? input[gid] : 0.0f;
    __syncthreads();
    
    // è§„çº¦
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();  // æ¯æ­¥éƒ½éœ€è¦åŒæ­¥
    }
    
    // å†™å›ç»“æœ
    if (tid == 0) {
        output[blockIdx.x] = sdata[0];
    }
}
```

---

## 7. Blocké—´åŒæ­¥

### 7.1 CUDAæ²¡æœ‰ç›´æ¥çš„Gridçº§åŒæ­¥

Blockä¹‹é—´æ‰§è¡Œé¡ºåºä¸ç¡®å®šï¼Œæ— æ³•ç›´æ¥åŒæ­¥ã€‚

### 7.2 è§£å†³æ–¹æ¡ˆ

1. **å¤šæ¬¡Kernelè°ƒç”¨**
   ```cpp
   kernel1<<<grid, block>>>();
   cudaDeviceSynchronize();  // Hostç«¯åŒæ­¥
   kernel2<<<grid, block>>>();
   ```

2. **åä½œç»„ï¼ˆCooperative Groupsï¼‰** - CUDA 9+
   ```cpp
   #include <cooperative_groups.h>
   namespace cg = cooperative_groups;
   
   __global__ void kernel() {
       cg::grid_group grid = cg::this_grid();
       // ...
       grid.sync();  // Gridçº§åŒæ­¥
   }
   
   // å¯åŠ¨æ—¶éœ€è¦ç‰¹æ®Šæ–¹å¼
   void* args[] = {...};
   cudaLaunchCooperativeKernel((void*)kernel, grid, block, args);
   ```

3. **å…¨å±€å†…å­˜æ ‡å¿— + åŸå­æ“ä½œ**
   ```cpp
   // ç­‰å¾…æ‰€æœ‰Blockåˆ°è¾¾
   __global__ void waitForAll(int* counter, int numBlocks) {
       if (threadIdx.x == 0) {
           atomicAdd(counter, 1);
           while (atomicAdd(counter, 0) < numBlocks);  // å¿™ç­‰å¾…
       }
       __syncthreads();
   }
   ```

---

## 8. ä¸FlashAttentionçš„å…³è”

FlashAttentionä¸­çš„åŒæ­¥ä½¿ç”¨ï¼š

```cpp
// å…¸å‹æ¨¡å¼
// 1. åŠ è½½K/Vå—åˆ°Shared Memory
flash::copy(gK, sK);
flash::copy(gV, sV);

// 2. åŒæ­¥ç¡®ä¿åŠ è½½å®Œæˆ
__syncthreads();

// 3. åœ¨Shared Memoryä¸­è¿›è¡Œè®¡ç®—
flash::gemm(acc_s, sQ, sK);  // Q @ K^T

// 4. å†æ¬¡åŒæ­¥
__syncthreads();

// 5. åŠ è½½ä¸‹ä¸€å—æˆ–å†™å›ç»“æœ
```

å…³é”®ç‚¹ï¼š
- æ•°æ®åŠ è½½åå¿…é¡»åŒæ­¥
- è®¡ç®—å‰ç¡®ä¿æ•°æ®å¯è§
- åˆç†å®‰æ’åŒæ­¥ç‚¹ï¼Œé¿å…è¿‡åº¦åŒæ­¥

---

## 9. æ€»ç»“

| åŒæ­¥æ–¹å¼ | èŒƒå›´ | ç”¨é€” |
|----------|------|------|
| `__syncthreads()` | Blockå†… | å…±äº«å†…å­˜è®¿é—®åŒæ­¥ |
| `__syncwarp()` | Warpå†… | Warpçº§æ•°æ®äº¤æ¢ |
| åŸå­æ“ä½œ | å…¨å±€ | æ— å†²çªçš„è¯»-æ”¹-å†™ |
| `cudaDeviceSynchronize()` | Host-Device | ç­‰å¾…GPUå®Œæˆ |
| åä½œç»„ | Grid | å…¨å±€åŒæ­¥ |

**æœ€ä½³å®è·µï¼š**
1. å†™å…¥å…±äº«å†…å­˜åã€è¯»å–å‰å¿…é¡»åŒæ­¥
2. æ‰€æœ‰çº¿ç¨‹å¿…é¡»æ‰§è¡Œç›¸åŒçš„`__syncthreads()`
3. ä¼˜å…ˆä½¿ç”¨Shared Memory + åŒæ­¥ï¼Œè€ŒéåŸå­æ“ä½œ
4. åˆ©ç”¨Warpå†…éšå¼åŒæ­¥ä¼˜åŒ–æ€§èƒ½

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [CUDA C++ Programming Guide - Synchronization](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions)
- [Cooperative Groups](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups)

