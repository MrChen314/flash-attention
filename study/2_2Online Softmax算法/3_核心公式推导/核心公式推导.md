# 核心公式推导

> FlashAttention中Online Softmax的完整数学推导

---

## 1. 符号定义

### 1.1 基本符号

| 符号 | 含义 |
|------|------|
| $x = [x_1, x_2, ..., x_n]$ | 输入向量（如注意力分数） |
| $m$ | 最大值：$m = \max_i(x_i)$ |
| $l$ | 归一化因子：$l = \sum_i e^{x_i - m}$ |
| $\text{softmax}(x)_i$ | Softmax输出的第 $i$ 个元素 |

### 1.2 分块符号

假设将数据分成 $K$ 个块：

| 符号 | 含义 |
|------|------|
| $x^{(k)}$ | 第 $k$ 个块的数据 |
| $m^{(k)}$ | 第 $k$ 个块的局部最大值 |
| $l^{(k)}$ | 第 $k$ 个块的局部累加和 |
| $m_{1:k}$ | 前 $k$ 个块的全局最大值 |
| $l_{1:k}$ | 前 $k$ 个块的全局累加和 |

---

## 2. 基础公式推导

### 2.1 数值稳定的Softmax

**定理 1（平移不变性）：** 对任意常数 $c$，有
$$
\text{softmax}(x)_i = \text{softmax}(x - c)_i
$$

**证明：**
$$
\text{softmax}(x - c)_i = \frac{e^{x_i - c}}{\sum_j e^{x_j - c}} = \frac{e^{x_i} \cdot e^{-c}}{\sum_j e^{x_j} \cdot e^{-c}} = \frac{e^{x_i}}{\sum_j e^{x_j}} = \text{softmax}(x)_i
$$

**推论：** 选择 $c = \max_i(x_i)$ 可避免数值溢出。

### 2.2 Softmax的分解形式

令 $m = \max_i(x_i)$，$l = \sum_i e^{x_i - m}$，则：

$$
\text{softmax}(x)_i = \frac{e^{x_i - m}}{l}
$$

这意味着只需要存储 $(m, l)$ 就能计算任意元素的Softmax值。

---

## 3. Online更新推导

### 3.1 两块合并

假设有两块数据：
- 块1：最大值 $m^{(1)}$，累加和 $l^{(1)} = \sum_{x \in \text{块1}} e^{x - m^{(1)}}$
- 块2：最大值 $m^{(2)}$，累加和 $l^{(2)} = \sum_{x \in \text{块2}} e^{x - m^{(2)}}$

**目标：** 计算全局的 $(m_{1:2}, l_{1:2})$

**Step 1：计算全局最大值**
$$
m_{1:2} = \max(m^{(1)}, m^{(2)})
$$

**Step 2：计算全局累加和**

全局累加和的定义：
$$
l_{1:2} = \sum_{\text{所有} x} e^{x - m_{1:2}}
$$

分成两部分：
$$
l_{1:2} = \sum_{x \in \text{块1}} e^{x - m_{1:2}} + \sum_{x \in \text{块2}} e^{x - m_{1:2}}
$$

对于块1的贡献：
$$
\sum_{x \in \text{块1}} e^{x - m_{1:2}} = \sum_{x \in \text{块1}} e^{x - m^{(1)}} \cdot e^{m^{(1)} - m_{1:2}} = l^{(1)} \cdot e^{m^{(1)} - m_{1:2}}
$$

同理，块2的贡献：
$$
\sum_{x \in \text{块2}} e^{x - m_{1:2}} = l^{(2)} \cdot e^{m^{(2)} - m_{1:2}}
$$

因此：
$$
\boxed{l_{1:2} = e^{m^{(1)} - m_{1:2}} \cdot l^{(1)} + e^{m^{(2)} - m_{1:2}} \cdot l^{(2)}}
$$

### 3.2 递推形式

更一般地，已知前 $k-1$ 块的状态 $(m_{1:k-1}, l_{1:k-1})$，加入第 $k$ 块 $(m^{(k)}, l^{(k)})$：

$$
\begin{aligned}
m_{1:k} &= \max(m_{1:k-1}, m^{(k)}) \\
l_{1:k} &= e^{m_{1:k-1} - m_{1:k}} \cdot l_{1:k-1} + e^{m^{(k)} - m_{1:k}} \cdot l^{(k)}
\end{aligned}
$$

---

## 4. 带输出的Online更新

### 4.1 Attention输出的定义

标准Attention的输出：
$$
O = \text{softmax}(S) \cdot V = P \cdot V
$$

其中 $S$ 是注意力分数，$V$ 是Value矩阵。

### 4.2 未归一化的输出

定义**未归一化输出**：
$$
\tilde{O} = \sum_j e^{S_j - m} \cdot V_j
$$

则最终输出为：
$$
O = \frac{\tilde{O}}{l}
$$

### 4.3 块的贡献

对于第 $k$ 块：
- 分数：$S^{(k)}$
- 局部最大值：$m^{(k)} = \max(S^{(k)})$
- 局部累加和：$l^{(k)} = \sum_j e^{S^{(k)}_j - m^{(k)}}$
- 局部未归一化输出：$\tilde{O}^{(k)} = \sum_j e^{S^{(k)}_j - m^{(k)}} \cdot V^{(k)}_j$

### 4.4 输出的合并公式

**定理 2：** 未归一化输出的合并遵循与累加和相同的规则：

$$
\tilde{O}_{1:k} = e^{m_{1:k-1} - m_{1:k}} \cdot \tilde{O}_{1:k-1} + e^{m^{(k)} - m_{1:k}} \cdot \tilde{O}^{(k)}
$$

**证明：**
$$
\begin{aligned}
\tilde{O}_{1:k} &= \sum_{\text{所有} j} e^{S_j - m_{1:k}} \cdot V_j \\
&= \sum_{j \in \text{块1到k-1}} e^{S_j - m_{1:k}} \cdot V_j + \sum_{j \in \text{块k}} e^{S_j - m_{1:k}} \cdot V_j \\
&= e^{m_{1:k-1} - m_{1:k}} \cdot \tilde{O}_{1:k-1} + e^{m^{(k)} - m_{1:k}} \cdot \tilde{O}^{(k)}
\end{aligned}
$$

---

## 5. 完整的FlashAttention更新公式

### 5.1 状态变量

维护三元组 $(m, l, \tilde{O})$：
- $m$：当前全局最大值
- $l$：当前全局累加和
- $\tilde{O}$：当前未归一化输出

### 5.2 每块的更新规则

```
输入: 当前状态 (m_old, l_old, Õ_old)
      新块数据: K_block, V_block
      查询: Q

Step 1: 计算块内统计量
        S_block = Q @ K_block.T
        m_block = max(S_block)
        P_block = exp(S_block - m_block)
        l_block = sum(P_block)
        Õ_block = P_block @ V_block

Step 2: 更新全局状态
        m_new = max(m_old, m_block)
        α = exp(m_old - m_new)      # 旧状态的缩放因子
        β = exp(m_block - m_new)    # 新块的缩放因子
        l_new = α * l_old + β * l_block
        Õ_new = α * Õ_old + β * Õ_block

输出: 更新后的状态 (m_new, l_new, Õ_new)
```

### 5.3 最终归一化

处理完所有块后：
$$
O = \frac{\tilde{O}}{l}
$$

---

## 6. 数学性质分析

### 6.1 缩放因子的性质

令 $\alpha = e^{m_{old} - m_{new}}$，$\beta = e^{m_{block} - m_{new}}$

**性质1：** $\alpha, \beta \in (0, 1]$

因为 $m_{new} = \max(m_{old}, m_{block})$，所以：
- $m_{old} - m_{new} \leq 0 \Rightarrow \alpha \leq 1$
- $m_{block} - m_{new} \leq 0 \Rightarrow \beta \leq 1$

**性质2：** $\max(\alpha, \beta) = 1$

当 $m_{new} = m_{old}$ 时 $\alpha = 1$；当 $m_{new} = m_{block}$ 时 $\beta = 1$。

这保证了数值稳定性：所有exp运算的参数都 ≤ 0。

### 6.2 正确性证明

**定理 3：** Online算法的输出与标准算法完全一致。

**证明思路：**
1. 归纳证明 $l_{1:k}$ 始终等于真实的累加和
2. 归纳证明 $\tilde{O}_{1:k}$ 始终等于真实的未归一化输出
3. 最终 $O = \tilde{O} / l$ 给出正确结果

（详细证明见3.3节的归纳证明）

---

## 7. 代码实现

### 7.1 简化的Python实现

```python
import numpy as np

def online_softmax_attention(Q, K, V, block_size):
    """
    使用Online Softmax的注意力计算
    
    Args:
        Q: (seq_len, d) 查询矩阵
        K: (seq_len, d) 键矩阵
        V: (seq_len, d) 值矩阵
        block_size: 块大小
    
    Returns:
        O: (seq_len, d) 输出矩阵
    """
    seq_len, d = Q.shape
    
    # 初始化状态
    O_tilde = np.zeros((seq_len, d))
    l = np.zeros(seq_len)
    m = np.full(seq_len, -np.inf)
    
    # 按块处理
    for j in range(0, seq_len, block_size):
        j_end = min(j + block_size, seq_len)
        K_j = K[j:j_end]
        V_j = V[j:j_end]
        
        # 计算块内分数
        S_j = Q @ K_j.T  # (seq_len, block_size)
        
        # 块内统计量
        m_j = np.max(S_j, axis=1)
        P_j = np.exp(S_j - m_j[:, None])
        l_j = np.sum(P_j, axis=1)
        O_j = P_j @ V_j
        
        # 更新状态
        m_new = np.maximum(m, m_j)
        alpha = np.exp(m - m_new)
        beta = np.exp(m_j - m_new)
        
        l = alpha * l + beta * l_j
        O_tilde = alpha[:, None] * O_tilde + beta[:, None] * O_j
        m = m_new
    
    # 归一化
    O = O_tilde / l[:, None]
    
    return O
```

### 7.2 验证正确性

```python
def standard_attention(Q, K, V):
    """标准Attention实现"""
    S = Q @ K.T
    P = np.exp(S - S.max(axis=1, keepdims=True))
    P = P / P.sum(axis=1, keepdims=True)
    return P @ V

# 测试
np.random.seed(42)
seq_len, d = 64, 32
Q = np.random.randn(seq_len, d)
K = np.random.randn(seq_len, d)
V = np.random.randn(seq_len, d)

O_standard = standard_attention(Q, K, V)
O_online = online_softmax_attention(Q, K, V, block_size=16)

print(f"最大误差: {np.max(np.abs(O_standard - O_online))}")
# 输出: 最大误差: 约1e-15（浮点误差级别）
```

---

## 8. 总结

### 核心公式一览

| 公式 | 描述 |
|------|------|
| $m_{new} = \max(m_{old}, m_{block})$ | 最大值更新 |
| $l_{new} = e^{m_{old} - m_{new}} l_{old} + e^{m_{block} - m_{new}} l_{block}$ | 累加和更新 |
| $\tilde{O}_{new} = e^{m_{old} - m_{new}} \tilde{O}_{old} + e^{m_{block} - m_{new}} \tilde{O}_{block}$ | 未归一化输出更新 |
| $O = \tilde{O} / l$ | 最终归一化 |

### 关键洞察

1. **缩放因子保证数值稳定**
   - 所有exp参数都 ≤ 0
   - 避免上溢

2. **状态合并的代数结构**
   - 满足结合律：$(A \oplus B) \oplus C = A \oplus (B \oplus C)$
   - 可以按任意顺序合并块

3. **内存效率**
   - 只需存储 $(m, l, \tilde{O})$
   - 不需要存储完整的 $S$ 和 $P$ 矩阵

---

**下一节：** [LSE_Log-Sum-Exp](../4_LSE_Log-Sum-Exp/) - 理解LSE在FlashAttention中的作用


