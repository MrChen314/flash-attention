{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Online Softmax增量计算 - 实践篇\n",
        "\n",
        "本notebook通过实际代码帮助你理解Online Softmax算法。\n",
        "\n",
        "**学习目标：**\n",
        "- 实现逐元素的Online Softmax\n",
        "- 实现块级Online Softmax\n",
        "- 验证Online算法与标准算法的结果一致\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(f\"NumPy版本: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 标准Softmax（作为对照）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_softmax(x):\n",
        "    \"\"\"标准的数值稳定Softmax\"\"\"\n",
        "    m = np.max(x)\n",
        "    exp_x = np.exp(x - m)\n",
        "    l = np.sum(exp_x)\n",
        "    return exp_x / l\n",
        "\n",
        "# 测试\n",
        "x = np.array([2.0, 1.0, 3.0, 0.5, 2.5])\n",
        "result = standard_softmax(x)\n",
        "print(f\"输入: {x}\")\n",
        "print(f\"输出: {result}\")\n",
        "print(f\"验证和为1: {np.sum(result):.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 逐元素Online Softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def online_softmax_element_by_element(x, verbose=False):\n",
        "    \"\"\"\n",
        "    逐元素的Online Softmax\n",
        "    \n",
        "    核心思想：\n",
        "    - 维护运行时状态 (m, l)\n",
        "    - m: 当前最大值\n",
        "    - l: 当前（相对于m的）累加和\n",
        "    \n",
        "    更新规则：\n",
        "    - m_new = max(m, x_i)\n",
        "    - l_new = l * exp(m - m_new) + exp(x_i - m_new)\n",
        "    \"\"\"\n",
        "    n = len(x)\n",
        "    \n",
        "    # 初始化：处理第一个元素\n",
        "    m = x[0]  # 最大值\n",
        "    l = 1.0   # exp(x[0] - m) = exp(0) = 1\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"逐元素Online Softmax过程\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"初始化: x[0]={x[0]:.2f}, m={m:.2f}, l={l:.4f}\")\n",
        "    \n",
        "    # 遍历剩余元素\n",
        "    for i in range(1, n):\n",
        "        m_prev = m\n",
        "        m = max(m, x[i])  # 更新最大值\n",
        "        \n",
        "        # 关键：缩放之前的累加和\n",
        "        l = l * np.exp(m_prev - m) + np.exp(x[i] - m)\n",
        "        \n",
        "        if verbose:\n",
        "            scale_factor = np.exp(m_prev - m)\n",
        "            print(f\"处理 x[{i}]={x[i]:.2f}:\")\n",
        "            print(f\"  m: {m_prev:.2f} → {m:.2f}\")\n",
        "            print(f\"  缩放因子: exp({m_prev:.2f}-{m:.2f}) = {scale_factor:.4f}\")\n",
        "            print(f\"  l: {l:.4f}\")\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n最终状态: m={m:.2f}, l={l:.4f}\")\n",
        "    \n",
        "    # 计算最终结果\n",
        "    result = np.exp(x - m) / l\n",
        "    return result, m, l\n",
        "\n",
        "# 测试\n",
        "x = np.array([2.0, 1.0, 3.0, 0.5, 2.5])\n",
        "result_online, m, l = online_softmax_element_by_element(x, verbose=True)\n",
        "\n",
        "print(f\"\\n结果: {result_online}\")\n",
        "print(f\"验证和为1: {np.sum(result_online):.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 验证Online结果与标准结果一致\n",
        "print(\"=\" * 50)\n",
        "print(\"验证Online Softmax与标准Softmax结果一致\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "x = np.array([2.0, 1.0, 3.0, 0.5, 2.5])\n",
        "result_standard = standard_softmax(x)\n",
        "result_online, _, _ = online_softmax_element_by_element(x, verbose=False)\n",
        "\n",
        "print(f\"\\n标准Softmax: {result_standard}\")\n",
        "print(f\"Online Softmax: {result_online}\")\n",
        "print(f\"最大差异: {np.max(np.abs(result_standard - result_online)):.2e}\")\n",
        "print(\"\\n✓ 结果完全一致！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 块级Online Softmax\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
