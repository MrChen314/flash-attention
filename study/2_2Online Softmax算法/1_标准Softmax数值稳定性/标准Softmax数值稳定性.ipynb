{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 标准Softmax数值稳定性 - 实践篇\n",
        "\n",
        "本notebook通过实际代码帮助你理解Softmax的数值稳定性问题。\n",
        "\n",
        "**学习目标：**\n",
        "- 观察直接计算Softmax的数值溢出问题\n",
        "- 实现数值稳定的Softmax\n",
        "- 验证减去最大值技巧的正确性\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(f\"NumPy版本: {np.__version__}\")\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Softmax基础\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax_formula(x):\n",
        "    \"\"\"\n",
        "    Softmax公式: softmax(x)_i = exp(x_i) / sum(exp(x_j))\n",
        "    \n",
        "    这是直接按定义实现的版本，有数值稳定性问题！\n",
        "    \"\"\"\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "# 测试正常情况\n",
        "x = np.array([1.0, 2.0, 3.0])\n",
        "result = softmax_formula(x)\n",
        "\n",
        "print(\"输入:\", x)\n",
        "print(\"Softmax输出:\", result)\n",
        "print(\"输出之和:\", np.sum(result))\n",
        "print(\"\\n✓ 正常情况下工作正常\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 数值溢出问题\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试大数情况\n",
        "print(\"=\" * 50)\n",
        "print(\"测试大数情况\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "x_large = np.array([1000.0, 1001.0, 1002.0])\n",
        "\n",
        "print(f\"\\n输入: {x_large}\")\n",
        "print(f\"exp(1000) = {np.exp(1000.0)}\")\n",
        "print(f\"exp(1001) = {np.exp(1001.0)}\")\n",
        "print(f\"exp(1002) = {np.exp(1002.0)}\")\n",
        "\n",
        "result_large = softmax_formula(x_large)\n",
        "print(f\"\\nSoftmax输出: {result_large}\")\n",
        "print(\"\\n✗ 输出是NaN，计算失败！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试小数情况（负数）\n",
        "print(\"=\" * 50)\n",
        "print(\"测试极小数情况\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "x_small = np.array([-1000.0, -1001.0, -1002.0])\n",
        "\n",
        "print(f\"\\n输入: {x_small}\")\n",
        "print(f\"exp(-1000) = {np.exp(-1000.0)}\")\n",
        "print(f\"exp(-1001) = {np.exp(-1001.0)}\")\n",
        "\n",
        "result_small = softmax_formula(x_small)\n",
        "print(f\"\\nSoftmax输出: {result_small}\")\n",
        "print(\"\\n✗ 输出是NaN（0/0的结果）\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 数值稳定的Softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stable_softmax(x):\n",
        "    \"\"\"\n",
        "    数值稳定的Softmax实现\n",
        "    \n",
        "    关键技巧：减去最大值\n",
        "    softmax(x - max(x)) = softmax(x)\n",
        "    \"\"\"\n",
        "    m = np.max(x)           # 找最大值\n",
        "    exp_x = np.exp(x - m)   # 减去最大值后再exp\n",
        "    l = np.sum(exp_x)       # 求和\n",
        "    return exp_x / l\n",
        "\n",
        "# 验证数学等价性\n",
        "print(\"=\" * 50)\n",
        "print(\"验证：减去最大值不改变结果\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "x = np.array([1.0, 2.0, 3.0])\n",
        "\n",
        "result_naive = softmax_formula(x)\n",
        "result_stable = stable_softmax(x)\n",
        "\n",
        "print(f\"\\n输入: {x}\")\n",
        "print(f\"直接计算: {result_naive}\")\n",
        "print(f\"稳定版本: {result_stable}\")\n",
        "print(f\"差异: {np.max(np.abs(result_naive - result_stable))}\")\n",
        "print(\"\\n✓ 结果完全一致！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试稳定版本处理大数\n",
        "print(\"=\" * 50)\n",
        "print(\"稳定版本处理大数\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "x_large = np.array([1000.0, 1001.0, 1002.0])\n",
        "\n",
        "print(f\"\\n输入: {x_large}\")\n",
        "print(f\"最大值: {np.max(x_large)}\")\n",
        "print(f\"减去最大值后: {x_large - np.max(x_large)}\")\n",
        "\n",
        "result_stable = stable_softmax(x_large)\n",
        "print(f\"\\nSoftmax输出: {result_stable}\")\n",
        "print(f\"输出之和: {np.sum(result_stable)}\")\n",
        "print(\"\\n✓ 正确计算！\")\n",
        "\n",
        "# 与小数版本对比\n",
        "x_small_equiv = np.array([0.0, 1.0, 2.0])  # [1000, 1001, 1002] - 1000\n",
        "print(f\"\\n对比 [0, 1, 2] 的结果: {stable_softmax(x_small_equiv)}\")\n",
        "print(\"✓ 结果一致，验证平移不变性\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 可视化对比\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化减去最大值的效果\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = np.exp(x)\n",
        "\n",
        "# 图1: exp函数\n",
        "axes[0].plot(x, y, 'b-', linewidth=2)\n",
        "axes[0].axhline(y=1, color='r', linestyle='--', alpha=0.5)\n",
        "axes[0].axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
        "axes[0].set_xlabel('x')\n",
        "axes[0].set_ylabel('exp(x)')\n",
        "axes[0].set_title('指数函数 exp(x)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_ylim([0, 50])\n",
        "\n",
        "# 图2: 原始值的exp\n",
        "x_orig = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "exp_orig = np.exp(x_orig)\n",
        "axes[1].bar(range(len(x_orig)), exp_orig, color='steelblue', alpha=0.7)\n",
        "axes[1].set_xlabel('索引')\n",
        "axes[1].set_ylabel('exp(x)')\n",
        "axes[1].set_title('原始值的exp')\n",
        "for i, v in enumerate(exp_orig):\n",
        "    axes[1].text(i, v + 2, f'{v:.1f}', ha='center')\n",
        "\n",
        "# 图3: 减去最大值后的exp\n",
        "x_shifted = x_orig - np.max(x_orig)\n",
        "exp_shifted = np.exp(x_shifted)\n",
        "axes[2].bar(range(len(x_shifted)), exp_shifted, color='coral', alpha=0.7)\n",
        "axes[2].set_xlabel('索引')\n",
        "axes[2].set_ylabel('exp(x - max)')\n",
        "axes[2].set_title('减去最大值后的exp')\n",
        "for i, v in enumerate(exp_shifted):\n",
        "    axes[2].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"关键：减去最大值后，所有exp值都 ≤ 1，不会溢出！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 与PyTorch对比验证\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 与PyTorch的实现对比\n",
        "print(\"=\" * 50)\n",
        "print(\"与PyTorch实现对比\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "test_cases = [\n",
        "    np.array([1.0, 2.0, 3.0]),\n",
        "    np.array([100.0, 200.0, 300.0]),\n",
        "    np.array([-1.0, 0.0, 1.0]),\n",
        "    np.random.randn(10) * 100,\n",
        "]\n",
        "\n",
        "for i, x in enumerate(test_cases):\n",
        "    x_torch = torch.tensor(x, dtype=torch.float32)\n",
        "    \n",
        "    our_result = stable_softmax(x)\n",
        "    torch_result = F.softmax(x_torch, dim=0).numpy()\n",
        "    \n",
        "    max_diff = np.max(np.abs(our_result - torch_result))\n",
        "    \n",
        "    print(f\"\\n测试 {i+1}:\")\n",
        "    print(f\"  输入范围: [{x.min():.2f}, {x.max():.2f}]\")\n",
        "    print(f\"  最大差异: {max_diff:.2e}\")\n",
        "    print(f\"  ✓ 通过\" if max_diff < 1e-6 else f\"  ✗ 失败\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 分块处理的问题\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 演示分块处理的问题\n",
        "print(\"=\" * 50)\n",
        "print(\"分块处理的问题\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 假设有一个长序列\n",
        "full_sequence = np.array([1.0, 3.0, 2.0, 5.0, 4.0, 6.0])\n",
        "print(f\"\\n完整序列: {full_sequence}\")\n",
        "print(f\"正确的Softmax结果: {stable_softmax(full_sequence)}\")\n",
        "\n",
        "# 分成两块处理\n",
        "block1 = full_sequence[:3]  # [1, 3, 2]\n",
        "block2 = full_sequence[3:]  # [5, 4, 6]\n",
        "\n",
        "print(f\"\\n分块:\")\n",
        "print(f\"  Block 1: {block1}, 局部最大值 = {np.max(block1)}\")\n",
        "print(f\"  Block 2: {block2}, 局部最大值 = {np.max(block2)}\")\n",
        "\n",
        "# 如果用局部最大值计算\n",
        "result1_wrong = stable_softmax(block1)\n",
        "result2_wrong = stable_softmax(block2)\n",
        "\n",
        "print(f\"\\n如果独立处理每个块:\")\n",
        "print(f\"  Block 1的Softmax: {result1_wrong}\")\n",
        "print(f\"  Block 2的Softmax: {result2_wrong}\")\n",
        "\n",
        "# 拼接结果\n",
        "wrong_result = np.concatenate([result1_wrong, result2_wrong])\n",
        "print(f\"\\n拼接结果: {wrong_result}\")\n",
        "print(f\"拼接结果之和: {np.sum(wrong_result):.4f}\")\n",
        "print(\"\\n✗ 问题：和不等于1，结果错误！\")\n",
        "print(\"\\n原因：每个块使用了不同的局部最大值和局部归一化因子\")\n",
        "print(\"\\n解决方案：需要Online Softmax算法来正确合并各块的结果！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 总结\n",
        "\n",
        "### 关键点\n",
        "\n",
        "1. **直接计算Softmax存在数值溢出风险**\n",
        "   - 大正数 → exp上溢为inf\n",
        "   - 大负数 → exp下溢为0\n",
        "   - 导致NaN结果\n",
        "\n",
        "2. **解决方案：减去最大值**\n",
        "   - 利用Softmax的平移不变性\n",
        "   - 保证所有exp值在[0, 1]范围内\n",
        "\n",
        "3. **标准实现需要三遍遍历**\n",
        "   - Pass 1: 找最大值\n",
        "   - Pass 2: 计算归一化因子\n",
        "   - Pass 3: 计算输出\n",
        "\n",
        "4. **分块处理的挑战**\n",
        "   - 无法预知全局最大值\n",
        "   - 独立处理各块会导致错误结果\n",
        "   - 需要Online Softmax来解决\n",
        "\n",
        "### 下一步\n",
        "\n",
        "学习 **Online Softmax**，了解如何在只看到部分数据的情况下正确计算Softmax！\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
