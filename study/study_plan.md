# FlashAttention-2 C/CUDA ä»£ç å­¦ä¹ è®¡åˆ’

> é€‚ç”¨äºCUDAåˆå­¦è€…ï¼Œå¾ªåºæ¸è¿›æŒæ¡FlashAttention-2çš„å®ç°åŸç†

---

## ğŸ“š ç¬¬ä¸€é˜¶æ®µï¼šå‰ç½®çŸ¥è¯†å‡†å¤‡ï¼ˆ1-2å‘¨ï¼‰

### 1.1 CUDAç¼–ç¨‹åŸºç¡€
**ç›®æ ‡ï¼š** ç†è§£GPUå¹¶è¡Œè®¡ç®—çš„åŸºæœ¬æ¦‚å¿µ

**å­¦ä¹ å†…å®¹ï¼š**
- [ ] GPUæ¶æ„åŸºç¡€ï¼ˆSMã€Warpã€Threadï¼‰
- [ ] CUDAç¼–ç¨‹æ¨¡å‹ï¼ˆGrid â†’ Block â†’ Threadå±‚æ¬¡ç»“æ„ï¼‰
- [ ] å†…å­˜å±‚æ¬¡ç»“æ„ï¼ˆGlobal Memoryã€Shared Memoryã€Registersï¼‰
- [ ] çº¿ç¨‹åŒæ­¥ï¼ˆ`__syncthreads()`ï¼‰
- [ ] å¼‚æ­¥å†…å­˜æ‹·è´ï¼ˆ`cp.async`ï¼‰

**æ¨èèµ„æºï¼š**
- NVIDIA CUDA C++ Programming Guideï¼ˆå®˜æ–¹æ–‡æ¡£ï¼‰
- ã€ŠCUDA by Exampleã€‹å…¥é—¨ä¹¦ç±
- å®è·µï¼šç¼–å†™ä¸€ä¸ªç®€å•çš„çŸ©é˜µä¹˜æ³•CUDA kernel

### 1.2 C++æ¨¡æ¿å…ƒç¼–ç¨‹åŸºç¡€
**ç›®æ ‡ï¼š** ç†è§£FlashAttentionä»£ç ä¸­å¤§é‡ä½¿ç”¨çš„æ¨¡æ¿æŠ€æœ¯

**å­¦ä¹ å†…å®¹ï¼š**
- [ ] æ¨¡æ¿å‡½æ•°ä¸æ¨¡æ¿ç±»
- [ ] æ¨¡æ¿ç‰¹åŒ–ä¸SFINAE
- [ ] `constexpr` ç¼–è¯‘æœŸè®¡ç®—
- [ ] `if constexpr` ç¼–è¯‘æœŸåˆ†æ”¯

### 1.3 Attentionæœºåˆ¶åŸç†
**ç›®æ ‡ï¼š** æ·±å…¥ç†è§£æ ‡å‡†Attentionçš„è®¡ç®—è¿‡ç¨‹

**å­¦ä¹ å†…å®¹ï¼š**
- [ ] Self-Attentionå…¬å¼ï¼š`Attention(Q,K,V) = softmax(QK^T / âˆšd) Ã— V`
- [ ] Multi-Head Attention (MHA)
- [ ] Grouped-Query Attention (GQA) / Multi-Query Attention (MQA)
- [ ] Causal Maskï¼ˆå› æœæ©ç ï¼‰

---

## ğŸ“š ç¬¬äºŒé˜¶æ®µï¼šFlashAttentionç®—æ³•åŸç†ï¼ˆ1å‘¨ï¼‰

### 2.1 FlashAttentionæ ¸å¿ƒæ€æƒ³
**ç›®æ ‡ï¼š** ç†è§£ä¸ºä»€ä¹ˆéœ€è¦FlashAttentionä»¥åŠå®ƒçš„æ ¸å¿ƒä¼˜åŒ–æ€è·¯

**å­¦ä¹ å†…å®¹ï¼š**
- [ ] æ ‡å‡†Attentionçš„å†…å­˜ç“¶é¢ˆåˆ†æï¼ˆO(NÂ²)æ˜¾å­˜å ç”¨ï¼‰
- [ ] IO-Awareç®—æ³•è®¾è®¡æ€æƒ³
- [ ] Tilingï¼ˆåˆ†å—ï¼‰æŠ€æœ¯ï¼šå°†å¤§çŸ©é˜µåˆ†æˆå°å—åœ¨SRAMä¸­è®¡ç®—
- [ ] Recomputationï¼ˆé‡è®¡ç®—ï¼‰ç­–ç•¥

**æ¨èé˜…è¯»ï¼š**
- FlashAttentionè®ºæ–‡ï¼šhttps://arxiv.org/abs/2205.14135
- FlashAttention-2è®ºæ–‡ï¼šhttps://arxiv.org/abs/2307.08691

### 2.2 Online Softmaxç®—æ³• â­é‡è¦
**ç›®æ ‡ï¼š** è¿™æ˜¯FlashAttentionçš„æ ¸å¿ƒç®—æ³•ï¼Œå¿…é¡»æ·±å…¥ç†è§£

**å­¦ä¹ å†…å®¹ï¼š**
- [ ] æ ‡å‡†Softmaxçš„è®¡ç®—è¿‡ç¨‹åŠæ•°å€¼ç¨³å®šæ€§é—®é¢˜
- [ ] Online Softmaxï¼šå¦‚ä½•åœ¨ä¸çŸ¥é“å®Œæ•´åºåˆ—çš„æƒ…å†µä¸‹å¢é‡è®¡ç®—softmax
- [ ] æ ¸å¿ƒå…¬å¼æ¨å¯¼ï¼š
  ```
  m_new = max(m_old, m_block)           # æ›´æ–°æœ€å¤§å€¼
  l_new = exp(m_old - m_new) * l_old + exp(m_block - m_new) * l_block  # æ›´æ–°ç´¯åŠ å’Œ
  O_new = exp(m_old - m_new) * O_old + exp(m_block - m_new) * O_block  # æ›´æ–°è¾“å‡º
  ```
- [ ] LSE (Log-Sum-Exp) çš„ä½œç”¨ä¸è®¡ç®—

**å¯¹åº”ä»£ç æ–‡ä»¶ï¼š**
```
csrc/flash_attn/src/softmax.h
```

---

## ğŸ“š ç¬¬ä¸‰é˜¶æ®µï¼šCUTLASS/cuteåº“åŸºç¡€ï¼ˆ1-2å‘¨ï¼‰

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦CUTLASS/cute
**ç›®æ ‡ï¼š** ç†è§£FlashAttentionä¸ºä½•åŸºäºCUTLASSæ„å»º

**å­¦ä¹ å†…å®¹ï¼š**
- [ ] CUTLASSæ˜¯ä»€ä¹ˆï¼šNVIDIAçš„é«˜æ€§èƒ½CUDAæ¨¡æ¿åº“
- [ ] cuteæ˜¯ä»€ä¹ˆï¼šCUTLASS 3.xä¸­çš„å¼ é‡æŠ½è±¡å±‚
- [ ] ä¸ºä»€ä¹ˆé€‰æ‹©CUTLASSï¼šå°è£…äº†Tensor Coreæ“ä½œã€å†…å­˜è®¿é—®ä¼˜åŒ–

### 3.2 cuteæ ¸å¿ƒæ¦‚å¿µ â­å…³é”®
**ç›®æ ‡ï¼š** æŒæ¡FlashAttentionä»£ç ä¸­å¤§é‡ä½¿ç”¨çš„cuteæŠ½è±¡

**å­¦ä¹ å†…å®¹ï¼š**
- [ ] **Tensor**: å¤šç»´æ•°ç»„æŠ½è±¡
  ```cpp
  Tensor mQ = make_tensor(make_gmem_ptr(ptr), shape, stride);
  ```
- [ ] **Layout**: æè¿°æ•°æ®åœ¨å†…å­˜ä¸­çš„æ’å¸ƒ
  ```cpp
  Layout layout = make_layout(shape, stride);
  ```
- [ ] **TiledMMA**: å°è£…Tensor CoreçŸ©é˜µä¹˜æ³•
- [ ] **TiledCopy**: é«˜æ•ˆå†…å­˜æ‹·è´æŠ½è±¡
- [ ] **local_tile**: è·å–tensorçš„å±€éƒ¨tile

**æ¨èèµ„æºï¼š**
- CUTLASS cuteæ•™ç¨‹ï¼šhttps://github.com/NVIDIA/cutlass/tree/main/media/docs/cute
- åŠ¨æ‰‹å®è·µï¼šé˜…è¯»cutlass/examplesä¸­çš„cuteç¤ºä¾‹

### 3.3 å¸¸è§cuteæ“ä½œé€ŸæŸ¥
```cpp
// åˆ›å»ºå…¨å±€å†…å­˜tensor
Tensor gQ = make_tensor(make_gmem_ptr(ptr), shape, stride);

// åˆ›å»ºå…±äº«å†…å­˜tensor  
Tensor sQ = make_tensor(make_smem_ptr(smem_ptr), SmemLayout{});

// è·å–å±€éƒ¨tile
Tensor tile = local_tile(gQ, tile_shape, coord);

// åˆ†åŒºï¼ˆç”¨äºçº¿ç¨‹çº§å¹¶è¡Œï¼‰
Tensor tQgQ = gmem_thr_copy.partition_S(gQ);

// æ•°æ®æ‹·è´
cute::copy(src, dst);

// å¼‚æ­¥æ‹·è´æ …æ 
cute::cp_async_fence();
cute::cp_async_wait<N>();
```

---

## ğŸ“š ç¬¬å››é˜¶æ®µï¼šä»£ç ç»“æ„æ€»è§ˆï¼ˆ3å¤©ï¼‰

### 4.1 ä»£ç ç›®å½•ç»“æ„
```
csrc/flash_attn/
â”œâ”€â”€ flash_api.cpp              # â­ APIå…¥å£ï¼ŒPyTorchç»‘å®š
â””â”€â”€ src/
    â”œâ”€â”€ flash.h                # â­ æ ¸å¿ƒæ•°æ®ç»“æ„å®šä¹‰
    â”œâ”€â”€ flash_fwd_kernel.h     # â­â­â­ å‰å‘kernelæ ¸å¿ƒå®ç°
    â”œâ”€â”€ flash_bwd_kernel.h     # åå‘kernelæ ¸å¿ƒå®ç°
    â”œâ”€â”€ flash_fwd_launch_template.h  # å‰å‘kernelå¯åŠ¨æ¨¡æ¿
    â”œâ”€â”€ flash_bwd_launch_template.h  # åå‘kernelå¯åŠ¨æ¨¡æ¿
    â”œâ”€â”€ kernel_traits.h        # Kernelé…ç½®traits
    â”œâ”€â”€ block_info.h           # å—ä¿¡æ¯å¤„ç†
    â”œâ”€â”€ softmax.h              # â­ Online Softmaxå®ç°
    â”œâ”€â”€ mask.h                 # Causal/Local maskå®ç°
    â”œâ”€â”€ dropout.h              # Dropoutå®ç°
    â”œâ”€â”€ rotary.h               # RoPEæ—‹è½¬ä½ç½®ç¼–ç 
    â”œâ”€â”€ alibi.h                # ALiBiä½ç½®ç¼–ç 
    â”œâ”€â”€ utils.h                # å·¥å…·å‡½æ•°
    â””â”€â”€ flash_fwd_hdim*.cu     # é¢„ç¼–è¯‘çš„kernelå®ä¾‹
```

### 4.2 è°ƒç”¨é“¾æ¦‚è§ˆ
```
Pythonè°ƒç”¨
    â†“
flash_attn_interface.py
    â†“
flash_api.cpp (mha_fwd / mha_bwd)
    â†“
run_mha_fwd / run_mha_bwd
    â†“
flash_fwd_kernel.h::compute_attn_1rowblock
```

---

## ğŸ“š ç¬¬äº”é˜¶æ®µï¼šæ ¸å¿ƒä»£ç ç²¾è¯»ï¼ˆ2-3å‘¨ï¼‰

### 5.1 ç¬¬ä¸€å‘¨ï¼šæ•°æ®ç»“æ„ä¸APIå±‚

#### Day 1-2: flash.h - å‚æ•°ç»“æ„ä½“
**æ–‡ä»¶è·¯å¾„ï¼š** `csrc/flash_attn/src/flash.h`

**å­¦ä¹ è¦ç‚¹ï¼š**
- [ ] `Qkv_params` ç»“æ„ä½“ï¼šQKVçŸ©é˜µæŒ‡é’ˆå’Œstride
- [ ] `Flash_fwd_params` ç»“æ„ä½“ï¼šå‰å‘ä¼ æ’­æ‰€éœ€å…¨éƒ¨å‚æ•°
- [ ] `Flash_bwd_params` ç»“æ„ä½“ï¼šåå‘ä¼ æ’­é¢å¤–å‚æ•°
- [ ] ç†è§£å„ç§strideçš„å«ä¹‰ï¼ˆbatch_stride, row_stride, head_strideï¼‰

**å…³é”®ä»£ç æ®µï¼š**
```cpp
struct Flash_fwd_params : public Qkv_params {
    void * __restrict__ o_ptr;           // è¾“å‡ºæŒ‡é’ˆ
    void * __restrict__ softmax_lse_ptr; // LSEæŒ‡é’ˆ
    int b, seqlen_q, seqlen_k, d;        // ç»´åº¦ä¿¡æ¯
    float scale_softmax;                 // ç¼©æ”¾å› å­
    bool is_causal;                      // æ˜¯å¦å› æœ
    int window_size_left, window_size_right; // æ»‘åŠ¨çª—å£
    // ...
};
```

#### Day 3-4: flash_api.cpp - APIå…¥å£
**æ–‡ä»¶è·¯å¾„ï¼š** `csrc/flash_attn/flash_api.cpp`

**å­¦ä¹ è¦ç‚¹ï¼š**
- [ ] `set_params_fprop`: å¦‚ä½•è®¾ç½®å‰å‘å‚æ•°
- [ ] `mha_fwd`: æ ‡å‡†å‰å‘ä¼ æ’­å…¥å£
- [ ] `mha_varlen_fwd`: å˜é•¿åºåˆ—å‰å‘ä¼ æ’­
- [ ] `run_mha_fwd`: kernelè°ƒåº¦é€»è¾‘
- [ ] Split-KVç­–ç•¥ï¼šä½•æ—¶ä½¿ç”¨ã€å¦‚ä½•é€‰æ‹©num_splits

**å…³é”®ä»£ç æ®µï¼š**
```cpp
void run_mha_fwd(Flash_fwd_params &params, cudaStream_t stream) {
    FP16_SWITCH(!params.is_bf16, [&] {
        HEADDIM_SWITCH(params.d, [&] {
            BOOL_SWITCH(params.is_causal, Is_causal, [&] {
                run_mha_fwd_<elem_type, kHeadDim, Is_causal>(params, stream);
            });
        });
    });
}
```

#### Day 5: kernel_traits.h - Kernelé…ç½®
**æ–‡ä»¶è·¯å¾„ï¼š** `csrc/flash_attn/src/kernel_traits.h`

**å­¦ä¹ è¦ç‚¹ï¼š**
- [ ] `Flash_fwd_kernel_traits`: å®šä¹‰kernelçš„å„ç§ç¼–è¯‘æœŸå¸¸é‡
- [ ] kBlockM, kBlockN: tileå¤§å°é…ç½®
- [ ] SmemLayout: å…±äº«å†…å­˜å¸ƒå±€
- [ ] TiledMma: çŸ©é˜µä¹˜æ³•é…ç½®

---

### 5.2 ç¬¬äºŒå‘¨ï¼šå‰å‘Kernelæ ¸å¿ƒå®ç° â­â­â­æœ€é‡è¦

#### Day 1-3: flash_fwd_kernel.h æ•´ä½“æµç¨‹
**æ–‡ä»¶è·¯å¾„ï¼š** `csrc/flash_attn/src/flash_fwd_kernel.h`

**æ ¸å¿ƒå‡½æ•°ï¼š** `compute_attn_1rowblock`

**å­¦ä¹ è¦ç‚¹ï¼ˆæŒ‰ä»£ç é¡ºåºï¼‰ï¼š**
```
1. åˆå§‹åŒ–é˜¶æ®µ
   - [ ] è®¡ç®—å—ç´¢å¼•(bidb, bidh, m_block)
   - [ ] è·å–å—ä¿¡æ¯(BlockInfo)
   - [ ] è®¡ç®—n_block_min, n_block_maxï¼ˆéœ€è¦å¤„ç†çš„K/Vå—èŒƒå›´ï¼‰

2. å‡†å¤‡é˜¶æ®µ
   - [ ] åˆ›å»ºå…¨å±€å†…å­˜Tensor (mQ, mK, mV, gQ, gK, gV)
   - [ ] åˆ›å»ºå…±äº«å†…å­˜Tensor (sQ, sK, sV)
   - [ ] è®¾ç½®TiledCopyå’ŒTiledMma
   - [ ] åˆå§‹åŒ–ç´¯åŠ å™¨ acc_o

3. QåŠ è½½
   - [ ] ä»å…¨å±€å†…å­˜åŠ è½½Qåˆ°å…±äº«å†…å­˜
   - [ ] cp_asyncå¼‚æ­¥æ‹·è´

4. ä¸»å¾ªç¯ï¼ˆä»åå‘å‰éå†K/Vå—ï¼‰
   for n_block = n_block_max-1 to n_block_min:
       
   a) åŠ è½½Kåˆ°å…±äº«å†…å­˜
   b) è®¡ç®— S = Q @ K^T (gemm)
   c) åº”ç”¨softcapï¼ˆå¯é€‰ï¼‰
   d) åº”ç”¨mask
   e) åŠ è½½Våˆ°å…±äº«å†…å­˜
   f) è®¡ç®—åœ¨çº¿softmaxï¼Œæ›´æ–°acc_o
   g) è®¡ç®— O_partial = softmax(S) @ V (gemm_rs)

5. æ”¶å°¾é˜¶æ®µ
   - [ ] å½’ä¸€åŒ–è¾“å‡º
   - [ ] å†™å›å…¨å±€å†…å­˜
   - [ ] ä¿å­˜LSE
```

#### Day 4-5: æ·±å…¥ç†è§£ä¸»å¾ªç¯
**é‡ç‚¹å…³æ³¨ä»¥ä¸‹æ¨¡å¼ï¼š**

```cpp
// 1. å¼‚æ­¥åŠ è½½K
flash::copy<Is_even_MN, Is_even_K>(gmem_tiled_copy_QKV, tKgK, tKsK, ...);
cute::cp_async_fence();

// 2. ç­‰å¾…KåŠ è½½å®Œæˆï¼Œè®¡ç®—S = Q @ K^T  
flash::cp_async_wait<0>();
__syncthreads();
flash::gemm(acc_s, tSrQ, tSrK, ...);  // S = Q @ K^T

// 3. åº”ç”¨mask
mask.template apply_mask<Is_causal, Is_even_MN>(acc_s, ...);

// 4. åœ¨çº¿softmaxæ›´æ–°
softmax.template softmax_rescale_o<Is_first, Check_inf>(acc_s, acc_o, ...);

// 5. è®¡ç®—O_partial = softmax(S) @ V
Tensor rP = flash::convert_type<Element>(acc_s);
flash::gemm_rs(acc_o, tOrP, tOrVt, ...);
```

#### Day 6-7: è¾…åŠ©æ¨¡å—æ·±å…¥

**softmax.h - åœ¨çº¿Softmaxå®ç°**
```cpp
template <int kNRows>
struct Softmax {
    // æ ¸å¿ƒæ–¹æ³•ï¼šæ›´æ–°æœ€å¤§å€¼å’Œç´¯åŠ å’Œï¼ŒåŒæ—¶rescaleè¾“å‡º
    template<bool Is_first, bool Check_inf>
    __forceinline__ __device__ void softmax_rescale_o(
        Tensor<float> &acc_s,    // å½“å‰å—çš„scores
        Tensor<float> &acc_o,    // ç´¯ç§¯è¾“å‡º
        float scale              // softmaxç¼©æ”¾
    );
};
```

**mask.h - Maskåº”ç”¨**
```cpp
template <bool Is_causal, bool Is_local, bool Has_alibi>
struct Mask {
    // åº”ç”¨å› æœmaskæˆ–å±€éƒ¨mask
    template <bool Causal_mask, bool Is_even_MN>
    __forceinline__ __device__ void apply_mask(
        Tensor &scores,
        int col_idx_offset,
        int row_idx_offset,
        int warp_row_stride
    );
};
```

---

### 5.3 ç¬¬ä¸‰å‘¨ï¼šSplit-KVä¸åå‘ä¼ æ’­

#### Day 1-2: Split-KVæœºåˆ¶
**å‡½æ•°ï¼š** `compute_attn_1rowblock_splitkv`

**å­¦ä¹ è¦ç‚¹ï¼š**
- [ ] ä½•æ—¶éœ€è¦Split-KVï¼šé•¿åºåˆ—æ¨ç†ä¼˜åŒ–
- [ ] å¦‚ä½•åˆ†å‰²K/Våºåˆ—
- [ ] å¦‚ä½•åˆå¹¶å¤šä¸ªsplitçš„ç»“æœ
- [ ] `combine_attn_seqk_parallel`: åˆå¹¶å‡½æ•°

#### Day 3-5: åå‘ä¼ æ’­kernelï¼ˆé€‰å­¦ï¼‰
**æ–‡ä»¶ï¼š** `csrc/flash_attn/src/flash_bwd_kernel.h`

**å­¦ä¹ è¦ç‚¹ï¼š**
- [ ] åå‘ä¼ æ’­çš„æ•°å­¦æ¨å¯¼
- [ ] dQ, dK, dVçš„è®¡ç®—
- [ ] é‡è®¡ç®—ç­–ç•¥ï¼šä¸ä¿å­˜PçŸ©é˜µï¼Œåå‘æ—¶é‡æ–°è®¡ç®—
- [ ] åŸå­æ“ä½œå¤„ç†å¹¶å‘å†™å…¥

---

## ğŸ“š ç¬¬å…­é˜¶æ®µï¼šè¿›é˜¶ä¸“é¢˜ï¼ˆ1-2å‘¨ï¼‰

### 6.1 æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] å…±äº«å†…å­˜bankå†²çªé¿å…
- [ ] å¯„å­˜å™¨åˆ†é…ä¼˜åŒ–
- [ ] æŒ‡ä»¤çº§å¹¶è¡Œ(ILP)
- [ ] Warpçº§åŸè¯­ä½¿ç”¨

### 6.2 æ‰©å±•åŠŸèƒ½å®ç°
- [ ] Paged KV Cacheå®ç°
- [ ] Rotary Position Embedding (RoPE)
- [ ] ALiBiä½ç½®ç¼–ç 
- [ ] Sliding Window Attention
- [ ] Softcapæœºåˆ¶

### 6.3 FlashAttention-3 æ–°ç‰¹æ€§ï¼ˆHopperæ¶æ„ï¼‰
**ç›®å½•ï¼š** `hopper/`
- [ ] TMA (Tensor Memory Accelerator)
- [ ] Warp-specializedè®¾è®¡
- [ ] FP8æ”¯æŒ

---

## ğŸ“ å­¦ä¹ å»ºè®®

### é«˜æ•ˆå­¦ä¹ æ–¹æ³•
1. **å…ˆæ•´ä½“åå±€éƒ¨**ï¼šå…ˆç†è§£ç®—æ³•åŸç†å’Œä»£ç æ¶æ„ï¼Œå†æ·±å…¥ç»†èŠ‚
2. **ç»“åˆè°ƒè¯•**ï¼šä½¿ç”¨`printf`æˆ–CUDAè°ƒè¯•å·¥å…·è§‚å¯Ÿä¸­é—´å€¼
3. **ç”»å›¾ç†è§£**ï¼šæ‰‹ç»˜Tilingè¿‡ç¨‹å’Œæ•°æ®æµåŠ¨
4. **å¯¹æ¯”å­¦ä¹ **ï¼šå¯¹æ¯”æ ‡å‡†Attentionå®ç°ï¼Œç†è§£ä¼˜åŒ–ç‚¹

### å…³é”®æ£€æŸ¥ç‚¹
å®Œæˆä»¥ä¸‹ä»»åŠ¡è¯´æ˜ä½ å·²æŒæ¡æ ¸å¿ƒå†…å®¹ï¼š
- [ ] èƒ½ç”¨è‡ªå·±çš„è¯è§£é‡ŠOnline Softmaxç®—æ³•
- [ ] èƒ½ç”»å‡º`compute_attn_1rowblock`çš„æ‰§è¡Œæµç¨‹å›¾
- [ ] ç†è§£æ¯ä¸ª`__syncthreads()`çš„ä½œç”¨
- [ ] èƒ½è§£é‡Šä¸ºä»€ä¹ˆä½¿ç”¨å¼‚æ­¥å†…å­˜æ‹·è´

### æ¨èå­¦ä¹ é¡ºåºæ€»ç»“
```
Week 1-2: CUDAåŸºç¡€ + AttentionåŸç†
Week 3:   FlashAttentionç®—æ³• + Online Softmax
Week 4:   CUTLASS/cuteåº“
Week 5:   flash.h + flash_api.cpp
Week 6-7: flash_fwd_kernel.h (æ ¸å¿ƒï¼)
Week 8:   è¾…åŠ©æ¨¡å— + Split-KV
Week 9+:  åå‘ä¼ æ’­ + è¿›é˜¶ä¼˜åŒ–
```

---

## ğŸ“– å‚è€ƒèµ„æº

### è®ºæ–‡
- [FlashAttention](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2](https://arxiv.org/abs/2307.08691)
- [FlashAttention-3](https://arxiv.org/abs/2407.08608)

### ä»£ç åº“
- [FlashAttention GitHub](https://github.com/Dao-AILab/flash-attention)
- [CUTLASS](https://github.com/NVIDIA/cutlass)

### åšå®¢æ–‡ç« 
- [FlashAttentionæ ¸å¿ƒé€»è¾‘è¯¦è§£](https://zhuanlan.zhihu.com/p/669926191)
- [Online Softmaxæ¨å¯¼](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)

### è§†é¢‘æ•™ç¨‹
- [Tri Daoçš„FlashAttentionè®²è§£](https://www.youtube.com/watch?v=FThvfkXWqtE)

---

**ç¥å­¦ä¹ é¡ºåˆ©ï¼ğŸš€**

