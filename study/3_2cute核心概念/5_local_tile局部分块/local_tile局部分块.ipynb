{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# local_tile局部分块 - 实践篇\n",
        "\n",
        "本notebook帮助你理解cute的local_tile操作及其在FlashAttention中的应用。\n",
        "\n",
        "**学习目标：**\n",
        "- 理解local_tile的分块语义\n",
        "- 可视化分块过程\n",
        "- 分析FlashAttention中的分块策略\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"环境准备完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. local_tile基本概念可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_local_tile_basic():\n",
        "    \"\"\"可视化local_tile的基本概念\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # 参数\n",
        "    total_rows = 2048\n",
        "    total_cols = 64\n",
        "    tile_rows = 128\n",
        "    tile_cols = 64\n",
        "    \n",
        "    num_row_tiles = total_rows // tile_rows\n",
        "    \n",
        "    # 左图：完整矩阵和分块\n",
        "    ax = axes[0]\n",
        "    ax.set_xlim(0, 10)\n",
        "    ax.set_ylim(0, 10)\n",
        "    \n",
        "    # 画完整矩阵\n",
        "    rect = patches.Rectangle((1, 1), 3, 8, linewidth=2, \n",
        "                               edgecolor='black', facecolor='lightblue')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(2.5, 0.5, f'mQ ({total_rows}×{total_cols})', ha='center', fontsize=10)\n",
        "    \n",
        "    # 画分块线\n",
        "    tile_height = 8 / num_row_tiles\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, num_row_tiles))\n",
        "    \n",
        "    for i in range(num_row_tiles):\n",
        "        y = 1 + i * tile_height\n",
        "        tile = patches.Rectangle((1, y), 3, tile_height, linewidth=1,\n",
        "                                   edgecolor='gray', facecolor=colors[i], alpha=0.7)\n",
        "        ax.add_patch(tile)\n",
        "        if i < 4 or i >= num_row_tiles - 2:\n",
        "            ax.text(0.8, y + tile_height/2, f'{i}', ha='right', va='center', fontsize=8)\n",
        "        elif i == 4:\n",
        "            ax.text(0.8, y + tile_height/2, '...', ha='right', va='center', fontsize=8)\n",
        "    \n",
        "    ax.text(2.5, 9.3, f'{num_row_tiles}个分块，每块{tile_rows}行', ha='center', fontsize=10)\n",
        "    ax.set_title('原始Tensor与分块划分', fontsize=12)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # 右图：local_tile操作\n",
        "    ax = axes[1]\n",
        "    ax.set_xlim(0, 10)\n",
        "    ax.set_ylim(0, 10)\n",
        "    \n",
        "    # 原矩阵（缩小）\n",
        "    rect = patches.Rectangle((0.5, 2), 2, 6, linewidth=2, \n",
        "                               edgecolor='black', facecolor='lightblue')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(1.5, 1.5, 'mQ', ha='center', fontsize=10)\n",
        "    \n",
        "    # 高亮第3个块\n",
        "    highlight_idx = 3\n",
        "    tile_h = 6 / num_row_tiles\n",
        "    highlight = patches.Rectangle((0.5, 2 + highlight_idx * tile_h), 2, tile_h,\n",
        "                                    linewidth=3, edgecolor='red', facecolor='coral')\n",
        "    ax.add_patch(highlight)\n",
        "    \n",
        "    # 箭头\n",
        "    ax.annotate('', xy=(5, 5), xytext=(3, 5),\n",
        "                arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
        "    ax.text(4, 5.5, 'local_tile\\n(tile_shape, coord)', ha='center', fontsize=9)\n",
        "    \n",
        "    # 结果块\n",
        "    result = patches.Rectangle((5.5, 3.5), 3, 3, linewidth=2,\n",
        "                                 edgecolor='red', facecolor='coral')\n",
        "    ax.add_patch(result)\n",
        "    ax.text(7, 3, f'gQ ({tile_rows}×{tile_cols})', ha='center', fontsize=10)\n",
        "    ax.text(7, 5, '第3块的视图', ha='center', va='center', fontsize=11)\n",
        "    \n",
        "    # 代码示例\n",
        "    ax.text(5, 1.5, 'local_tile(mQ,\\n  Shape<_128, _64>{},\\n  make_coord(3, 0))', \n",
        "            ha='center', fontsize=9, family='monospace',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat'))\n",
        "    \n",
        "    ax.set_title('local_tile操作：获取第3个分块', fontsize=12)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    plt.suptitle('local_tile 基本概念', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_local_tile_basic()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Python模拟local_tile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorView:\n",
        "    \"\"\"模拟cute Tensor的视图\"\"\"\n",
        "    def __init__(self, data, offset=0, shape=None, name=\"tensor\"):\n",
        "        self.data = data  # 底层数据（共享）\n",
        "        self.offset = offset  # 起始偏移\n",
        "        self.shape = shape if shape else data.shape\n",
        "        self.name = name\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"支持切片访问\"\"\"\n",
        "        if isinstance(idx, tuple):\n",
        "            actual_idx = tuple(i + self.offset if j == 0 else i \n",
        "                              for j, i in enumerate(idx))\n",
        "            return self.data[actual_idx]\n",
        "        return self.data[idx + self.offset]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"{self.name}: shape={self.shape}, offset={self.offset}\"\n",
        "\n",
        "\n",
        "def local_tile_py(tensor, tile_shape, tile_coord):\n",
        "    \"\"\"\n",
        "    Python模拟cute的local_tile操作\n",
        "    \n",
        "    Args:\n",
        "        tensor: numpy数组或TensorView\n",
        "        tile_shape: (tile_rows, tile_cols)\n",
        "        tile_coord: (row_block, col_block) 或 (None, col_block) 表示通配符\n",
        "    \n",
        "    Returns:\n",
        "        分块视图\n",
        "    \"\"\"\n",
        "    data = tensor.data if isinstance(tensor, TensorView) else tensor\n",
        "    base_offset = tensor.offset if isinstance(tensor, TensorView) else 0\n",
        "    \n",
        "    tile_rows, tile_cols = tile_shape\n",
        "    row_coord, col_coord = tile_coord\n",
        "    \n",
        "    if row_coord is None:\n",
        "        # 通配符：返回所有行块\n",
        "        num_row_blocks = data.shape[0] // tile_rows\n",
        "        result_shape = (tile_rows, tile_cols, num_row_blocks)\n",
        "        return TensorView(data, base_offset, result_shape, \n",
        "                         f\"local_tile(all_rows, col={col_coord})\")\n",
        "    else:\n",
        "        # 固定行块\n",
        "        row_offset = row_coord * tile_rows\n",
        "        col_offset = col_coord * tile_cols\n",
        "        result_shape = (tile_rows, tile_cols)\n",
        "        return TensorView(data, base_offset + row_offset, result_shape,\n",
        "                         f\"local_tile(row={row_coord}, col={col_coord})\")\n",
        "\n",
        "\n",
        "# 测试\n",
        "print(\"=\" * 60)\n",
        "print(\"Python模拟 local_tile\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 创建模拟的Q矩阵\n",
        "seqlen_q = 2048\n",
        "headdim = 64\n",
        "kBlockM = 128\n",
        "\n",
        "mQ = np.arange(seqlen_q * headdim).reshape(seqlen_q, headdim)\n",
        "print(f\"\\n原始Tensor mQ: shape = {mQ.shape}\")\n",
        "print(f\"分块大小: {kBlockM} × {headdim}\")\n",
        "print(f\"分块数量: {seqlen_q // kBlockM}\")\n",
        "\n",
        "# 获取第3个块\n",
        "m_block = 3\n",
        "gQ = local_tile_py(mQ, (kBlockM, headdim), (m_block, 0))\n",
        "print(f\"\\n获取第{m_block}个块: {gQ}\")\n",
        "\n",
        "# 验证：检查第一个元素\n",
        "expected_first = m_block * kBlockM * headdim\n",
        "actual_first = mQ[m_block * kBlockM, 0]\n",
        "print(f\"第{m_block}块第一个元素: 预期={expected_first}, 实际={actual_first}\")\n",
        "\n",
        "# 获取所有块\n",
        "gQ_all = local_tile_py(mQ, (kBlockM, headdim), (None, 0))\n",
        "print(f\"\\n获取所有块: {gQ_all}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 可视化FlashAttention的分块策略\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_flash_attention_tiling():\n",
        "    \"\"\"可视化FlashAttention的分块计算\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
        "    \n",
        "    # 参数\n",
        "    seqlen_q = 2048\n",
        "    seqlen_k = 2048\n",
        "    headdim = 64\n",
        "    kBlockM = 128\n",
        "    kBlockN = 64\n",
        "    \n",
        "    num_m_blocks = seqlen_q // kBlockM\n",
        "    num_n_blocks = seqlen_k // kBlockN\n",
        "    \n",
        "    # 1. Q矩阵分块\n",
        "    ax = axes[0]\n",
        "    q_grid = np.zeros((num_m_blocks, 1))\n",
        "    im = ax.imshow(q_grid, cmap='Blues', aspect='auto')\n",
        "    \n",
        "    # 高亮当前处理的块\n",
        "    current_m = 5\n",
        "    ax.add_patch(patches.Rectangle((-0.5, current_m - 0.5), 1, 1, \n",
        "                                    fill=False, edgecolor='red', linewidth=3))\n",
        "    \n",
        "    for i in range(num_m_blocks):\n",
        "        if i < 4 or i >= num_m_blocks - 2 or i == current_m:\n",
        "            ax.text(0, i, f'Q{i}', ha='center', va='center', fontsize=8)\n",
        "        elif i == 4:\n",
        "            ax.text(0, i, '...', ha='center', va='center', fontsize=8)\n",
        "    \n",
        "    ax.set_title(f'Q矩阵\\n{num_m_blocks}块，每块{kBlockM}行\\n红框=当前Thread Block处理', fontsize=10)\n",
        "    ax.set_ylabel('M维度')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    \n",
        "    # 2. K/V矩阵分块\n",
        "    ax = axes[1]\n",
        "    kv_grid = np.zeros((num_n_blocks, 1))\n",
        "    im = ax.imshow(kv_grid, cmap='Greens', aspect='auto')\n",
        "    \n",
        "    for i in range(num_n_blocks):\n",
        "        if i < 4 or i >= num_n_blocks - 2:\n",
        "            ax.text(0, i, f'K{i}', ha='center', va='center', fontsize=8)\n",
        "        elif i == 4:\n",
        "            ax.text(0, i, '...', ha='center', va='center', fontsize=8)\n",
        "    \n",
        "    ax.set_title(f'K/V矩阵\\n{num_n_blocks}块，每块{kBlockN}行\\n需要遍历所有块', fontsize=10)\n",
        "    ax.set_ylabel('N维度')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    \n",
        "    # 3. 注意力矩阵分块计算\n",
        "    ax = axes[2]\n",
        "    attn_grid = np.zeros((num_m_blocks, num_n_blocks))\n",
        "    \n",
        "    # 标记当前计算的行\n",
        "    attn_grid[current_m, :] = 1\n",
        "    \n",
        "    im = ax.imshow(attn_grid, cmap='OrRd', aspect='auto')\n",
        "    \n",
        "    # 标记\n",
        "    ax.axhline(y=current_m - 0.5, color='red', linewidth=2)\n",
        "    ax.axhline(y=current_m + 0.5, color='red', linewidth=2)\n",
        "    \n",
        "    ax.set_title(f'注意力矩阵 S=QK^T\\n每个Thread Block处理一行\\n遍历所有列', fontsize=10)\n",
        "    ax.set_xlabel('N维度（K块）')\n",
        "    ax.set_ylabel('M维度（Q块）')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    \n",
        "    plt.suptitle('FlashAttention 分块计算策略', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"配置: seqlen_q={seqlen_q}, seqlen_k={seqlen_k}, headdim={headdim}\")\n",
        "    print(f\"分块: kBlockM={kBlockM}, kBlockN={kBlockN}\")\n",
        "    print(f\"Q块数: {num_m_blocks}, K块数: {num_n_blocks}\")\n",
        "    print(f\"\\n每个Thread Block:\")\n",
        "    print(f\"  - 处理固定的1个Q块 (local_tile with fixed coord)\")\n",
        "    print(f\"  - 遍历所有{num_n_blocks}个K/V块 (local_tile with _ wildcard)\")\n",
        "\n",
        "visualize_flash_attention_tiling()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 分块坐标详解\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_tile_coordinates():\n",
        "    \"\"\"可视化不同tile_coord的效果\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # 模拟一个4x4块的矩阵\n",
        "    num_m = 4\n",
        "    num_n = 4\n",
        "    \n",
        "    # 1. make_coord(2, 1) - 固定坐标\n",
        "    ax = axes[0]\n",
        "    grid = np.zeros((num_m, num_n))\n",
        "    grid[2, 1] = 1  # 高亮(2,1)\n",
        "    \n",
        "    im = ax.imshow(grid, cmap='Reds', vmin=0, vmax=1)\n",
        "    for i in range(num_m):\n",
        "        for j in range(num_n):\n",
        "            ax.text(j, i, f'({i},{j})', ha='center', va='center', fontsize=10)\n",
        "    \n",
        "    ax.set_title('make_coord(2, 1)\\n返回单个块 (TileM, TileN)', fontsize=11)\n",
        "    ax.set_xlabel('N维度')\n",
        "    ax.set_ylabel('M维度')\n",
        "    \n",
        "    # 2. make_coord(_, 1) - M维度通配符\n",
        "    ax = axes[1]\n",
        "    grid = np.zeros((num_m, num_n))\n",
        "    grid[:, 1] = 1  # 高亮第1列所有块\n",
        "    \n",
        "    im = ax.imshow(grid, cmap='Greens', vmin=0, vmax=1)\n",
        "    for i in range(num_m):\n",
        "        for j in range(num_n):\n",
        "            ax.text(j, i, f'({i},{j})', ha='center', va='center', fontsize=10)\n",
        "    \n",
        "    ax.set_title('make_coord(_, 1)\\n返回所有M块 (TileM, TileN, num_m)', fontsize=11)\n",
        "    ax.set_xlabel('N维度')\n",
        "    ax.set_ylabel('M维度')\n",
        "    \n",
        "    # 3. make_coord(2, _) - N维度通配符\n",
        "    ax = axes[2]\n",
        "    grid = np.zeros((num_m, num_n))\n",
        "    grid[2, :] = 1  # 高亮第2行所有块\n",
        "    \n",
        "    im = ax.imshow(grid, cmap='Blues', vmin=0, vmax=1)\n",
        "    for i in range(num_m):\n",
        "        for j in range(num_n):\n",
        "            ax.text(j, i, f'({i},{j})', ha='center', va='center', fontsize=10)\n",
        "    \n",
        "    ax.set_title('make_coord(2, _)\\n返回所有N块 (TileM, TileN, num_n)', fontsize=11)\n",
        "    ax.set_xlabel('N维度')\n",
        "    ax.set_ylabel('M维度')\n",
        "    \n",
        "    plt.suptitle('tile_coord 不同用法', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_tile_coordinates()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. FlashAttention中的local_tile代码分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_tile_code = '''\n",
        "// ============================================================\n",
        "// FlashAttention中的local_tile使用\n",
        "// 来自 hopper/mainloop_fwd_sm80.hpp\n",
        "// ============================================================\n",
        "\n",
        "// 定义TileShape\n",
        "using TileShape_MNK = Shape<Int<kBlockM>, Int<kBlockN>, Int<kHeadDim>>;\n",
        "//                          ^M=128       ^N=64        ^K=64\n",
        "\n",
        "// 创建全局Q矩阵Tensor\n",
        "Tensor mQ = make_tensor(\n",
        "    make_gmem_ptr(params.ptr_Q + offset), \n",
        "    params.shape_Q_packed,    // (seqlen_q, headdim, ...)\n",
        "    params.stride_Q_packed\n",
        ")(_, _, bidh, bidb);          // 选择head和batch\n",
        "\n",
        "// 使用local_tile获取Q的第m_block个块\n",
        "// select<0, 2> 选择M和K维度 → Shape<_128, _64>\n",
        "Tensor gQ = local_tile(mQ, select<0, 2>(TileShape_MNK{}), make_coord(m_block, _0{}));\n",
        "// gQ形状: (kBlockM, headdim) = (128, 64)\n",
        "\n",
        "// 创建K矩阵Tensor并获取所有块\n",
        "Tensor mK = make_tensor(make_gmem_ptr(params.ptr_K + offset), ...);\n",
        "\n",
        "// 使用通配符_获取所有K块\n",
        "// select<1, 2> 选择N和K维度 → Shape<_64, _64>\n",
        "Tensor gK = local_tile(mK, select<1, 2>(TileShape_MNK{}), make_coord(_, _0{}));\n",
        "// gK形状: (kBlockN, headdim, num_n_blocks) = (64, 64, seqlen_k/64)\n",
        "\n",
        "// 类似地获取V的所有块\n",
        "Tensor gV = local_tile(mV, select<1, 2>(TileShape_MNK{}), make_coord(_, _0{}));\n",
        "\n",
        "// ============================================================\n",
        "// 主循环中使用分块\n",
        "// ============================================================\n",
        "\n",
        "for (int n_block = n_block_min; n_block < n_block_max; ++n_block) {\n",
        "    // 获取第n_block个K块 (通过索引gK的第3维)\n",
        "    // 这里tKgK是partition后的结果\n",
        "    Tensor tKgK_cur = tKgK(_, _, _, n_block);\n",
        "    \n",
        "    // 拷贝到共享内存\n",
        "    cute::copy(gmem_tiled_copy_QKV, tKgK_cur, tKsK);\n",
        "    cute::cp_async_fence();\n",
        "    \n",
        "    // 类似处理V\n",
        "    Tensor tVgV_cur = tVgV(_, _, _, n_block);\n",
        "    cute::copy(gmem_tiled_copy_QKV, tVgV_cur, tVsV);\n",
        "    \n",
        "    // 等待拷贝完成\n",
        "    cute::cp_async_wait<0>();\n",
        "    __syncthreads();\n",
        "    \n",
        "    // MMA计算: S = Q @ K^T\n",
        "    cute::gemm(tiled_mma, tSrQ, tSrK, acc_s);\n",
        "    \n",
        "    // ... softmax ...\n",
        "    \n",
        "    // MMA计算: O += P @ V\n",
        "    cute::gemm(tiled_mma, tSrP, tSrV, acc_o);\n",
        "}\n",
        "'''\n",
        "\n",
        "print(local_tile_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 边界处理可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_boundary_handling():\n",
        "    \"\"\"可视化不完整块的边界处理\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # 参数\n",
        "    seqlen = 2000  # 不是128的整数倍\n",
        "    kBlockM = 128\n",
        "    \n",
        "    num_full_blocks = seqlen // kBlockM\n",
        "    remainder = seqlen % kBlockM\n",
        "    total_blocks = num_full_blocks + (1 if remainder > 0 else 0)\n",
        "    \n",
        "    # 左图：完整块 vs 不完整块\n",
        "    ax = axes[0]\n",
        "    \n",
        "    # 画块\n",
        "    block_height = 0.8\n",
        "    for i in range(min(total_blocks, 8)):  # 最多显示8个\n",
        "        if i < num_full_blocks and i < 7:\n",
        "            # 完整块\n",
        "            rect = patches.Rectangle((0.5, i), 3, block_height,\n",
        "                                       facecolor='lightblue', edgecolor='blue')\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(2, i + block_height/2, f'块{i}: {kBlockM}行 (完整)', \n",
        "                   ha='center', va='center', fontsize=9)\n",
        "        elif i == total_blocks - 1:\n",
        "            # 不完整块\n",
        "            valid_width = 3 * remainder / kBlockM\n",
        "            rect1 = patches.Rectangle((0.5, i), valid_width, block_height,\n",
        "                                        facecolor='lightgreen', edgecolor='green')\n",
        "            ax.add_patch(rect1)\n",
        "            \n",
        "            rect2 = patches.Rectangle((0.5 + valid_width, i), 3 - valid_width, block_height,\n",
        "                                        facecolor='lightcoral', edgecolor='red', hatch='//')\n",
        "            ax.add_patch(rect2)\n",
        "            \n",
        "            ax.text(2, i + block_height/2, f'块{num_full_blocks}: {remainder}行有效', \n",
        "                   ha='center', va='center', fontsize=9)\n",
        "        elif i == 6:\n",
        "            ax.text(2, i + block_height/2, '...', ha='center', va='center', fontsize=12)\n",
        "    \n",
        "    ax.set_xlim(0, 4)\n",
        "    ax.set_ylim(-0.5, 8.5)\n",
        "    ax.set_title(f'seqlen={seqlen}, kBlockM={kBlockM}\\n最后一块不完整', fontsize=11)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # 右图：predicate处理\n",
        "    ax = axes[1]\n",
        "    \n",
        "    # 最后一块的详细视图\n",
        "    tile_rows = 16  # 简化显示\n",
        "    valid_rows = int(tile_rows * remainder / kBlockM)\n",
        "    \n",
        "    for i in range(tile_rows):\n",
        "        if i < valid_rows:\n",
        "            color = 'lightgreen'\n",
        "            label = 'copy'\n",
        "        else:\n",
        "            color = 'lightcoral'\n",
        "            label = 'clear'\n",
        "        \n",
        "        rect = patches.Rectangle((0.5, i), 3, 0.8, facecolor=color, edgecolor='gray')\n",
        "        ax.add_patch(rect)\n",
        "        if i < 3 or i >= tile_rows - 2 or i == valid_rows - 1 or i == valid_rows:\n",
        "            ax.text(2, i + 0.4, f'行{i}: {label}', ha='center', va='center', fontsize=8)\n",
        "    \n",
        "    ax.axhline(y=valid_rows - 0.1, color='red', linewidth=2, linestyle='--')\n",
        "    ax.text(4, valid_rows - 0.1, '边界', ha='left', va='center', fontsize=10, color='red')\n",
        "    \n",
        "    ax.set_xlim(0, 5)\n",
        "    ax.set_ylim(-0.5, tile_rows + 0.5)\n",
        "    ax.set_title('Predicate边界处理\\n有效行拷贝，越界行清零', fontsize=11)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    plt.suptitle('不完整块的边界处理', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"seqlen = {seqlen}\")\n",
        "    print(f\"kBlockM = {kBlockM}\")\n",
        "    print(f\"完整块数 = {num_full_blocks}\")\n",
        "    print(f\"最后一块有效行数 = {remainder}\")\n",
        "    print(f\"最后一块越界行数 = {kBlockM - remainder}\")\n",
        "\n",
        "visualize_boundary_handling()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. select辅助函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_select():\n",
        "    \"\"\"演示select函数的作用\"\"\"\n",
        "    print(\"select 函数演示\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 模拟TileShape_MNK\n",
        "    TileShape_MNK = (128, 64, 64)  # M, N, K\n",
        "    \n",
        "    print(f\"\\nTileShape_MNK = {TileShape_MNK}\")\n",
        "    print(f\"  索引0 (M) = {TileShape_MNK[0]}\")\n",
        "    print(f\"  索引1 (N) = {TileShape_MNK[1]}\")\n",
        "    print(f\"  索引2 (K) = {TileShape_MNK[2]}\")\n",
        "    \n",
        "    # select<0, 2> 选择M和K\n",
        "    select_0_2 = (TileShape_MNK[0], TileShape_MNK[2])\n",
        "    print(f\"\\nselect<0, 2>(TileShape_MNK) = {select_0_2}\")\n",
        "    print(f\"  用于Q矩阵: (seqlen_q, headdim) 分块为 ({select_0_2[0]}, {select_0_2[1]})\")\n",
        "    \n",
        "    # select<1, 2> 选择N和K\n",
        "    select_1_2 = (TileShape_MNK[1], TileShape_MNK[2])\n",
        "    print(f\"\\nselect<1, 2>(TileShape_MNK) = {select_1_2}\")\n",
        "    print(f\"  用于K/V矩阵: (seqlen_k, headdim) 分块为 ({select_1_2[0]}, {select_1_2[1]})\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FlashAttention中的使用:\")\n",
        "    print(\"\"\"\n",
        "// Q矩阵分块\n",
        "Tensor gQ = local_tile(mQ, select<0, 2>(TileShape_MNK{}), ...);\n",
        "// 分块大小: (kBlockM=128, headdim=64)\n",
        "\n",
        "// K矩阵分块  \n",
        "Tensor gK = local_tile(mK, select<1, 2>(TileShape_MNK{}), ...);\n",
        "// 分块大小: (kBlockN=64, headdim=64)\n",
        "\n",
        "// V矩阵分块 (与K相同)\n",
        "Tensor gV = local_tile(mV, select<1, 2>(TileShape_MNK{}), ...);\n",
        "// 分块大小: (kBlockN=64, headdim=64)\n",
        "\"\"\")\n",
        "\n",
        "demonstrate_select()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 完整数据流可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_complete_dataflow():\n",
        "    \"\"\"可视化local_tile在完整数据流中的位置\"\"\"\n",
        "    print(\"FlashAttention 完整数据流\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    flow = \"\"\"\n",
        "1. 创建全局Tensor\n",
        "   ┌─────────────────────────────────────────────────────────────────┐\n",
        "   │  Tensor mQ = make_tensor(make_gmem_ptr(ptr_Q), shape, stride)  │\n",
        "   │  Tensor mK = make_tensor(make_gmem_ptr(ptr_K), shape, stride)  │\n",
        "   │  Tensor mV = make_tensor(make_gmem_ptr(ptr_V), shape, stride)  │\n",
        "   └─────────────────────────────────────────────────────────────────┘\n",
        "                                   │\n",
        "                                   ▼\n",
        "2. 使用local_tile获取分块视图  ◄── 本节重点\n",
        "   ┌─────────────────────────────────────────────────────────────────┐\n",
        "   │  Tensor gQ = local_tile(mQ, tile_shape, make_coord(m_block, 0)) │\n",
        "   │  Tensor gK = local_tile(mK, tile_shape, make_coord(_, 0))       │\n",
        "   │  Tensor gV = local_tile(mV, tile_shape, make_coord(_, 0))       │\n",
        "   └─────────────────────────────────────────────────────────────────┘\n",
        "                                   │\n",
        "                                   ▼\n",
        "3. 使用TiledCopy分区\n",
        "   ┌─────────────────────────────────────────────────────────────────┐\n",
        "   │  auto thr_copy = gmem_tiled_copy.get_thread_slice(threadIdx.x) │\n",
        "   │  Tensor tQgQ = thr_copy.partition_S(gQ)                         │\n",
        "   │  Tensor tKgK = thr_copy.partition_S(gK)                         │\n",
        "   └─────────────────────────────────────────────────────────────────┘\n",
        "                                   │\n",
        "                                   ▼\n",
        "4. 主循环：遍历K/V块\n",
        "   ┌─────────────────────────────────────────────────────────────────┐\n",
        "   │  for n_block in range(num_blocks):                              │\n",
        "   │      // 拷贝K[n_block]到共享内存                                │\n",
        "   │      copy(tKgK(_, _, _, n_block), tKsK)                        │\n",
        "   │      cp_async_fence(); cp_async_wait<0>()                       │\n",
        "   │                                                                  │\n",
        "   │      // 拷贝到寄存器                                            │\n",
        "   │      copy(smem_tiled_copy, tCsK, tCrK)                          │\n",
        "   │                                                                  │\n",
        "   │      // MMA计算                                                 │\n",
        "   │      gemm(tiled_mma, tCrQ, tCrK, acc_s)  // S = Q @ K^T        │\n",
        "   │      // ... softmax ...                                         │\n",
        "   │      gemm(tiled_mma, tCrP, tCrV, acc_o)  // O += P @ V         │\n",
        "   └─────────────────────────────────────────────────────────────────┘\n",
        "                                   │\n",
        "                                   ▼\n",
        "5. 写回输出\n",
        "   ┌─────────────────────────────────────────────────────────────────┐\n",
        "   │  Tensor gO = local_tile(mO, tile_shape, make_coord(m_block, 0)) │\n",
        "   │  copy(acc_o, gO)                                                │\n",
        "   └─────────────────────────────────────────────────────────────────┘\n",
        "\"\"\"\n",
        "    print(flow)\n",
        "\n",
        "visualize_complete_dataflow()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "\n",
        "1. **local_tile基本概念**: 从大Tensor获取分块视图，零拷贝\n",
        "2. **tile_shape**: 定义分块大小\n",
        "3. **tile_coord**: 指定分块位置，支持通配符`_`\n",
        "4. **select辅助函数**: 从多维Shape中选择特定维度\n",
        "5. **边界处理**: 最后一块可能不完整，需要predicate\n",
        "6. **在FlashAttention中的应用**: Q固定块，K/V遍历所有块\n",
        "\n",
        "## 关键代码模式\n",
        "\n",
        "```cpp\n",
        "// 1. 定义TileShape\n",
        "using TileShape_MNK = Shape<Int<128>, Int<64>, Int<64>>;\n",
        "\n",
        "// 2. 创建全局Tensor\n",
        "Tensor mQ = make_tensor(make_gmem_ptr(ptr), shape, stride);\n",
        "\n",
        "// 3. 获取固定块（Q）\n",
        "Tensor gQ = local_tile(mQ, select<0, 2>(TileShape_MNK{}), make_coord(m_block, _0{}));\n",
        "\n",
        "// 4. 获取所有块（K/V）用于遍历\n",
        "Tensor gK = local_tile(mK, select<1, 2>(TileShape_MNK{}), make_coord(_, _0{}));\n",
        "\n",
        "// 5. 在循环中访问每个块\n",
        "for (int n = 0; n < size<2>(gK); ++n) {\n",
        "    Tensor gK_n = gK(_, _, n);  // 第n个K块\n",
        "}\n",
        "```\n",
        "\n",
        "## 3_2cute核心概念 学习完成！\n",
        "\n",
        "至此，你已经学习了cute的5个核心概念：\n",
        "1. **Tensor**: 多维数组抽象\n",
        "2. **Layout**: 描述数据在内存中的排布\n",
        "3. **TiledMMA**: 封装Tensor Core矩阵乘法\n",
        "4. **TiledCopy**: 高效内存拷贝抽象\n",
        "5. **local_tile**: 获取Tensor的局部分块视图\n",
        "\n",
        "这些概念是理解FlashAttention实现的基础！\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
