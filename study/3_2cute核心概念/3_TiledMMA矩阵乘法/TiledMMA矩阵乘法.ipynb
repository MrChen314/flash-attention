{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TiledMMA矩阵乘法 - 实践篇\n",
        "\n",
        "本notebook通过概念讲解和代码分析帮助你理解cute的TiledMMA抽象。\n",
        "\n",
        "**学习目标：**\n",
        "- 理解Tensor Core和MMA操作的基本概念\n",
        "- 分析FlashAttention中的TiledMMA使用\n",
        "- 理解partition操作的含义\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"环境准备完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Tensor Core MMA操作概念\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_mma_atom():\n",
        "    \"\"\"可视化单个MMA Atom操作\"\"\"\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    \n",
        "    # MMA 16x8x16: A(16x16) @ B(16x8) = C(16x8)\n",
        "    \n",
        "    # A矩阵\n",
        "    ax = axes[0]\n",
        "    A = np.random.rand(16, 16)\n",
        "    ax.imshow(A, cmap='Blues')\n",
        "    ax.set_title('A矩阵\\n16×16 (FP16)')\n",
        "    ax.set_xlabel('K=16')\n",
        "    ax.set_ylabel('M=16')\n",
        "    \n",
        "    # B矩阵\n",
        "    ax = axes[1]\n",
        "    B = np.random.rand(16, 8)\n",
        "    ax.imshow(B, cmap='Greens')\n",
        "    ax.set_title('B矩阵\\n16×8 (FP16)')\n",
        "    ax.set_xlabel('N=8')\n",
        "    ax.set_ylabel('K=16')\n",
        "    \n",
        "    # 乘法符号\n",
        "    ax = axes[2]\n",
        "    ax.text(0.5, 0.5, '×', fontsize=60, ha='center', va='center')\n",
        "    ax.axis('off')\n",
        "    ax.set_title('矩阵乘法')\n",
        "    \n",
        "    # C矩阵\n",
        "    ax = axes[3]\n",
        "    C = np.random.rand(16, 8)\n",
        "    ax.imshow(C, cmap='Oranges')\n",
        "    ax.set_title('C矩阵\\n16×8 (FP32)')\n",
        "    ax.set_xlabel('N=8')\n",
        "    ax.set_ylabel('M=16')\n",
        "    \n",
        "    plt.suptitle('SM80_16x8x16 MMA Atom: D = A × B + C', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_mma_atom()\n",
        "\n",
        "print(\"MMA Atom规格: SM80_16x8x16_F32F16F16F32_TN\")\n",
        "print(\"- M=16: 输出矩阵的行数\")\n",
        "print(\"- N=8:  输出矩阵的列数\")\n",
        "print(\"- K=16: 累加的维度\")\n",
        "print(\"- 输入精度: FP16\")\n",
        "print(\"- 输出精度: FP32\")\n",
        "print(\"- TN: A转置, B不转置\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. TiledMMA: 多个Atom组合\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_tiled_mma():\n",
        "    \"\"\"可视化TiledMMA如何组合多个Atom\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # 左图: 单个Atom\n",
        "    ax = axes[0]\n",
        "    atom = plt.Rectangle((0, 0), 8, 16, fill=True, facecolor='lightblue', edgecolor='blue', linewidth=2)\n",
        "    ax.add_patch(atom)\n",
        "    ax.set_xlim(-1, 10)\n",
        "    ax.set_ylim(-1, 18)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title('单个MMA Atom\\n16×8 输出')\n",
        "    ax.set_xlabel('N')\n",
        "    ax.set_ylabel('M')\n",
        "    ax.text(4, 8, 'Atom\\n16×8', ha='center', va='center', fontsize=12)\n",
        "    \n",
        "    # 右图: TiledMMA (2x2 Atoms)\n",
        "    ax = axes[1]\n",
        "    colors = ['lightcoral', 'lightgreen', 'lightyellow', 'lightblue']\n",
        "    labels = ['Atom(0,0)', 'Atom(0,1)', 'Atom(1,0)', 'Atom(1,1)']\n",
        "    \n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            rect = plt.Rectangle((j*8, (1-i)*16), 8, 16, \n",
        "                                 fill=True, facecolor=colors[i*2+j], \n",
        "                                 edgecolor='black', linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(j*8+4, (1-i)*16+8, labels[i*2+j], ha='center', va='center', fontsize=10)\n",
        "    \n",
        "    ax.set_xlim(-1, 18)\n",
        "    ax.set_ylim(-1, 34)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title('TiledMMA (AtomLayout=2×2)\\n32×16 输出')\n",
        "    ax.set_xlabel('N = 8×2 = 16')\n",
        "    ax.set_ylabel('M = 16×2 = 32')\n",
        "    \n",
        "    plt.suptitle('TiledMMA组合多个MMA Atom覆盖更大矩阵', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_tiled_mma()\n",
        "\n",
        "print(\"TiledMMA配置示例:\")\n",
        "print(\"- MMA Atom: SM80_16x8x16\")\n",
        "print(\"- AtomLayout: Shape<_2, _2, _1>  (2×2个Atom)\")\n",
        "print(\"- 输出矩阵大小: M=16×2=32, N=8×2=16\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 线程到数据的映射\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_thread_mapping():\n",
        "    \"\"\"可视化线程如何映射到MMA操作\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # 32个线程的简化映射 (16x8 输出矩阵)\n",
        "    # 每个线程持有多个元素\n",
        "    \n",
        "    M, N = 16, 8\n",
        "    thread_map = np.zeros((M, N), dtype=int)\n",
        "    \n",
        "    # 简化的线程映射模式\n",
        "    for i in range(M):\n",
        "        for j in range(N):\n",
        "            # 这是一个简化模型，实际映射更复杂\n",
        "            thread_map[i, j] = (i % 4) * 8 + j\n",
        "    \n",
        "    im = ax.imshow(thread_map, cmap='tab20', vmin=0, vmax=31)\n",
        "    ax.set_title('MMA Atom中的线程映射 (简化示意)\\\\n颜色=线程ID', fontsize=12)\n",
        "    ax.set_xlabel('N (列)')\n",
        "    ax.set_ylabel('M (行)')\n",
        "    \n",
        "    for i in range(M):\n",
        "        for j in range(N):\n",
        "            ax.text(j, i, f'T{thread_map[i,j]}', ha='center', va='center', \n",
        "                   fontsize=7, color='white')\n",
        "    \n",
        "    plt.colorbar(im, ax=ax, label='线程ID')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"关键概念:\")\n",
        "    print(\"- 一个Warp (32线程) 协作完成一个MMA Atom\")\n",
        "    print(\"- 每个线程持有输出矩阵的多个元素 (Fragment)\")\n",
        "    print(\"- partition_C 返回当前线程负责的元素\")\n",
        "\n",
        "visualize_thread_mapping()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. FlashAttention中的TiledMMA代码分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flashattention_tiledmma_code = '''\n",
        "// ============================================================\n",
        "// FlashAttention中的TiledMMA使用模式\n",
        "// 来自 hopper/utils.h 和 mainloop_*.hpp\n",
        "// ============================================================\n",
        "\n",
        "// 1. TiledMMA类型定义\n",
        "using TiledMmaSdP = TiledMMA<\n",
        "    MMA_Atom<SM80_16x8x16_F32F16F16F32_TN>,\n",
        "    Layout<Shape<_2, _2, _1>>,   // 2×2 Atoms\n",
        "    Tile<_32, _32, _16>          // 输出Tile大小\n",
        ">;\n",
        "\n",
        "// 2. 在kernel中使用\n",
        "__device__ void compute_attention() {\n",
        "    TiledMmaSdP tiled_mma_SdP;\n",
        "    \n",
        "    // 获取当前线程的视图\n",
        "    auto thr_mma = tiled_mma_SdP.get_thread_slice(threadIdx.x);\n",
        "    \n",
        "    // 创建累加器Fragment\n",
        "    Tensor acc_s = partition_fragment_C(tiled_mma_SdP, \n",
        "                                        Shape<Int<kBlockM>, Int<kBlockN>>{});\n",
        "    clear(acc_s);\n",
        "    \n",
        "    // 分区输入矩阵\n",
        "    Tensor tSrQ = thr_mma.partition_A(sQ);  // Q的分区\n",
        "    Tensor tSrK = thr_mma.partition_B(sK);  // K的分区\n",
        "    \n",
        "    // 执行矩阵乘法: S = Q @ K^T\n",
        "    cute::gemm(tiled_mma_SdP, tSrQ, tSrK, acc_s);\n",
        "    \n",
        "    // acc_s 现在包含 S 的结果\n",
        "}\n",
        "\n",
        "// 3. gemm函数的封装 (utils.h)\n",
        "template <bool zero_init=true, typename TiledMma, typename Tensor0, \n",
        "          typename Tensor1, typename Tensor2>\n",
        "CUTLASS_DEVICE void gemm(TiledMma& tiled_mma, \n",
        "                         Tensor0 const& tCrA, \n",
        "                         Tensor1 const& tCrB, \n",
        "                         Tensor2& tCrC) {\n",
        "    // K维度的循环\n",
        "    constexpr int MMA_K = decltype(size<2>(tCrA))::value;\n",
        "    \n",
        "    #pragma unroll\n",
        "    for (int k = 0; k < MMA_K; ++k) {\n",
        "        cute::gemm(tiled_mma, tCrA(_, _, k), tCrB(_, _, k), tCrC);\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "print(flashattention_tiledmma_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 模拟partition操作\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_partition():\n",
        "    \"\"\"模拟partition操作的概念\"\"\"\n",
        "    \n",
        "    print(\"partition操作概念:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 假设的矩阵大小\n",
        "    M, N, K = 128, 64, 64\n",
        "    num_threads = 128\n",
        "    threads_per_warp = 32\n",
        "    num_warps = num_threads // threads_per_warp\n",
        "    \n",
        "    print(f\"矩阵大小: A={M}×{K}, B={K}×{N}, C={M}×{N}\")\n",
        "    print(f\"线程数: {num_threads} ({num_warps} warps)\")\n",
        "    print()\n",
        "    \n",
        "    # TiledMMA配置\n",
        "    mma_m, mma_n, mma_k = 16, 8, 16\n",
        "    atom_layout_m, atom_layout_n = 2, 2\n",
        "    \n",
        "    tile_m = mma_m * atom_layout_m  # 32\n",
        "    tile_n = mma_n * atom_layout_n  # 16\n",
        "    \n",
        "    print(f\"MMA Atom大小: {mma_m}×{mma_n}×{mma_k}\")\n",
        "    print(f\"Atom Layout: {atom_layout_m}×{atom_layout_n}\")\n",
        "    print(f\"每个TiledMMA输出: {tile_m}×{tile_n}\")\n",
        "    print()\n",
        "    \n",
        "    # 计算需要多少个TiledMMA\n",
        "    num_tiles_m = M // tile_m\n",
        "    num_tiles_n = N // tile_n\n",
        "    \n",
        "    print(f\"需要的TiledMMA数量: {num_tiles_m}×{num_tiles_n} = {num_tiles_m * num_tiles_n}\")\n",
        "    print()\n",
        "    \n",
        "    # partition后每个线程的数据量\n",
        "    elements_per_atom = (mma_m * mma_n) // threads_per_warp\n",
        "    elements_per_thread = elements_per_atom * atom_layout_m * atom_layout_n\n",
        "    \n",
        "    print(f\"partition_C 结果:\")\n",
        "    print(f\"  每个线程在一个Atom中: {elements_per_atom} 元素\")\n",
        "    print(f\"  每个线程在TiledMMA中: {elements_per_thread} 元素\")\n",
        "    print(f\"  Fragment形状类似: ((4), (2), (2)) for 2×2 Atoms\")\n",
        "\n",
        "simulate_partition()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "\n",
        "1. **Tensor Core MMA**: 硬件级别的矩阵乘累加操作，如16×8×16\n",
        "2. **MMA Atom**: 单个Tensor Core指令的抽象\n",
        "3. **TiledMMA**: 多个Atom的组合，覆盖更大的矩阵\n",
        "4. **partition**: 将矩阵分配给线程，返回Fragment\n",
        "5. **cute::gemm**: 执行矩阵乘法的高层接口\n",
        "\n",
        "## 关键代码模式\n",
        "\n",
        "```cpp\n",
        "// 1. 定义TiledMMA\n",
        "TiledMma tiled_mma = make_tiled_mma(...);\n",
        "\n",
        "// 2. 获取线程视图\n",
        "auto thr_mma = tiled_mma.get_thread_slice(threadIdx.x);\n",
        "\n",
        "// 3. 分区\n",
        "Tensor tCrA = thr_mma.partition_A(sA);\n",
        "Tensor tCrB = thr_mma.partition_B(sB);\n",
        "\n",
        "// 4. 执行GEMM\n",
        "cute::gemm(tiled_mma, tCrA, tCrB, acc);\n",
        "```\n",
        "\n",
        "## 下一步\n",
        "\n",
        "在下一节\"TiledCopy内存拷贝\"中，我们将学习如何高效地在内存层次间传输数据。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
