{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensor张量抽象 - 实践篇\n",
        "\n",
        "本notebook通过Python模拟和代码分析帮助你理解cute Tensor的核心概念。\n",
        "\n",
        "**学习目标：**\n",
        "- 理解Tensor = 指针 + Layout 的结构\n",
        "- 模拟cute的索引计算过程\n",
        "- 分析FlashAttention中的Tensor使用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List, Union\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"环境准备完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 模拟cute Layout\n",
        "\n",
        "Layout是cute的基础概念，它定义了逻辑坐标到物理偏移的映射。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Layout:\n",
        "    \"\"\"\n",
        "    模拟cute的Layout类\n",
        "    Layout = (Shape, Stride)\n",
        "    \"\"\"\n",
        "    def __init__(self, shape: Tuple[int, ...], stride: Tuple[int, ...] = None):\n",
        "        self.shape = shape\n",
        "        # 如果没有指定stride，默认使用行优先（row-major）\n",
        "        if stride is None:\n",
        "            self.stride = self._compute_row_major_stride(shape)\n",
        "        else:\n",
        "            self.stride = stride\n",
        "    \n",
        "    def _compute_row_major_stride(self, shape: Tuple[int, ...]) -> Tuple[int, ...]:\n",
        "        \"\"\"计算行优先的stride\"\"\"\n",
        "        stride = [1]\n",
        "        for s in reversed(shape[1:]):\n",
        "            stride.append(stride[-1] * s)\n",
        "        return tuple(reversed(stride))\n",
        "    \n",
        "    def __call__(self, *coords) -> int:\n",
        "        \"\"\"给定逻辑坐标，返回线性偏移\"\"\"\n",
        "        assert len(coords) == len(self.shape), f\"坐标维度不匹配: {len(coords)} vs {len(self.shape)}\"\n",
        "        offset = 0\n",
        "        for c, s in zip(coords, self.stride):\n",
        "            offset += c * s\n",
        "        return offset\n",
        "    \n",
        "    def size(self) -> int:\n",
        "        \"\"\"返回总元素数\"\"\"\n",
        "        result = 1\n",
        "        for s in self.shape:\n",
        "            result *= s\n",
        "        return result\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"Layout(shape={self.shape}, stride={self.stride})\"\n",
        "\n",
        "# 测试Layout\n",
        "layout = Layout((4, 3))\n",
        "print(f\"行优先 4x3 矩阵: {layout}\")\n",
        "print(f\"元素(2,1)的偏移: {layout(2, 1)}\")\n",
        "print(f\"总大小: {layout.size()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 模拟cute Tensor\n",
        "\n",
        "Tensor = Engine (数据存储) + Layout (索引映射)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tensor:\n",
        "    \"\"\"\n",
        "    模拟cute的Tensor类\n",
        "    Tensor = Engine (数据存储) + Layout (索引映射)\n",
        "    \"\"\"\n",
        "    def __init__(self, data: np.ndarray, layout: Layout):\n",
        "        self.data = data.flatten()  # 模拟线性内存\n",
        "        self.layout = layout\n",
        "    \n",
        "    def __getitem__(self, coords) -> float:\n",
        "        \"\"\"多维索引访问\"\"\"\n",
        "        if isinstance(coords, int):\n",
        "            return self.data[coords]\n",
        "        offset = self.layout(*coords)\n",
        "        return self.data[offset]\n",
        "    \n",
        "    def __setitem__(self, coords, value):\n",
        "        \"\"\"多维索引写入\"\"\"\n",
        "        if isinstance(coords, int):\n",
        "            self.data[coords] = value\n",
        "        else:\n",
        "            offset = self.layout(*coords)\n",
        "            self.data[offset] = value\n",
        "    \n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self.layout.shape\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"Tensor(shape={self.shape}, layout={self.layout})\"\n",
        "\n",
        "def make_tensor(data: np.ndarray, shape: Tuple[int, ...], stride: Tuple[int, ...] = None) -> Tensor:\n",
        "    \"\"\"模拟cute的make_tensor函数\"\"\"\n",
        "    layout = Layout(shape, stride)\n",
        "    return Tensor(data, layout)\n",
        "\n",
        "# 创建测试数据\n",
        "data = np.arange(12, dtype=np.float32)\n",
        "print(f\"原始数据: {data}\")\n",
        "\n",
        "# 创建Tensor\n",
        "tensor = make_tensor(data, (4, 3))\n",
        "print(f\"\\nTensor: {tensor}\")\n",
        "print(f\"tensor[2, 1] = {tensor[2, 1]}\")\n",
        "print(f\"tensor[0, 0] = {tensor[0, 0]}\")\n",
        "print(f\"tensor[3, 2] = {tensor[3, 2]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 对比行优先和列优先的区别\n",
        "data = np.arange(12, dtype=np.float32)\n",
        "\n",
        "# 行优先 (Row-major): stride = (3, 1)\n",
        "row_major = make_tensor(data, (4, 3), stride=(3, 1))\n",
        "\n",
        "# 列优先 (Column-major): stride = (1, 4)\n",
        "col_major = make_tensor(data, (4, 3), stride=(1, 4))\n",
        "\n",
        "print(\"线性内存: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\")\n",
        "print()\n",
        "\n",
        "print(\"行优先 (stride=(3,1)):\")\n",
        "print(\"  逻辑视图:\")\n",
        "for i in range(4):\n",
        "    row = [f\"{row_major[i, j]:.0f}\" for j in range(3)]\n",
        "    print(f\"    [{', '.join(row)}]\")\n",
        "\n",
        "print(\"\\n列优先 (stride=(1,4)):\")\n",
        "print(\"  逻辑视图:\")\n",
        "for i in range(4):\n",
        "    row = [f\"{col_major[i, j]:.0f}\" for j in range(3)]\n",
        "    print(f\"    [{', '.join(row)}]\")\n",
        "\n",
        "print(\"\\n注意: 相同的逻辑坐标在不同Layout下对应不同的物理偏移\")\n",
        "print(f\"  row_major[1,0] = {row_major[1, 0]} (offset={row_major.layout(1, 0)})\")\n",
        "print(f\"  col_major[1,0] = {col_major[1, 0]} (offset={col_major.layout(1, 0)})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 可视化Layout映射\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_layout(layout: Layout, title: str):\n",
        "    \"\"\"可视化Layout的逻辑→物理映射\"\"\"\n",
        "    M, N = layout.shape\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # 左图: 逻辑视图\n",
        "    ax1 = axes[0]\n",
        "    logical = np.zeros((M, N))\n",
        "    for i in range(M):\n",
        "        for j in range(N):\n",
        "            logical[i, j] = layout(i, j)\n",
        "    \n",
        "    im1 = ax1.imshow(logical, cmap='viridis')\n",
        "    ax1.set_title(f'逻辑视图 ({M}x{N})')\n",
        "    ax1.set_xlabel('列索引 j')\n",
        "    ax1.set_ylabel('行索引 i')\n",
        "    \n",
        "    # 添加数值标注\n",
        "    for i in range(M):\n",
        "        for j in range(N):\n",
        "            ax1.text(j, i, f'{int(logical[i,j])}', ha='center', va='center', color='white', fontweight='bold')\n",
        "    \n",
        "    plt.colorbar(im1, ax=ax1, label='物理偏移')\n",
        "    \n",
        "    # 右图: 线性内存视图\n",
        "    ax2 = axes[1]\n",
        "    total = layout.size()\n",
        "    linear = np.arange(total).reshape(1, -1)\n",
        "    \n",
        "    im2 = ax2.imshow(linear, cmap='viridis', aspect='auto')\n",
        "    ax2.set_title('线性内存')\n",
        "    ax2.set_xlabel('物理偏移')\n",
        "    ax2.set_yticks([])\n",
        "    \n",
        "    for idx in range(total):\n",
        "        ax2.text(idx, 0, f'{idx}', ha='center', va='center', color='white', fontsize=8)\n",
        "    \n",
        "    plt.suptitle(f'{title}\\nshape={layout.shape}, stride={layout.stride}', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 可视化行优先\n",
        "visualize_layout(Layout((4, 3), (3, 1)), \"行优先 (Row-major)\")\n",
        "\n",
        "# 可视化列优先\n",
        "visualize_layout(Layout((4, 3), (1, 4)), \"列优先 (Column-major)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 模拟local_tile操作\n",
        "\n",
        "local_tile从大Tensor中获取一个小的分块视图，不拷贝数据。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorView:\n",
        "    \"\"\"\n",
        "    Tensor的视图（不拷贝数据）\n",
        "    用于模拟local_tile返回的分块视图\n",
        "    \"\"\"\n",
        "    def __init__(self, tensor: Tensor, offset: int, layout: Layout):\n",
        "        self.tensor = tensor\n",
        "        self.offset = offset  # 在原Tensor中的起始偏移\n",
        "        self.layout = layout\n",
        "    \n",
        "    def __getitem__(self, coords) -> float:\n",
        "        local_offset = self.layout(*coords) if not isinstance(coords, int) else coords\n",
        "        return self.tensor.data[self.offset + local_offset]\n",
        "    \n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self.layout.shape\n",
        "\n",
        "def local_tile(tensor: Tensor, tile_shape: Tuple[int, ...], tile_coord: Tuple[int, ...]) -> TensorView:\n",
        "    \"\"\"\n",
        "    模拟cute的local_tile函数\n",
        "    从大Tensor中获取一个小的分块视图\n",
        "    \"\"\"\n",
        "    # 计算tile在原Tensor中的起始位置\n",
        "    start_coords = tuple(tc * ts for tc, ts in zip(tile_coord, tile_shape))\n",
        "    offset = tensor.layout(*start_coords)\n",
        "    \n",
        "    # 创建tile的Layout（继承原始stride）\n",
        "    tile_layout = Layout(tile_shape, tensor.layout.stride)\n",
        "    \n",
        "    return TensorView(tensor, offset, tile_layout)\n",
        "\n",
        "# 创建一个 8x6 的Tensor\n",
        "data = np.arange(48, dtype=np.float32)\n",
        "tensor = make_tensor(data, (8, 6))\n",
        "\n",
        "print(f\"原始Tensor: shape={tensor.shape}\")\n",
        "print(f\"内容预览:\")\n",
        "for i in range(8):\n",
        "    row = [f\"{tensor[i, j]:2.0f}\" for j in range(6)]\n",
        "    print(f\"  [{', '.join(row)}]\")\n",
        "\n",
        "# 获取 4x3 的tile\n",
        "tile_00 = local_tile(tensor, (4, 3), (0, 0))\n",
        "tile_11 = local_tile(tensor, (4, 3), (1, 1))\n",
        "\n",
        "print(f\"\\ntile(0,0) at offset {tile_00.offset}:\")\n",
        "for i in range(4):\n",
        "    row = [f\"{tile_00[i, j]:2.0f}\" for j in range(3)]\n",
        "    print(f\"  [{', '.join(row)}]\")\n",
        "\n",
        "print(f\"\\ntile(1,1) at offset {tile_11.offset}:\")\n",
        "for i in range(4):\n",
        "    row = [f\"{tile_11[i, j]:2.0f}\" for j in range(3)]\n",
        "    print(f\"  [{', '.join(row)}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. FlashAttention中的Tensor使用分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FlashAttention中Tensor使用的代码分析\n",
        "\n",
        "flashattention_code = '''\n",
        "// ============================================================\n",
        "// FlashAttention中的Tensor创建和使用模式\n",
        "// 来自 hopper/mainloop_fwd_sm80.hpp\n",
        "// ============================================================\n",
        "\n",
        "// 1. 创建全局内存Tensor\n",
        "Tensor mQ = make_tensor(\n",
        "    make_gmem_ptr(reinterpret_cast<Element*>(params.q_ptr)),\n",
        "    make_shape(params.seqlen_q, params.d),     // [N, d]\n",
        "    make_stride(params.q_row_stride, Int<1>{}) // 行优先\n",
        ");\n",
        "\n",
        "// 2. 使用local_tile获取当前block处理的分块\n",
        "Tensor gQ = local_tile(\n",
        "    mQ,                               // 原始Tensor\n",
        "    TileShape_MK{},                   // tile大小，如(128, 64)\n",
        "    make_coord(m_block, 0)            // tile坐标\n",
        ");\n",
        "\n",
        "// 3. 创建共享内存Tensor\n",
        "extern __shared__ char smem_[];\n",
        "Tensor sQ = make_tensor(\n",
        "    make_smem_ptr(reinterpret_cast<Element*>(smem_)),\n",
        "    SmemLayoutQ{}  // 预定义的共享内存Layout\n",
        ");\n",
        "\n",
        "// 4. 使用TiledCopy进行分区和拷贝\n",
        "auto gmem_thr_copy = gmem_tiled_copy.get_thread_slice(threadIdx.x);\n",
        "Tensor tQgQ = gmem_thr_copy.partition_S(gQ);  // 源分区\n",
        "Tensor tQsQ = gmem_thr_copy.partition_D(sQ);  // 目标分区\n",
        "cute::copy(gmem_tiled_copy, tQgQ, tQsQ);\n",
        "\n",
        "// 5. 创建Fragment并执行MMA\n",
        "Tensor acc = partition_fragment_C(tiled_mma, Shape<Int<kBlockM>, Int<kBlockN>>{});\n",
        "clear(acc);\n",
        "cute::gemm(tiled_mma, tSrQ, tSrK, acc);\n",
        "'''\n",
        "\n",
        "print(flashattention_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 模拟FlashAttention的分块策略\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_flashattention_tiling(seqlen, headdim, block_m, block_n):\n",
        "    \"\"\"模拟FlashAttention的分块策略\"\"\"\n",
        "    print(f\"=\" * 60)\n",
        "    print(f\"FlashAttention分块模拟\")\n",
        "    print(f\"=\" * 60)\n",
        "    print(f\"序列长度: {seqlen}\")\n",
        "    print(f\"头维度: {headdim}\")\n",
        "    print(f\"Q块大小 (kBlockM): {block_m}\")\n",
        "    print(f\"K/V块大小 (kBlockN): {block_n}\")\n",
        "    print()\n",
        "    \n",
        "    # 计算分块数\n",
        "    num_q_blocks = (seqlen + block_m - 1) // block_m\n",
        "    num_kv_blocks = (seqlen + block_n - 1) // block_n\n",
        "    \n",
        "    print(f\"Q分块数: {num_q_blocks}\")\n",
        "    print(f\"K/V分块数: {num_kv_blocks}\")\n",
        "    print()\n",
        "    \n",
        "    # 模拟处理流程\n",
        "    print(\"处理流程:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for m_block in range(min(num_q_blocks, 2)):\n",
        "        q_start = m_block * block_m\n",
        "        q_end = min((m_block + 1) * block_m, seqlen)\n",
        "        \n",
        "        print(f\"\\nm_block={m_block}: 处理 Q[{q_start}:{q_end}, :]\")\n",
        "        print(f\"  gQ = local_tile(mQ, ({block_m}, {headdim}), ({m_block}, 0))\")\n",
        "        \n",
        "        for n_block in range(min(num_kv_blocks, 2)):\n",
        "            k_start = n_block * block_n\n",
        "            k_end = min((n_block + 1) * block_n, seqlen)\n",
        "            \n",
        "            print(f\"    n_block={n_block}: K/V[{k_start}:{k_end}, :]\")\n",
        "            print(f\"      S_block = Q_block @ K_block^T\")\n",
        "            print(f\"      O_block += softmax(S_block) @ V_block\")\n",
        "        \n",
        "        if num_kv_blocks > 2:\n",
        "            print(f\"    ... 还有 {num_kv_blocks - 2} 个K/V块\")\n",
        "    \n",
        "    if num_q_blocks > 2:\n",
        "        print(f\"\\n... 还有 {num_q_blocks - 2} 个Q块\")\n",
        "\n",
        "# 模拟典型配置\n",
        "simulate_flashattention_tiling(seqlen=2048, headdim=64, block_m=128, block_n=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "\n",
        "1. **Tensor结构**: Tensor = Engine (数据) + Layout (索引映射)\n",
        "2. **指针类型**: `make_gmem_ptr()` 全局内存, `make_smem_ptr()` 共享内存\n",
        "3. **Layout与Stride**: 行优先 stride=(N,1), 列优先 stride=(1,M)\n",
        "4. **local_tile操作**: 从大Tensor获取分块视图，不拷贝数据\n",
        "5. **FlashAttention应用**: Q/K/V创建为gmem Tensor，分块处理\n",
        "\n",
        "## 下一步\n",
        "\n",
        "在下一节\"Layout内存布局\"中，我们将深入学习Layout的高级特性。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
