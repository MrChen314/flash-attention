# ä»£ç ç›®å½•ç»“æ„

> FlashAttention-2 å­¦ä¹ è®¡åˆ’ Â· ç¬¬å››é˜¶æ®µ Â· ä»£ç ç»“æ„æ€»è§ˆ

---

## ğŸ“– æœ¬ç« æ¦‚è¿°

FlashAttention ä»£ç åº“åŒ…å« Python æ¥å£å±‚å’Œ CUDA æ ¸å¿ƒå®ç°å±‚ã€‚ç†è§£ä»£ç ç›®å½•ç»“æ„æ˜¯æ·±å…¥å­¦ä¹ å…·ä½“å®ç°çš„ç¬¬ä¸€æ­¥ã€‚

**æ ¸å¿ƒå‘ç°ï¼š** FlashAttention çš„ä»£ç ç»„ç»‡éµå¾ªæ¸…æ™°çš„åˆ†å±‚è®¾è®¡ï¼š
1. **Python å±‚**ï¼šç”¨æˆ·æ¥å£å’Œ PyTorch é›†æˆ
2. **C++/CUDA å±‚**ï¼šæ ¸å¿ƒç®—æ³•å®ç°ï¼ŒåŸºäº CUTLASS/cute

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FlashAttention ä»£ç æ¶æ„                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Python Layer                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  flash_attn/                                          â”‚      â”‚
â”‚   â”‚    â”œâ”€â”€ flash_attn_interface.py  # ç”¨æˆ·è°ƒç”¨å…¥å£        â”‚      â”‚
â”‚   â”‚    â”œâ”€â”€ layers/                  # é«˜çº§æ¨¡å—å°è£…        â”‚      â”‚
â”‚   â”‚    â””â”€â”€ models/                  # å®Œæ•´æ¨¡å‹å®ç°        â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                           â†“ pybind11                            â”‚
â”‚   C++/CUDA Layer                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  csrc/flash_attn/                                     â”‚      â”‚
â”‚   â”‚    â”œâ”€â”€ flash_api.cpp            # PyTorch C++ç»‘å®š     â”‚      â”‚
â”‚   â”‚    â””â”€â”€ src/                     # CUDA Kernelå®ç°     â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. é¡¶å±‚ç›®å½•ç»“æ„

```
flash-attention/
â”œâ”€â”€ flash_attn/              # â­ PythonåŒ…ï¼šç”¨æˆ·æ¥å£å±‚
â”œâ”€â”€ csrc/                    # â­ C++/CUDAæºç ï¼šæ ¸å¿ƒå®ç°å±‚
â”‚   â””â”€â”€ flash_attn/          # FlashAttention CUDAå®ç°
â”œâ”€â”€ hopper/                  # â­ Hopperæ¶æ„(SM90)ä¼˜åŒ–å®ç°
â”œâ”€â”€ benchmarks/              # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”œâ”€â”€ tests/                   # å•å…ƒæµ‹è¯•
â”œâ”€â”€ training/                # è®­ç»ƒç¤ºä¾‹å’Œé…ç½®
â”œâ”€â”€ examples/                # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ setup.py                 # PythonåŒ…å®‰è£…è„šæœ¬
â””â”€â”€ README.md                # é¡¹ç›®è¯´æ˜
```

---

## 2. Python æ¥å£å±‚ï¼š`flash_attn/`

```
flash_attn/
â”œâ”€â”€ __init__.py                      # åŒ…å…¥å£
â”œâ”€â”€ flash_attn_interface.py          # â­â­â­ æ ¸å¿ƒæ¥å£ï¼Œç”¨æˆ·ç›´æ¥è°ƒç”¨
â”œâ”€â”€ bert_padding.py                  # BERT paddingå¤„ç†å·¥å…·
â”‚
â”œâ”€â”€ layers/                          # é«˜çº§å±‚å°è£…
â”‚   â”œâ”€â”€ rotary.py                    # æ—‹è½¬ä½ç½®ç¼–ç (RoPE)
â”‚   â””â”€â”€ patch_embed.py               # Vision Transformerçš„Patch Embedding
â”‚
â”œâ”€â”€ modules/                         # å®Œæ•´æ¨¡å—å°è£…
â”‚   â”œâ”€â”€ mha.py                       # â­ Multi-Head Attentionæ¨¡å—
â”‚   â”œâ”€â”€ mlp.py                       # MLPæ¨¡å—
â”‚   â”œâ”€â”€ block.py                     # Transformer Block
â”‚   â””â”€â”€ embedding.py                 # Embeddingå±‚
â”‚
â”œâ”€â”€ models/                          # é¢„è®­ç»ƒæ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ gpt.py                       # GPTç³»åˆ—
â”‚   â”œâ”€â”€ llama.py                     # LLaMAç³»åˆ—
â”‚   â”œâ”€â”€ bert.py                      # BERT
â”‚   â”œâ”€â”€ falcon.py                    # Falcon
â”‚   â”œâ”€â”€ opt.py                       # OPT
â”‚   â””â”€â”€ vit.py                       # Vision Transformer
â”‚
â”œâ”€â”€ ops/                             # åº•å±‚æ“ä½œå°è£…
â”‚   â”œâ”€â”€ fused_dense.py               # èåˆçš„Denseå±‚
â”‚   â”œâ”€â”€ layer_norm.py                # LayerNorm
â”‚   â”œâ”€â”€ rms_norm.py                  # RMSNorm
â”‚   â””â”€â”€ activations.py               # æ¿€æ´»å‡½æ•°
â”‚
â”œâ”€â”€ losses/                          # æŸå¤±å‡½æ•°
â”‚   â””â”€â”€ cross_entropy.py             # èåˆçš„CrossEntropy
â”‚
â”œâ”€â”€ utils/                           # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ benchmark.py                 # æ€§èƒ½æµ‹è¯•å·¥å…·
â”‚   â”œâ”€â”€ distributed.py               # åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
â”‚   â””â”€â”€ generation.py                # æ–‡æœ¬ç”Ÿæˆå·¥å…·
â”‚
â””â”€â”€ cute/                            # â­ æ–°ç‰ˆcute-basedå®ç°(å®éªŒæ€§)
    â”œâ”€â”€ flash_fwd.py                 # cuteå‰å‘å®ç°
    â”œâ”€â”€ flash_bwd.py                 # cuteåå‘å®ç°
    â””â”€â”€ interface.py                 # cuteç‰ˆæœ¬æ¥å£
```

### 2.1 æ ¸å¿ƒæ–‡ä»¶ï¼š`flash_attn_interface.py`

è¿™æ˜¯ç”¨æˆ·æœ€å¸¸è°ƒç”¨çš„æ–‡ä»¶ï¼Œæä¾›ä»¥ä¸‹ä¸»è¦å‡½æ•°ï¼š

| å‡½æ•° | åŠŸèƒ½ | ä½¿ç”¨åœºæ™¯ |
|------|------|----------|
| `flash_attn_func` | æ ‡å‡†FlashAttention | å›ºå®šé•¿åº¦åºåˆ— |
| `flash_attn_varlen_func` | å˜é•¿åºåˆ—ç‰ˆæœ¬ | padding-freeè®­ç»ƒ |
| `flash_attn_qkvpacked_func` | QKVæ‰“åŒ…ç‰ˆæœ¬ | è‡ªæ³¨æ„åŠ›ä¼˜åŒ– |
| `flash_attn_kvpacked_func` | KVæ‰“åŒ…ç‰ˆæœ¬ | äº¤å‰æ³¨æ„åŠ› |
| `flash_attn_with_kvcache` | KV Cacheç‰ˆæœ¬ | æ¨ç†åŠ é€Ÿ |

```python
# ä½¿ç”¨ç¤ºä¾‹
from flash_attn import flash_attn_func

# æ ‡å‡†è°ƒç”¨
output = flash_attn_func(
    q, k, v,
    dropout_p=0.0,
    softmax_scale=None,
    causal=True
)
```

---

## 3. C++/CUDA æ ¸å¿ƒå±‚ï¼š`csrc/flash_attn/`

```
csrc/flash_attn/
â”œâ”€â”€ flash_api.cpp                    # â­ APIå…¥å£ï¼ŒPyTorch C++ç»‘å®š
â””â”€â”€ src/
    â”œâ”€â”€ flash.h                      # â­ æ ¸å¿ƒæ•°æ®ç»“æ„å®šä¹‰
    â”‚
    â”‚   â”€â”€ å‰å‘ä¼ æ’­ç›¸å…³ â”€â”€
    â”œâ”€â”€ flash_fwd_kernel.h           # â­â­â­ å‰å‘kernelæ ¸å¿ƒå®ç°
    â”œâ”€â”€ flash_fwd_launch_template.h  # å‰å‘kernelå¯åŠ¨æ¨¡æ¿
    â”œâ”€â”€ flash_fwd_hdim*.cu           # é¢„ç¼–è¯‘çš„å‰å‘kernelå®ä¾‹
    â”œâ”€â”€ flash_fwd_split_hdim*.cu     # Split-KVç‰ˆæœ¬çš„kernelå®ä¾‹
    â”‚
    â”‚   â”€â”€ åå‘ä¼ æ’­ç›¸å…³ â”€â”€
    â”œâ”€â”€ flash_bwd_kernel.h           # åå‘kernelæ ¸å¿ƒå®ç°
    â”œâ”€â”€ flash_bwd_launch_template.h  # åå‘kernelå¯åŠ¨æ¨¡æ¿
    â”œâ”€â”€ flash_bwd_preprocess_kernel.h # åå‘é¢„å¤„ç†kernel
    â”œâ”€â”€ flash_bwd_hdim*.cu           # é¢„ç¼–è¯‘çš„åå‘kernelå®ä¾‹
    â”‚
    â”‚   â”€â”€ æ ¸å¿ƒç»„ä»¶ â”€â”€
    â”œâ”€â”€ kernel_traits.h              # â­ Kernelé…ç½®traits
    â”œâ”€â”€ block_info.h                 # å—ä¿¡æ¯å¤„ç†
    â”œâ”€â”€ softmax.h                    # â­ Online Softmaxå®ç°
    â”œâ”€â”€ mask.h                       # Causal/Local maskå®ç°
    â”‚
    â”‚   â”€â”€ å¯é€‰åŠŸèƒ½ â”€â”€
    â”œâ”€â”€ dropout.h                    # Dropoutå®ç°
    â”œâ”€â”€ rotary.h                     # RoPEæ—‹è½¬ä½ç½®ç¼–ç 
    â”œâ”€â”€ alibi.h                      # ALiBiä½ç½®ç¼–ç 
    â”‚
    â”‚   â”€â”€ å·¥å…·æ–‡ä»¶ â”€â”€
    â”œâ”€â”€ utils.h                      # å·¥å…·å‡½æ•°
    â”œâ”€â”€ hardware_info.h              # ç¡¬ä»¶ä¿¡æ¯æ£€æµ‹
    â”œâ”€â”€ static_switch.h              # ç¼–è¯‘æœŸæ¡ä»¶åˆ†æ”¯
    â”œâ”€â”€ namespace_config.h           # å‘½åç©ºé—´é…ç½®
    â”œâ”€â”€ philox.cuh                   # éšæœºæ•°ç”Ÿæˆï¼ˆDropoutç”¨ï¼‰
    â””â”€â”€ generate_kernels.py          # Kernelå®ä¾‹ç”Ÿæˆè„šæœ¬
```

### 3.1 æ ¸å¿ƒæ–‡ä»¶è¯¦è§£

#### `flash.h` - å‚æ•°ç»“æ„ä½“å®šä¹‰

å®šä¹‰äº†å‰å‘å’Œåå‘ä¼ æ’­æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ï¼š

```cpp
// åŸºç¡€QKVå‚æ•°
struct Qkv_params {
    void *q_ptr, *k_ptr, *v_ptr;        // æ•°æ®æŒ‡é’ˆ
    int64_t q_row_stride, k_row_stride;  // è¡Œæ­¥é•¿
    int64_t q_head_stride, k_head_stride; // å¤´æ­¥é•¿
    int h, h_k;                          // å¤´æ•°
};

// å‰å‘ä¼ æ’­å‚æ•°ï¼ˆç»§æ‰¿Qkv_paramsï¼‰
struct Flash_fwd_params : public Qkv_params {
    void *o_ptr;                         // è¾“å‡ºæŒ‡é’ˆ
    void *softmax_lse_ptr;               // LSEï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰
    int b, seqlen_q, seqlen_k, d;        // ç»´åº¦ä¿¡æ¯
    float scale_softmax;                 // softmaxç¼©æ”¾å› å­
    int window_size_left, window_size_right; // å±€éƒ¨æ³¨æ„åŠ›çª—å£
    bool is_causal;                      // æ˜¯å¦å› æœæ©ç 
    // ... æ›´å¤šå‚æ•°
};
```

#### `kernel_traits.h` - Kernelé…ç½®

ä½¿ç”¨ C++ æ¨¡æ¿å®šä¹‰ Kernel çš„é…ç½®å‚æ•°ï¼š

```cpp
template<int kHeadDim_, int kBlockM_, int kBlockN_, int kNWarps_, typename elem_type>
struct Flash_fwd_kernel_traits {
    // æ•°æ®ç±»å‹
    using Element = elem_type;          // è¾“å…¥å…ƒç´ ç±»å‹ (fp16/bf16)
    using ElementAccum = float;         // ç´¯åŠ å™¨ç±»å‹ (fp32)

    // åˆ†å—å‚æ•°
    static constexpr int kBlockM = kBlockM_;  // Qçš„å—å¤§å°
    static constexpr int kBlockN = kBlockN_;  // K/Vçš„å—å¤§å°
    static constexpr int kHeadDim = kHeadDim_; // å¤´ç»´åº¦

    // å¹¶è¡Œå‚æ•°
    static constexpr int kNWarps = kNWarps_;   // Warpæ•°é‡
    static constexpr int kNThreads = kNWarps * 32; // çº¿ç¨‹æ•°

    // cuteç±»å‹å®šä¹‰
    using TiledMma = TiledMMA<...>;      // Tensor Core MMAæ“ä½œ
    using SmemLayoutQ = ...;             // Qçš„å…±äº«å†…å­˜å¸ƒå±€
    using SmemLayoutKV = ...;            // K/Vçš„å…±äº«å†…å­˜å¸ƒå±€
};
```

#### `flash_fwd_kernel.h` - æ ¸å¿ƒè®¡ç®—å‡½æ•°

åŒ…å«å®é™…çš„æ³¨æ„åŠ›è®¡ç®—é€»è¾‘ï¼š

```cpp
template<typename Kernel_traits, ...>
__device__ void compute_attn_1rowblock(
    const Params &params,
    const int bidb,     // batchç´¢å¼•
    const int bidh,     // headç´¢å¼•
    const int m_block   // Qçš„å—ç´¢å¼•
) {
    // 1. ä»HBMåŠ è½½Qå—åˆ°SRAM
    // 2. éå†K/Vå—ï¼š
    //    - åŠ è½½Kå—åˆ°SRAM
    //    - è®¡ç®— S = Q @ K^T
    //    - åº”ç”¨maskå’Œsoftmax
    //    - åŠ è½½Vå—åˆ°SRAM
    //    - è®¡ç®— O += P @ V
    // 3. å†™å›ç»“æœOåˆ°HBM
}
```

### 3.2 é¢„ç¼–è¯‘çš„Kernelå®ä¾‹

ä¸ºäº†å‡å°‘ç¼–è¯‘æ—¶é—´å’Œè¿è¡Œæ—¶åˆ†æ´¾å¼€é”€ï¼ŒFlashAttention é¢„ç¼–è¯‘äº†å¤šç§é…ç½®çš„ Kernelï¼š

```
flash_fwd_hdim{32,64,96,128,192,256}_{fp16,bf16}_{causal,écausal}_sm80.cu
```

å‘½åè§„åˆ™ï¼š
- `hdim*`ï¼šå¤´ç»´åº¦ï¼ˆ32/64/96/128/192/256ï¼‰
- `fp16/bf16`ï¼šæ•°æ®ç²¾åº¦
- `causal`ï¼šæ˜¯å¦ä½¿ç”¨å› æœæ©ç 
- `sm80`ï¼šç›®æ ‡æ¶æ„ï¼ˆAmpereï¼‰

---

## 4. Hopper æ¶æ„ä¼˜åŒ–ï¼š`hopper/`

é’ˆå¯¹ H100 (SM90) çš„ä¸“é—¨ä¼˜åŒ–ç‰ˆæœ¬ï¼š

```
hopper/
â”œâ”€â”€ flash_api.cpp                    # H100ç‰ˆæœ¬çš„API
â”œâ”€â”€ flash.h                          # å‚æ•°ç»“æ„
â”‚
â”‚   â”€â”€ å‰å‘ä¼ æ’­ â”€â”€
â”œâ”€â”€ flash_fwd_kernel_sm90.h          # â­ SM90å‰å‘kernel
â”œâ”€â”€ flash_fwd_kernel_sm80.h          # SM80å…¼å®¹kernel
â”œâ”€â”€ mainloop_fwd_sm90_tma_gmma_ws.hpp # â­ TMA+GMMAä¸»å¾ªç¯
â”œâ”€â”€ epilogue_fwd.hpp                 # å‰å‘Epilogue
â”‚
â”‚   â”€â”€ åå‘ä¼ æ’­ â”€â”€
â”œâ”€â”€ flash_bwd_kernel_sm90.h          # SM90åå‘kernel
â”œâ”€â”€ mainloop_bwd_sm90_tma_gmma_ws.hpp # TMA+GMMAåå‘ä¸»å¾ªç¯
â”œâ”€â”€ epilogue_bwd.hpp                 # åå‘Epilogue
â”‚
â”‚   â”€â”€ è°ƒåº¦å’Œä¼˜åŒ– â”€â”€
â”œâ”€â”€ tile_scheduler.hpp               # â­ Tileè°ƒåº¦å™¨
â”œâ”€â”€ tile_size.h                      # Tileå¤§å°é…ç½®
â”œâ”€â”€ heuristics.h                     # å¯å‘å¼å‚æ•°é€‰æ‹©
â”‚
â”‚   â”€â”€ H100ç‰¹æ€§æ”¯æŒ â”€â”€
â”œâ”€â”€ sm90_pipeline_no_cluster.hpp     # æµæ°´çº¿å®ç°
â”œâ”€â”€ named_barrier.hpp                # å‘½åBarrier
â”œâ”€â”€ paged_kv.h                       # åˆ†é¡µKV Cache
â”‚
â””â”€â”€ instantiations/                  # é¢„ç¼–è¯‘Kernelï¼ˆ~450ä¸ªï¼‰
    â””â”€â”€ *.cu
```

### Hopper ç‰ˆæœ¬çš„å…³é”®ä¼˜åŒ–

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| TMA (Tensor Memory Accelerator) | ç¡¬ä»¶åŠ é€Ÿçš„å¼‚æ­¥å†…å­˜æ‹·è´ |
| GMMA (Group Matrix Multiply-Add) | æ›´å¤§çš„ Tensor Core æ“ä½œ |
| Warp Specialization | ä¸“é—¨åŒ–çš„è®¡ç®—å’Œå†…å­˜è®¿é—® Warp |
| åˆ†é¡µ KV Cache | æ”¯æŒ vLLM é£æ ¼çš„åˆ†é¡µå†…å­˜ |

---

## 5. æ–‡ä»¶ä¾èµ–å…³ç³»

```mermaid
graph TD
    subgraph Python Layer
        A[flash_attn_interface.py] --> B[flash_attn_2_cuda]
    end
    
    subgraph C++ Binding
        B --> C[flash_api.cpp]
    end
    
    subgraph CUDA Kernel
        C --> D[flash.h]
        C --> E[flash_fwd_launch_template.h]
        E --> F[flash_fwd_kernel.h]
        F --> G[kernel_traits.h]
        F --> H[softmax.h]
        F --> I[mask.h]
        F --> J[block_info.h]
    end
    
    subgraph cute/CUTLASS
        G --> K[cute/tensor.hpp]
        G --> L[cutlass/cutlass.h]
    end
```

---

## 6. é‡è¦æ–‡ä»¶é€ŸæŸ¥è¡¨

| æ–‡ä»¶ | é‡è¦ç¨‹åº¦ | å†…å®¹æ¦‚è¦ |
|------|----------|----------|
| `flash_attn_interface.py` | â­â­â­ | Pythonç”¨æˆ·æ¥å£ |
| `flash_api.cpp` | â­â­â­ | PyTorchç»‘å®šï¼Œå‚æ•°å¤„ç† |
| `flash.h` | â­â­ | å‚æ•°ç»“æ„ä½“å®šä¹‰ |
| `flash_fwd_kernel.h` | â­â­â­ | å‰å‘è®¡ç®—æ ¸å¿ƒé€»è¾‘ |
| `kernel_traits.h` | â­â­ | Kernelé…ç½®å’Œcuteç±»å‹ |
| `softmax.h` | â­â­ | Online Softmaxå®ç° |
| `mask.h` | â­ | Causal/Local mask |
| `block_info.h` | â­ | å˜é•¿åºåˆ—å—ä¿¡æ¯ |
| `static_switch.h` | â­ | ç¼–è¯‘æœŸæ¡ä»¶åˆ†æ”¯å® |

---

## 7. ç¼–è¯‘äº§ç‰©

å®‰è£… FlashAttention åï¼Œä¼šç”Ÿæˆä»¥ä¸‹ç¼–è¯‘äº§ç‰©ï¼š

```
flash_attn_2_cuda.cpython-*.so     # ä¸»è¦çš„CUDAæ‰©å±•
â”œâ”€â”€ mha_fwd()                      # å‰å‘ä¼ æ’­
â”œâ”€â”€ mha_bwd()                      # åå‘ä¼ æ’­
â”œâ”€â”€ mha_varlen_fwd()               # å˜é•¿å‰å‘
â”œâ”€â”€ mha_varlen_bwd()               # å˜é•¿åå‘
â””â”€â”€ mha_fwd_kvcache()              # KV Cacheå‰å‘
```

---

## âœ… ç†è§£æ£€æŸ¥ç‚¹

å®Œæˆæœ¬èŠ‚åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] æè¿° FlashAttention çš„ä»£ç åˆ†å±‚ç»“æ„
- [ ] æ‰¾åˆ° Python æ¥å£çš„å…¥å£æ–‡ä»¶
- [ ] è¯´æ˜ `flash_fwd_kernel.h` çš„ä½œç”¨
- [ ] è§£é‡Šä¸ºä»€ä¹ˆæœ‰å¤šä¸ªé¢„ç¼–è¯‘çš„ `.cu` æ–‡ä»¶
- [ ] åŒºåˆ† `csrc/flash_attn/` å’Œ `hopper/` çš„åŒºåˆ«

---

**ä¸‹ä¸€èŠ‚ï¼š** [è°ƒç”¨é“¾æ¦‚è§ˆ](./2è°ƒç”¨é“¾æ¦‚è§ˆ.md)

