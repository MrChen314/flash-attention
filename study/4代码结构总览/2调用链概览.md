# è°ƒç”¨é“¾æ¦‚è§ˆ

> FlashAttention-2 å­¦ä¹ è®¡åˆ’ Â· ç¬¬å››é˜¶æ®µ Â· ä»£ç ç»“æ„æ€»è§ˆ

---

## ğŸ“– æœ¬ç« æ¦‚è¿°

ç†è§£ä» Python è°ƒç”¨åˆ° CUDA Kernel æ‰§è¡Œçš„å®Œæ•´è°ƒç”¨é“¾ï¼Œæ˜¯æ·±å…¥ç ”ç©¶ FlashAttention å®ç°çš„å…³é”®ã€‚æœ¬èŠ‚å°†è¿½è¸ªä¸€æ¬¡ `flash_attn_func` è°ƒç”¨çš„å®Œæ•´è·¯å¾„ã€‚

**æ ¸å¿ƒè°ƒç”¨é“¾ï¼š**

```
Python: flash_attn_func()
    â†“
Python: flash_attn_gpu.fwd()
    â†“
C++: mha_fwd()  [flash_api.cpp]
    â†“
C++: run_mha_fwd()
    â†“
C++: run_flash_fwd<Kernel_traits>()
    â†“
CUDA: flash_fwd_kernel<<<...>>>()
    â†“
CUDA: compute_attn_1rowblock()  [æ ¸å¿ƒè®¡ç®—]
```

---

## 1. å®Œæ•´è°ƒç”¨é“¾å›¾ç¤º

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Python Layer                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  ç”¨æˆ·ä»£ç                                                                 â”‚
â”‚  output = flash_attn_func(q, k, v, causal=True)                         â”‚
â”‚                    â”‚                                                     â”‚
â”‚                    â†“                                                     â”‚
â”‚  flash_attn/flash_attn_interface.py                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ def flash_attn_func(q, k, v, ...):                                â”‚   â”‚
â”‚  â”‚     ...                                                           â”‚   â”‚
â”‚  â”‚     out, softmax_lse, _, _ = _flash_attn_forward(...)             â”‚   â”‚
â”‚  â”‚     return FlashAttnFunc.apply(q, k, v, out, softmax_lse, ...)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                                     â”‚
â”‚                    â†“                                                     â”‚
â”‚  flash_attn_2_cuda.fwd()  â†â”€â”€ pybind11ç»‘å®š                              â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“ Python â†’ C++ è¾¹ç•Œ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           C++ Layer                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  csrc/flash_attn/flash_api.cpp                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {                        â”‚   â”‚
â”‚  â”‚     m.def("fwd", &mha_fwd, "Forward pass");                       â”‚   â”‚
â”‚  â”‚     ...                                                           â”‚   â”‚
â”‚  â”‚ }                                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                                     â”‚
â”‚                    â†“                                                     â”‚
â”‚  mha_fwd() å‡½æ•°                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. å‚æ•°éªŒè¯ï¼ˆCHECK_DEVICE, CHECK_CONTIGUOUSç­‰ï¼‰                    â”‚   â”‚
â”‚  â”‚ 2. åˆ†é…è¾“å‡ºtensor                                                 â”‚   â”‚
â”‚  â”‚ 3. è®¾ç½® Flash_fwd_params ç»“æ„ä½“                                   â”‚   â”‚
â”‚  â”‚ 4. è°ƒç”¨ run_mha_fwd(params, stream)                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                                     â”‚
â”‚                    â†“                                                     â”‚
â”‚  run_mha_fwd()                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. FP16_SWITCH: æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©fp16æˆ–bf16                        â”‚   â”‚
â”‚  â”‚ 2. HEADDIM_SWITCH: æ ¹æ®head_dimé€‰æ‹©Kernelé…ç½®                     â”‚   â”‚
â”‚  â”‚ 3. BOOL_SWITCH: æ ¹æ®is_causalé€‰æ‹©Kernelå˜ä½“                       â”‚   â”‚
â”‚  â”‚ 4. è°ƒç”¨ run_mha_fwd_<...>() æˆ– run_mha_fwd_splitkv_dispatch()    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“ C++ â†’ CUDA Kernel
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CUDA Kernel Layer                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  csrc/flash_attn/src/flash_fwd_launch_template.h                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ template<typename Kernel_traits, bool Is_dropout, bool Is_causal> â”‚   â”‚
â”‚  â”‚ void run_flash_fwd(Flash_fwd_params &params, cudaStream_t stream) â”‚   â”‚
â”‚  â”‚ {                                                                 â”‚   â”‚
â”‚  â”‚     dim3 grid(num_m_block, params.b, params.h);                   â”‚   â”‚
â”‚  â”‚     flash_fwd_kernel<...><<<grid, threads, smem, stream>>>(params)â”‚   â”‚
â”‚  â”‚ }                                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                                     â”‚
â”‚                    â†“                                                     â”‚
â”‚  flash_fwd_kernel() - GPU Kernelå…¥å£                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ __global__ void flash_fwd_kernel(const Flash_fwd_params params)   â”‚   â”‚
â”‚  â”‚ {                                                                 â”‚   â”‚
â”‚  â”‚     compute_attn<Kernel_traits, ...>(params);                     â”‚   â”‚
â”‚  â”‚ }                                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                                     â”‚
â”‚                    â†“                                                     â”‚
â”‚  csrc/flash_attn/src/flash_fwd_kernel.h                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ compute_attn() â†’ compute_attn_1rowblock()                         â”‚   â”‚
â”‚  â”‚ {                                                                 â”‚   â”‚
â”‚  â”‚   // â­ FlashAttentionæ ¸å¿ƒè®¡ç®—é€»è¾‘                                â”‚   â”‚
â”‚  â”‚   // 1. åŠ è½½Qå—åˆ°å…±äº«å†…å­˜                                         â”‚   â”‚
â”‚  â”‚   // 2. éå†K/Vå—:                                                â”‚   â”‚
â”‚  â”‚   //    - åŠ è½½Kå—ï¼Œè®¡ç®—S = QK^T                                   â”‚   â”‚
â”‚  â”‚   //    - åº”ç”¨maskå’Œonline softmax                                â”‚   â”‚
â”‚  â”‚   //    - åŠ è½½Vå—ï¼Œç´¯åŠ O += P @ V                                 â”‚   â”‚
â”‚  â”‚   // 3. å†™å›ç»“æœ                                                  â”‚   â”‚
â”‚  â”‚ }                                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. å„å±‚è¯¦ç»†åˆ†æ

### 2.1 Python å±‚ï¼š`flash_attn_interface.py`

#### å…¥å£å‡½æ•° `flash_attn_func`

```python
def flash_attn_func(
    q,                      # [batch, seqlen_q, nheads, headdim]
    k,                      # [batch, seqlen_k, nheads_k, headdim]
    v,                      # [batch, seqlen_k, nheads_k, headdim]
    dropout_p=0.0,
    softmax_scale=None,
    causal=False,
    window_size=(-1, -1),   # å±€éƒ¨æ³¨æ„åŠ›çª—å£
    softcap=0.0,            # softmaxä¸Šé™
    alibi_slopes=None,      # ALiBiä½ç½®ç¼–ç 
    deterministic=False,
    return_attn_probs=False,
) -> Tensor:
```

**å…³é”®æ­¥éª¤ï¼š**

1. **å‚æ•°é¢„å¤„ç†**ï¼šç¡®ä¿tensorè¿ç»­
2. **è°ƒç”¨åº•å±‚å‡½æ•°**ï¼š`_flash_attn_forward()` è°ƒç”¨ CUDA kernel
3. **è‡ªåŠ¨å¾®åˆ†æ”¯æŒ**ï¼šè¿”å› `FlashAttnFunc.apply()` æ”¯æŒåå‘ä¼ æ’­

#### CUDAç»‘å®šè°ƒç”¨

```python
# flash_attn_interface.py
import flash_attn_2_cuda as flash_attn_gpu

# è°ƒç”¨C++æ‰©å±•
out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(
    q, k, v,
    out,            # å¯é€‰çš„è¾“å‡ºtensor
    alibi_slopes,
    dropout_p,
    softmax_scale,
    causal,
    window_size_left,
    window_size_right,
    softcap,
    return_softmax,
    rng_state       # éšæœºçŠ¶æ€ï¼ˆç”¨äºDropoutï¼‰
)
```

### 2.2 C++ å±‚ï¼š`flash_api.cpp`

#### pybind11 ç»‘å®š

```cpp
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.doc() = "FlashAttention";
    m.def("fwd", &mha_fwd, "Forward pass");
    m.def("varlen_fwd", &mha_varlen_fwd, "Forward pass (variable length)");
    m.def("bwd", &mha_bwd, "Backward pass");
    m.def("varlen_bwd", &mha_varlen_bwd, "Backward pass (variable length)");
    m.def("fwd_kvcache", &mha_fwd_kvcache, "Forward pass with KV-cache");
}
```

#### `mha_fwd` å‡½æ•°

```cpp
std::vector<at::Tensor> mha_fwd(
    at::Tensor &q,          // [batch, seqlen_q, num_heads, head_size]
    const at::Tensor &k,    // [batch, seqlen_k, num_heads_k, head_size]
    const at::Tensor &v,    // [batch, seqlen_k, num_heads_k, head_size]
    c10::optional<at::Tensor> &out_,
    c10::optional<at::Tensor> &alibi_slopes_,
    const float p_dropout,
    const float softmax_scale,
    bool is_causal,
    int window_size_left,
    int window_size_right,
    const float softcap,
    const bool return_softmax,
    c10::optional<at::Generator> gen_
) {
    // 1. å‚æ•°éªŒè¯
    CHECK_DEVICE(q); CHECK_DEVICE(k); CHECK_DEVICE(v);
    CHECK_CONTIGUOUS(q); CHECK_CONTIGUOUS(k); CHECK_CONTIGUOUS(v);
    
    // 2. åˆ†é…è¾“å‡ºtensor
    at::Tensor out = out_.has_value() ? out_.value() : 
                     torch::empty_like(q);
    at::Tensor softmax_lse = torch::empty({batch, heads, seqlen_q}, ...);
    
    // 3. å¡«å……å‚æ•°ç»“æ„ä½“
    Flash_fwd_params params;
    set_params_fprop(params, batch, seqlen_q, seqlen_k, ...);
    
    // 4. è°ƒç”¨Kernel
    auto stream = at::cuda::getCurrentCUDAStream().stream();
    run_mha_fwd(params, stream);
    
    return {out, softmax_lse, ...};
}
```

#### `run_mha_fwd` - å¤šçº§é™æ€åˆ†æ´¾

```cpp
void run_mha_fwd(Flash_fwd_params &params, cudaStream_t stream, 
                 bool force_split_kernel=false) {
    // æ ¹æ®æ•°æ®ç±»å‹åˆ†æ´¾
    FP16_SWITCH(!params.is_bf16, [&] {
        // æ ¹æ®head_dimåˆ†æ´¾
        HEADDIM_SWITCH(params.d, [&] {
            // æ ¹æ®æ˜¯å¦causalåˆ†æ´¾
            BOOL_SWITCH(params.is_causal, Is_causal, [&] {
                if (params.num_splits <= 1 && !force_split_kernel) {
                    run_mha_fwd_<elem_type, kHeadDim, Is_causal>(params, stream);
                } else {
                    run_mha_fwd_splitkv_dispatch<...>(params, stream);
                }
            });
        });
    });
}
```

**é™æ€åˆ†æ´¾å®è§£é‡Šï¼š**

| å® | ä½œç”¨ | å¯èƒ½çš„å€¼ |
|------|------|----------|
| `FP16_SWITCH` | é€‰æ‹©æ•°æ®ç±»å‹ | `cutlass::half_t` / `cutlass::bfloat16_t` |
| `HEADDIM_SWITCH` | é€‰æ‹©å¤´ç»´åº¦ | 32, 64, 96, 128, 192, 256 |
| `BOOL_SWITCH` | é€‰æ‹©æ˜¯å¦causal | `true` / `false` |

### 2.3 CUDA Kernel å±‚

#### Kernel å¯åŠ¨ï¼š`flash_fwd_launch_template.h`

```cpp
template<typename Kernel_traits, bool Is_dropout, bool Is_causal>
void run_flash_fwd(Flash_fwd_params &params, cudaStream_t stream) {
    // è®¡ç®—Gridç»´åº¦
    const int num_m_block = (params.seqlen_q + Kernel_traits::kBlockM - 1) 
                          / Kernel_traits::kBlockM;
    dim3 grid(num_m_block, params.b, params.h);
    
    // è·å–Kernelå‡½æ•°æŒ‡é’ˆ
    auto kernel = &flash_fwd_kernel<Kernel_traits, Is_dropout, Is_causal, ...>;
    
    // è®¾ç½®å…±äº«å†…å­˜å¤§å°
    constexpr size_t smem_size = Kernel_traits::kSmemSize;
    if (smem_size >= 48 * 1024) {
        cudaFuncSetAttribute(kernel, 
            cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
    }
    
    // å¯åŠ¨Kernel
    kernel<<<grid, Kernel_traits::kNThreads, smem_size, stream>>>(params);
}
```

**Grid å’Œ Block é…ç½®ï¼š**

```
Grid = (num_m_blocks, batch_size, num_heads)
     = (ceil(seqlen_q / kBlockM), B, H)

Block = (kNThreads,)
      = (kNWarps * 32,)
      = é€šå¸¸ 128 æˆ– 256 ä¸ªçº¿ç¨‹
```

#### Kernel å…¥å£ï¼š`flash_fwd_kernel`

```cpp
template<typename Kernel_traits, bool Is_dropout, bool Is_causal, ...>
__global__ void flash_fwd_kernel(const Flash_fwd_params params) {
    #if defined(ARCH_SUPPORTS_FLASH)
        compute_attn<Kernel_traits, Is_dropout, Is_causal, ...>(params);
    #else
        FLASH_UNSUPPORTED_ARCH
    #endif
}
```

#### æ ¸å¿ƒè®¡ç®—ï¼š`compute_attn_1rowblock`

```cpp
template<typename Kernel_traits, ...>
__device__ void compute_attn_1rowblock(
    const Params &params, 
    const int bidb,     // batchç´¢å¼•
    const int bidh,     // headç´¢å¼•
    const int m_block   // Qå—ç´¢å¼•
) {
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 1. åˆå§‹åŒ–å’Œè¾¹ç•Œæ£€æŸ¥
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    extern __shared__ char smem_[];
    const int tidx = threadIdx.x;
    
    constexpr int kBlockM = Kernel_traits::kBlockM;  // Qå—å¤§å°
    constexpr int kBlockN = Kernel_traits::kBlockN;  // K/Vå—å¤§å°
    constexpr int kHeadDim = Kernel_traits::kHeadDim;
    
    // è®¡ç®—Kæ–¹å‘çš„å—èŒƒå›´
    int n_block_min = 0;  // å¯èƒ½å› local attentionè°ƒæ•´
    int n_block_max = ceil_div(seqlen_k, kBlockN);
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 2. åˆ›å»ºTensorè§†å›¾ (ä½¿ç”¨cute)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Q: [seqlen_q, nheads, headdim] çš„å…¨å±€å†…å­˜è§†å›¾
    Tensor mQ = make_tensor(make_gmem_ptr(params.q_ptr), ...);
    // è·å–å½“å‰å—çš„å±€éƒ¨tile
    Tensor gQ = local_tile(mQ, Shape<kBlockM, kHeadDim>{}, 
                          make_coord(m_block, 0));
    
    // K/Vç±»ä¼¼å¤„ç†
    Tensor gK = local_tile(mK, Shape<kBlockN, kHeadDim>{}, ...);
    Tensor gV = local_tile(mV, Shape<kBlockN, kHeadDim>{}, ...);
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 3. åŠ è½½Qåˆ°å…±äº«å†…å­˜/å¯„å­˜å™¨
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // å¼‚æ­¥æ‹·è´Qä»HBMåˆ°SRAM
    cute::copy(gmem_tiled_copy_Q, gQ, sQ);
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 4. ä¸»å¾ªç¯ï¼šéå†K/Vå—
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    for (int n_block = n_block_max - 1; n_block >= n_block_min; --n_block) {
        // 4.1 åŠ è½½Kå—åˆ°å…±äº«å†…å­˜
        cute::copy(gmem_tiled_copy_K, gK(_, _, n_block), sK);
        cp_async_fence();
        
        // 4.2 ç­‰å¾…KåŠ è½½å®Œæˆ
        cp_async_wait<0>();
        __syncthreads();
        
        // 4.3 è®¡ç®— S = Q @ K^T (ä½¿ç”¨Tensor Core)
        cute::gemm(tiled_mma, tCrQ, tCrK, tCrS);
        
        // 4.4 åº”ç”¨mask (causalæˆ–local)
        if (Is_causal || Is_local) {
            apply_mask(tCrS, m_block, n_block, ...);
        }
        
        // 4.5 Online Softmax
        // - æ›´æ–°æœ€å¤§å€¼ m_new = max(m_old, row_max(S))
        // - æ›´æ–°åˆ†æ¯ l_new = l_old * exp(m_old - m_new) + row_sum(exp(S - m_new))
        // - ç¼©æ”¾ä¹‹å‰çš„ç´¯åŠ ç»“æœ
        softmax_rescale_o(tCrS, scores_max, scores_sum, tCrO, ...);
        
        // 4.6 åŠ è½½Vå—åˆ°å…±äº«å†…å­˜
        cute::copy(gmem_tiled_copy_V, gV(_, _, n_block), sV);
        
        // 4.7 è®¡ç®— O += P @ V
        cute::gemm(tiled_mma, tCrP, tCrV, tCrO);
    }
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 5. Epilogueï¼šå†™å›ç»“æœ
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // æœ€ç»ˆå½’ä¸€åŒ–ï¼šO = O / l
    // å†™å›Oåˆ°HBM
    cute::copy(gmem_tiled_copy_O, tCrO, gO);
    // å†™å›LSE (ç”¨äºåå‘ä¼ æ’­)
    // LSE = log(l) + m
}
```

---

## 3. å…³é”®æ•°æ®ç»“æ„

### 3.1 `Flash_fwd_params` ç»“æ„ä½“

```cpp
struct Flash_fwd_params : public Qkv_params {
    // â”€â”€â”€â”€â”€ è¾“å…¥/è¾“å‡ºæŒ‡é’ˆ â”€â”€â”€â”€â”€
    void *q_ptr, *k_ptr, *v_ptr;     // Q/K/Væ•°æ®
    void *o_ptr;                      // è¾“å‡º
    void *softmax_lse_ptr;            // log-sum-exp (ç”¨äºåå‘)
    
    // â”€â”€â”€â”€â”€ ç»´åº¦ä¿¡æ¯ â”€â”€â”€â”€â”€
    int b;                            // batch size
    int h, h_k;                       // Qå¤´æ•°, K/Vå¤´æ•° (GQAæ”¯æŒ)
    int seqlen_q, seqlen_k;           // åºåˆ—é•¿åº¦
    int d;                            // head dimension
    
    // â”€â”€â”€â”€â”€ æ­¥é•¿ä¿¡æ¯ â”€â”€â”€â”€â”€
    int64_t q_batch_stride, q_row_stride, q_head_stride;
    int64_t k_batch_stride, k_row_stride, k_head_stride;
    // ... ç±»ä¼¼çš„Vå’ŒOæ­¥é•¿
    
    // â”€â”€â”€â”€â”€ ç¼©æ”¾å› å­ â”€â”€â”€â”€â”€
    float scale_softmax;              // 1/sqrt(d)
    float scale_softmax_log2;         // log2(e) / sqrt(d)
    
    // â”€â”€â”€â”€â”€ å¯é€‰åŠŸèƒ½ â”€â”€â”€â”€â”€
    int window_size_left, window_size_right;  // å±€éƒ¨æ³¨æ„åŠ›
    float softcap;                    // softmaxä¸Šé™
    float p_dropout;                  // dropoutæ¦‚ç‡
    bool is_causal;                   // å› æœæ©ç 
    
    // â”€â”€â”€â”€â”€ å˜é•¿åºåˆ—æ”¯æŒ â”€â”€â”€â”€â”€
    int *cu_seqlens_q, *cu_seqlens_k; // ç´¯ç§¯åºåˆ—é•¿åº¦
};
```

### 3.2 `Kernel_traits` ç±»å‹

```cpp
template<int kHeadDim_, int kBlockM_, int kBlockN_, int kNWarps_, ...>
struct Flash_fwd_kernel_traits {
    // â”€â”€â”€â”€â”€ ç¼–è¯‘æœŸå¸¸é‡ â”€â”€â”€â”€â”€
    static constexpr int kHeadDim = kHeadDim_;  // 64/128/256
    static constexpr int kBlockM = kBlockM_;     // 64/128
    static constexpr int kBlockN = kBlockN_;     // 64/128
    static constexpr int kNWarps = kNWarps_;     // 4/8
    static constexpr int kNThreads = kNWarps * 32;
    
    // â”€â”€â”€â”€â”€ ç±»å‹å®šä¹‰ â”€â”€â”€â”€â”€
    using Element = cutlass::half_t;       // è¾“å…¥ç±»å‹
    using ElementAccum = float;            // ç´¯åŠ ç±»å‹
    
    // â”€â”€â”€â”€â”€ cute MMAé…ç½® â”€â”€â”€â”€â”€
    using TiledMma = TiledMMA<
        MMA_Atom<SM80_16x8x16_F32F16F16F32_TN>,  // Tensor Coreæ“ä½œ
        Layout<Shape<Int<kNWarps>, _1, _1>>,     // Warpå¸ƒå±€
        Tile<Int<16*kNWarps>, _16, _16>          // Tileå¤§å°
    >;
    
    // â”€â”€â”€â”€â”€ å…±äº«å†…å­˜å¸ƒå±€ â”€â”€â”€â”€â”€
    using SmemLayoutQ = ...;   // Qçš„SRAMå¸ƒå±€ (å¸¦swizzle)
    using SmemLayoutKV = ...;  // K/Vçš„SRAMå¸ƒå±€
    
    // â”€â”€â”€â”€â”€ å…±äº«å†…å­˜å¤§å° â”€â”€â”€â”€â”€
    static constexpr int kSmemSize = sizeof(SharedStorage);
};
```

---

## 4. è°ƒç”¨é“¾ä¸­çš„å…³é”®ä¼˜åŒ–

### 4.1 é™æ€å¤šæ€ï¼ˆç¼–è¯‘æœŸåˆ†æ´¾ï¼‰

FlashAttention ä½¿ç”¨æ¨¡æ¿å…ƒç¼–ç¨‹é¿å…è¿è¡Œæ—¶å¼€é”€ï¼š

```cpp
// ä¸ä½¿ç”¨è¿è¡Œæ—¶if-elseï¼Œè€Œæ˜¯ç¼–è¯‘æœŸç”Ÿæˆå¤šä¸ªkernel
HEADDIM_SWITCH(params.d, [&] {
    // kHeadDim æ˜¯ç¼–è¯‘æœŸå¸¸é‡
    run_mha_fwd_<elem_type, kHeadDim, Is_causal>(params, stream);
});
```

### 4.2 Online Softmax

åœ¨ä¸»å¾ªç¯ä¸­å¢é‡è®¡ç®—softmaxï¼Œé¿å…å­˜å‚¨å®Œæ•´çš„NÃ—NçŸ©é˜µï¼š

```cpp
// ä¼ªä»£ç 
m_new = max(m_old, row_max(S_block))
l_new = l_old * exp(m_old - m_new) + row_sum(exp(S_block - m_new))
O_new = O_old * exp(m_old - m_new) + exp(S_block - m_new) @ V_block
```

### 4.3 å¼‚æ­¥å†…å­˜æ‹·è´

ä½¿ç”¨ `cp.async` æŒ‡ä»¤é‡å è®¡ç®—å’Œå†…å­˜è®¿é—®ï¼š

```cpp
// å‘èµ·å¼‚æ­¥æ‹·è´
cute::copy(gmem_tiled_copy, gK, sK);
cp_async_fence();

// æ‰§è¡Œå…¶ä»–è®¡ç®—...

// ç­‰å¾…æ‹·è´å®Œæˆ
cp_async_wait<0>();
```

---

## 5. è°ƒè¯•æŠ€å·§

### 5.1 è¿½è¸ªè°ƒç”¨è·¯å¾„

åœ¨å…³é”®ä½ç½®æ·»åŠ æ‰“å°ï¼š

```cpp
// flash_api.cpp
void run_mha_fwd(...) {
    printf("run_mha_fwd: b=%d, seqlen_q=%d, seqlen_k=%d, d=%d\n",
           params.b, params.seqlen_q, params.seqlen_k, params.d);
    ...
}
```

### 5.2 æ£€æŸ¥Kernelé…ç½®

```cpp
// flash_fwd_launch_template.h
template<typename Kernel_traits, ...>
void run_flash_fwd(...) {
    printf("Kernel config: kBlockM=%d, kBlockN=%d, kHeadDim=%d, kNWarps=%d\n",
           Kernel_traits::kBlockM, Kernel_traits::kBlockN,
           Kernel_traits::kHeadDim, Kernel_traits::kNWarps);
    printf("Grid: (%d, %d, %d), Block: (%d,)\n",
           num_m_block, params.b, params.h, Kernel_traits::kNThreads);
}
```

### 5.3 Python ç«¯è°ƒè¯•

```python
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥æ‰§è¡Œï¼Œæ–¹ä¾¿è°ƒè¯•

# æ£€æŸ¥è¾“å…¥è¾“å‡º
print(f"Q: {q.shape}, K: {k.shape}, V: {v.shape}")
print(f"Q.stride: {q.stride()}, K.stride: {k.stride()}")
```

---

## 6. è°ƒç”¨é“¾æ€»ç»“è¡¨

| å±‚çº§ | æ–‡ä»¶ | å‡½æ•° | åŠŸèƒ½ |
|------|------|------|------|
| Python | `flash_attn_interface.py` | `flash_attn_func` | ç”¨æˆ·API |
| Python | `flash_attn_interface.py` | `_flash_attn_forward` | è°ƒç”¨CUDA |
| C++ | `flash_api.cpp` | `mha_fwd` | å‚æ•°éªŒè¯ã€åˆ†é…å†…å­˜ |
| C++ | `flash_api.cpp` | `set_params_fprop` | å¡«å……å‚æ•°ç»“æ„ä½“ |
| C++ | `flash_api.cpp` | `run_mha_fwd` | é™æ€åˆ†æ´¾ |
| C++ | `flash_fwd_launch_template.h` | `run_flash_fwd` | å¯åŠ¨Kernel |
| CUDA | `flash_fwd_launch_template.h` | `flash_fwd_kernel` | Kernelå…¥å£ |
| CUDA | `flash_fwd_kernel.h` | `compute_attn` | åˆ†é…å—ä»»åŠ¡ |
| CUDA | `flash_fwd_kernel.h` | `compute_attn_1rowblock` | â­ æ ¸å¿ƒè®¡ç®— |

---

## âœ… ç†è§£æ£€æŸ¥ç‚¹

å®Œæˆæœ¬èŠ‚åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] æè¿°ä»Pythonåˆ°CUDAçš„å®Œæ•´è°ƒç”¨é“¾
- [ ] è§£é‡Šé™æ€åˆ†æ´¾çš„ç›®çš„å’Œå®ç°æ–¹å¼
- [ ] è¯´æ˜ `Flash_fwd_params` åŒ…å«å“ªäº›å…³é”®ä¿¡æ¯
- [ ] ç†è§£Gridå’ŒBlockçš„é…ç½®é€»è¾‘
- [ ] è§£é‡Š `compute_attn_1rowblock` çš„ä¸»è¦æ­¥éª¤

---

**ä¸Šä¸€èŠ‚ï¼š** [ä»£ç ç›®å½•ç»“æ„](./1ä»£ç ç›®å½•ç»“æ„.md)

**ä¸‹ä¸€ç« ï¼š** æ·±å…¥ Kernel å®ç°

